{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Shot Learning\n",
    "\n",
    "Here we are checking the performance of the model trained on the English Dataset on other Datasets and their translated versions without any finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_cleaning import Data_Preprocessing\n",
    "from arabert.preprocess import ArabertPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import random\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Tokeniser\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# Utility\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Optimiser\n",
    "from transformers import AdamW\n",
    "\n",
    "# Model\n",
    "\n",
    "import torch.nn as nn\n",
    "from models import weighted_Roberta\n",
    "\n",
    "\n",
    "class XLM_Roberta:\n",
    "    def __init__(self,args):\n",
    "        # fix the random\n",
    "        random.seed(args['seed_val'])\n",
    "        np.random.seed(args['seed_val'])\n",
    "        torch.manual_seed(args['seed_val'])\n",
    "        torch.cuda.manual_seed_all(args['seed_val'])\n",
    "        \n",
    "        # set device\n",
    "        self.device = torch.device(args['device'])\n",
    "\n",
    "        self.weights=args['weights']\n",
    "        \n",
    "        # initiliase tokeniser\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case = True)\n",
    "\n",
    "        self.model_save_path = args['model_save_path']\n",
    "        self.name = args['name']\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Utility Functions -----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def encode(self,data,max_len):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sent in tqdm(data):\n",
    "            # use in-built tokeniser of Bert\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                            sent,\n",
    "                            add_special_tokens =True, # for [CLS] and [SEP]\n",
    "                            max_length = max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = True,\n",
    "#                             return_tensors = 'pt', # return pytorch tensors\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            # attention masks notify where padding has been added \n",
    "            # and where is the sentence\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            X_data = torch.tensor(input_ids)\n",
    "            attention_masks_data = torch.tensor(attention_masks)\n",
    "            \n",
    "        return [X_data,attention_masks_data]\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##------------------ Dataloader -----------------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def get_dataloader(self,samples, batch_size,is_train=False):\n",
    "        inputs,masks,labels = samples\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "#         inputs = torch.cat(inputs, dim=0)\n",
    "#         masks = torch.cat(masks, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # convert to dataset\n",
    "        data = TensorDataset(inputs,masks,labels)\n",
    "\n",
    "        if(is_train==False):\n",
    "            # use random sampler for training to shuffle\n",
    "            # train data\n",
    "            sampler = SequentialSampler(data)\n",
    "        else:\n",
    "            # order does not matter for validation as we just \n",
    "            # need the metrics\n",
    "            sampler = RandomSampler(data)  \n",
    "\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size,drop_last=True)\n",
    "\n",
    "        return dataloader\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Training Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def get_optimiser(self,learning_rate,model):\n",
    "        # using AdamW optimiser from transformers library\n",
    "        return AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    def get_scheduler(self,epochs,optimiser,train_dl):\n",
    "        total_steps = len(train_dl) * epochs\n",
    "        return get_linear_schedule_with_warmup(optimiser, \n",
    "                num_warmup_steps = 0, \n",
    "                num_training_steps = total_steps)\n",
    "    \n",
    "    def evalMetric(self, y_true, y_pred, prefix):\n",
    "        # calculate all the metrics and add prefix to them\n",
    "        # before saving in dictionary\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "        f1Score = f1_score(y_true, y_pred)\n",
    "        area_under_c = roc_auc_score(y_true, y_pred)\n",
    "        recallScore = recall_score(y_true, y_pred)\n",
    "        precisionScore = precision_score(y_true, y_pred)\n",
    "\n",
    "        nonhate_f1Score = f1_score(y_true, y_pred, pos_label=0)\n",
    "        non_recallScore = recall_score(y_true, y_pred, pos_label=0)\n",
    "        non_precisionScore = precision_score(y_true, y_pred, pos_label=0)\n",
    "        return {prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "            prefix+'f1Score': f1Score, prefix+'auc': area_under_c,\n",
    "            prefix+'precision': precisionScore, \n",
    "            prefix+'recall': recallScore, \n",
    "            prefix+'non_hatef1Score': nonhate_f1Score, \n",
    "            prefix+'non_recallScore': non_recallScore, \n",
    "            prefix+'non_precisionScore': non_precisionScore}\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##---------------- Different Train Loops --------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def evaluate(self,model,loader,which):\n",
    "        # to evaluate model on test and validation set\n",
    "\n",
    "        model.eval() # put model in eval mode\n",
    "\n",
    "        # maintain total loss to save in metrics\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(loader):\n",
    "            # separate input, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            with torch.no_grad(): # do not construct compute graph\n",
    "                outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            # output is always a tuple, thus we have to \n",
    "            # separate it manually\n",
    "            #loss = outputs[0]\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float).to(self.device))\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "\n",
    "            # add the current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # calculate true labels and convert it into numpy array\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "            \n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = self.evalMetric(y_true,y_pred,which+\"_\")\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_loss = total_eval_loss / len(loader)\n",
    "        # add it to the metric\n",
    "        metrics[which+'_avg_loss'] = avg_loss\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def run_train_loop(self,model,train_loader,optimiser,scheduler):\n",
    "\n",
    "        model.train() # put model in train mode\n",
    "\n",
    "        # maintain total loss to add to metric\n",
    "        total_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            # separate inputs, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            # Ref: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch#:~:text=In%20PyTorch%20%2C%20we%20need%20to,backward()%20call.\n",
    "            model.zero_grad()                \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "            # outputs is always returned as tuple\n",
    "            # Separate it manually\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float).to(self.device))\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "            \n",
    "            # calculate current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Back-propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # calculate true labels\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "            # clip gradient to prevent exploding gradient\n",
    "            # problems\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # gradient descent\n",
    "            optimiser.step()\n",
    "            \n",
    "            # schedule learning rate accordingly\n",
    "            scheduler.step()\n",
    "\n",
    "        # calculate avg loss \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # calculate metrics\n",
    "        train_metrics = self.evalMetric(y_true,y_pred,\"Train_\")\n",
    "        \n",
    "        # print results\n",
    "        print('avg_train_loss',avg_train_loss)\n",
    "        print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "        print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "        # add loss to metrics\n",
    "        train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "        return train_metrics\n",
    "    \n",
    "    \n",
    "    ##------------------------------------------------------------##\n",
    "    ##----------------- Main Train Loop --------------------------##\n",
    "    ##------------------------------------------------------------##\n",
    "    def train(self,model,data_loaders,optimiser,scheduler,epochs,save_model):\n",
    "        # save train stats per epoch\n",
    "        train_stats = []\n",
    "        train_loader,val_loader,test_loader = data_loaders\n",
    "        # maintain best mF1 Score to save best model\n",
    "        best_mf1Score=-1.0\n",
    "        for epoch_i in range(0, epochs):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            \n",
    "            print(\"\")\n",
    "            print('Training...')\n",
    "            # run trian loop\n",
    "            train_metrics = self.run_train_loop(model,train_loader,\n",
    "                                            optimiser,scheduler)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\") \n",
    "            # test on validation set\n",
    "            val_metrics = self.evaluate(model,val_loader,\"Val\")\n",
    "            \n",
    "            print(\"Validation Loss: \",val_metrics['Val_avg_loss'])\n",
    "            print(\"Validation Accuracy: \",val_metrics['Val_accuracy'])\n",
    "            \n",
    "            stats = {}\n",
    "\n",
    "            # save model where validation mF1Score is best\n",
    "            if(val_metrics['Val_mF1Score']>best_mf1Score):\n",
    "                best_mf1Score=val_metrics['Val_mF1Score']\n",
    "                if(save_model):\n",
    "                    torch.save(model.state_dict(), self.model_save_path+\n",
    "                        '/best_bert_'+self.name+'.pt')\n",
    "                # evaluate best model on test set\n",
    "                test_metrics = self.evaluate(model,test_loader,\"Test\")\n",
    "\n",
    "            stats['epoch']=epoch_i+1\n",
    "\n",
    "            # add train and val metrics of the epoch to \n",
    "            # same dictionary\n",
    "            stats.update(train_metrics)\n",
    "            stats.update(val_metrics)\n",
    "\n",
    "            train_stats.append(stats)\n",
    "\n",
    "        return train_stats,test_metrics\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------------- Main Pipeline ---------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run(self,args,df_train,df_val,df_test):\n",
    "        # get X and Y data points \n",
    "        X_train = df_train['Text'].values\n",
    "        Y_train = df_train['Label'].values\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "        X_val = df_val['Text'].values\n",
    "        Y_val = df_val['Label'].values\n",
    "        \n",
    "        # encode data\n",
    "        # returns list of data and attention masks\n",
    "        train_data = self.encode(X_train,args['max_len'])\n",
    "        val_data = self.encode(X_val,args['max_len'])\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "        \n",
    "        # add labels to data so that we can send them to\n",
    "        # dataloader function together\n",
    "        train_data.append(Y_train)\n",
    "        val_data.append(Y_val)\n",
    "        test_data.append(Y_test)\n",
    "        \n",
    "        # convert to dataloader\n",
    "        train_dl =self.get_dataloader(train_data,args['batch_size'],True)\n",
    "        val_dl =self.get_dataloader(val_data,args['batch_size'])                          \n",
    "        test_dl =self.get_dataloader(test_data,args['batch_size'])\n",
    "        \n",
    "        # intialise model\n",
    "        model = weighted_Roberta.from_pretrained(\n",
    "            'xlm-roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
    "            params=args['params'],\n",
    "        )\n",
    "        model.to(self.device)\n",
    "        \n",
    "        optimiser = self.get_optimiser(args['learning_rate'],model)\n",
    "        \n",
    "        scheduler = self.get_scheduler(args['epochs'],optimiser,train_dl)\n",
    "        \n",
    "        # Run train loop and evaluate on validation data set\n",
    "        # on each epoch. Store best model from all epochs \n",
    "        # (best mF1 Score on Val set) and evaluate it on\n",
    "        # test set\n",
    "        train_stats,train_metrics = self.train(model,[train_dl,val_dl,test_dl],\n",
    "                                optimiser,scheduler,args['epochs'],args['save_model'])\n",
    "        \n",
    "        return train_stats,train_metrics\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##-------------------- Other Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run_test(self,model,df_test,args):\n",
    "        # to evaluate test set on the final saved model\n",
    "        # to retrieve results if necessary\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "\n",
    "        test_data.append(Y_test)\n",
    "\n",
    "        test_dl =self.get_dataloader(test_data,32)\n",
    "\n",
    "        metrics = self.evaluate(model,test_dl,\"Test\")\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def load_model(self,path,args):\n",
    "        # load saved best model\n",
    "        saved_model = weighted_Roberta.from_pretrained(\n",
    "            'xlm-roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
    "            params=args['params'],\n",
    "        )\n",
    "        \n",
    "        saved_model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        return saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,isArabic):\n",
    "    \n",
    "    X = df['Text']\n",
    "    X_new=[]\n",
    "    if(isArabic):\n",
    "        prep = ArabertPreprocessor('bert-base-arabertv02')\n",
    "        for text in tqdm(X):\n",
    "            text = prep.preprocess(text)\n",
    "            X_new.append(text)\n",
    "    else:\n",
    "        processer = Data_Preprocessing()\n",
    "        for text in tqdm(X):\n",
    "            text= processer.removeEmojis(text)\n",
    "            text = processer.removeUrls(text)\n",
    "            text=processer.removeSpecialChar(text)\n",
    "            X_new.append(text)\n",
    "\n",
    "    df['Text']=X_new\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(args,data_path,index):\n",
    "    # read dataframes\n",
    "    df_test = pd.read_csv(data_path+'test_'+str(index)+'.csv')\n",
    "\n",
    "    # clean data\n",
    "    df_test=preprocess(df_test,args['isArabic'])\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot_output(model_path,data_path,obj,args):\n",
    "    saved_model=obj.load_model(model_path,args)\n",
    "    device = torch.device(args['device'])\n",
    "    saved_model=saved_model.to(device)\n",
    "    \n",
    "    all_metrics={}\n",
    "    \n",
    "    # preprocessing\n",
    "    for fold in [1,2,3,4,5]:\n",
    "        df = load_dataset(args,data_path,fold)\n",
    "\n",
    "        metrics = obj.run_test(saved_model,df,args)\n",
    "        \n",
    "        for key,value in metrics.items():\n",
    "            if(key not in all_metrics):\n",
    "                all_metrics[key]=value\n",
    "            else:\n",
    "                all_metrics[key]+=value\n",
    "    \n",
    "    for key,value in all_metrics.items():\n",
    "        all_metrics[key]/=5\n",
    "    \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1047/1047 [00:00<00:00, 8987.96it/s]\n",
      "100%|██████████| 1047/1047 [00:05<00:00, 188.65it/s]\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.48it/s]\n",
      "100%|██████████| 1047/1047 [00:00<00:00, 10565.99it/s]\n",
      "100%|██████████| 1047/1047 [00:05<00:00, 182.96it/s]\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.41it/s]\n",
      "100%|██████████| 1047/1047 [00:00<00:00, 9864.38it/s]\n",
      "100%|██████████| 1047/1047 [00:05<00:00, 196.01it/s]\n",
      "100%|██████████| 32/32 [00:05<00:00,  6.39it/s]\n",
      "100%|██████████| 1047/1047 [00:00<00:00, 9445.11it/s]\n",
      "100%|██████████| 1047/1047 [00:05<00:00, 194.38it/s]\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.48it/s]\n",
      "100%|██████████| 1052/1052 [00:00<00:00, 10026.88it/s]\n",
      "100%|██████████| 1052/1052 [00:05<00:00, 193.26it/s]\n",
      "100%|██████████| 32/32 [00:05<00:00,  6.37it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Data_Processed/Let-Mi/\"\n",
    "MODEL_PATH = \"Saved_Models/Let-Mi/all_but_one/best_bert_xlm_roberta_3_all.pt\"\n",
    "\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda:1',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': 'Saved_Models/Let-Mi/all_but_one/',\n",
    "        'isArabic': True,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "model = XLM_Roberta(model_args)\n",
    "\n",
    "metrics = one_shot_output(MODEL_PATH,DATA_PATH,model,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.6279296875,\n",
       " 'Test_mF1Score': 0.6101464578805273,\n",
       " 'Test_f1Score': 0.5269973945013741,\n",
       " 'Test_auc': 0.6252956771101191,\n",
       " 'Test_precision': 0.7059106567784128,\n",
       " 'Test_recall': 0.4206533390769679,\n",
       " 'Test_non_hatef1Score': 0.6932955212596809,\n",
       " 'Test_non_recallScore': 0.8299380151432703,\n",
       " 'Test_non_precisionScore': 0.5953525767327599,\n",
       " 'Test_avg_loss': 2.762599606066942}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1983/1983 [00:01<00:00, 1526.98it/s]\n",
      "100%|██████████| 1983/1983 [00:19<00:00, 102.77it/s]\n",
      "100%|██████████| 61/61 [00:13<00:00,  4.46it/s]\n",
      "100%|██████████| 1983/1983 [00:00<00:00, 2850.13it/s]\n",
      "100%|██████████| 1983/1983 [00:21<00:00, 94.17it/s] \n",
      "100%|██████████| 61/61 [00:10<00:00,  6.02it/s]\n",
      "100%|██████████| 1983/1983 [00:00<00:00, 3189.12it/s]\n",
      "100%|██████████| 1983/1983 [00:19<00:00, 103.08it/s]\n",
      "100%|██████████| 61/61 [00:10<00:00,  6.01it/s]\n",
      "100%|██████████| 1983/1983 [00:00<00:00, 3200.98it/s]\n",
      "100%|██████████| 1983/1983 [00:19<00:00, 103.74it/s]\n",
      "100%|██████████| 61/61 [00:09<00:00,  6.13it/s]\n",
      "100%|██████████| 1990/1990 [00:00<00:00, 3012.76it/s]\n",
      "100%|██████████| 1990/1990 [00:19<00:00, 102.67it/s]\n",
      "100%|██████████| 62/62 [00:10<00:00,  6.03it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Data_Processed/AMI-2020/\"\n",
    "MODEL_PATH = \"Saved_Models/AMI-2020/all_but_one/best_bert_xlm_roberta_2_all.pt\"\n",
    "\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "model = XLM_Roberta(model_args)\n",
    "\n",
    "metrics = one_shot_output(MODEL_PATH,DATA_PATH,model,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.5427898598625067,\n",
       " 'Test_mF1Score': 0.535056540137524,\n",
       " 'Test_f1Score': 0.4751968596074784,\n",
       " 'Test_auc': 0.5390935528334275,\n",
       " 'Test_precision': 0.5336107947288198,\n",
       " 'Test_recall': 0.4283610136925239,\n",
       " 'Test_non_hatef1Score': 0.5949162206675698,\n",
       " 'Test_non_recallScore': 0.6498260919743313,\n",
       " 'Test_non_precisionScore': 0.5485890780253498,\n",
       " 'Test_avg_loss': 2.458373664126454}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hindi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1236/1236 [00:00<00:00, 2083.93it/s]\n",
      "100%|██████████| 1236/1236 [00:07<00:00, 157.70it/s]\n",
      "100%|██████████| 38/38 [00:06<00:00,  5.92it/s]\n",
      "100%|██████████| 1236/1236 [00:00<00:00, 2404.25it/s]\n",
      "100%|██████████| 1236/1236 [00:07<00:00, 162.12it/s]\n",
      "100%|██████████| 38/38 [00:06<00:00,  6.03it/s]\n",
      "100%|██████████| 1236/1236 [00:00<00:00, 2598.96it/s]\n",
      "100%|██████████| 1236/1236 [00:07<00:00, 160.34it/s]\n",
      "100%|██████████| 38/38 [00:06<00:00,  6.02it/s]\n",
      "100%|██████████| 1236/1236 [00:00<00:00, 2665.70it/s]\n",
      "100%|██████████| 1236/1236 [00:07<00:00, 162.72it/s]\n",
      "100%|██████████| 38/38 [00:08<00:00,  4.51it/s]\n",
      "100%|██████████| 1237/1237 [00:00<00:00, 2511.37it/s]\n",
      "100%|██████████| 1237/1237 [00:07<00:00, 165.73it/s]\n",
      "100%|██████████| 38/38 [00:08<00:00,  4.51it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Data_Processed/Shared_Task_hin/\"\n",
    "MODEL_PATH = \"Saved_Models/Shared_Task_hin/all_but_one/best_bert_xlm_roberta_1_all.pt\"\n",
    "\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 4.5],\n",
    "        'save_model': True,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,4.5],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "model = XLM_Roberta(model_args)\n",
    "\n",
    "metrics = one_shot_output(MODEL_PATH,DATA_PATH,model,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.8106907894736842,\n",
       " 'Test_mF1Score': 0.7072033568661275,\n",
       " 'Test_f1Score': 0.5331543320860481,\n",
       " 'Test_auc': 0.6944916196142356,\n",
       " 'Test_precision': 0.5942272860851362,\n",
       " 'Test_recall': 0.4842924915714734,\n",
       " 'Test_non_hatef1Score': 0.8812523816462068,\n",
       " 'Test_non_recallScore': 0.9046907476569981,\n",
       " 'Test_non_precisionScore': 0.8590809995031726,\n",
       " 'Test_avg_loss': 1.4857775659153334}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bengali "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1194/1194 [00:00<00:00, 5265.84it/s]\n",
      "100%|██████████| 1194/1194 [00:06<00:00, 173.11it/s]\n",
      "100%|██████████| 37/37 [00:05<00:00,  6.39it/s]\n",
      "100%|██████████| 1194/1194 [00:00<00:00, 5073.55it/s]\n",
      "100%|██████████| 1194/1194 [00:06<00:00, 171.36it/s]\n",
      "100%|██████████| 37/37 [00:06<00:00,  6.08it/s]\n",
      "100%|██████████| 1194/1194 [00:00<00:00, 4088.03it/s]\n",
      "100%|██████████| 1194/1194 [00:07<00:00, 170.01it/s]\n",
      "100%|██████████| 37/37 [00:06<00:00,  5.95it/s]\n",
      "100%|██████████| 1194/1194 [00:00<00:00, 4230.58it/s]\n",
      "100%|██████████| 1194/1194 [00:07<00:00, 161.84it/s]\n",
      "100%|██████████| 37/37 [00:06<00:00,  6.07it/s]\n",
      "100%|██████████| 1195/1195 [00:00<00:00, 4843.44it/s]\n",
      "100%|██████████| 1195/1195 [00:07<00:00, 151.79it/s]\n",
      "100%|██████████| 37/37 [00:06<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Data_Processed/Shared_Task_iben/\"\n",
    "MODEL_PATH = \"Saved_Models/Shared_Task_iben/all_but_one/best_bert_xlm_roberta_5_all.pt\"\n",
    "\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 6.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,6.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "model = XLM_Roberta(model_args)\n",
    "\n",
    "metrics = one_shot_output(MODEL_PATH,DATA_PATH,model,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.8231418918918918,\n",
       " 'Test_mF1Score': 0.6108749057141973,\n",
       " 'Test_f1Score': 0.32349589068059503,\n",
       " 'Test_auc': 0.5935847165052681,\n",
       " 'Test_precision': 0.5534011023955905,\n",
       " 'Test_recall': 0.22943937299347628,\n",
       " 'Test_non_hatef1Score': 0.8982539207477995,\n",
       " 'Test_non_recallScore': 0.9577300600170597,\n",
       " 'Test_non_precisionScore': 0.8457708116643599,\n",
       " 'Test_avg_loss': 1.9669636692751098}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 660/660 [00:00<00:00, 2238.76it/s]\n",
      "100%|██████████| 660/660 [00:02<00:00, 294.08it/s] \n",
      "100%|██████████| 20/20 [00:03<00:00,  5.91it/s]\n",
      "100%|██████████| 660/660 [00:00<00:00, 2312.59it/s]\n",
      "100%|██████████| 660/660 [00:02<00:00, 277.34it/s] \n",
      "100%|██████████| 20/20 [00:03<00:00,  6.10it/s]\n",
      "100%|██████████| 660/660 [00:00<00:00, 2158.08it/s]\n",
      "100%|██████████| 660/660 [00:02<00:00, 290.40it/s] \n",
      "100%|██████████| 20/20 [00:03<00:00,  6.01it/s]\n",
      "100%|██████████| 660/660 [00:00<00:00, 1994.55it/s]\n",
      "100%|██████████| 660/660 [00:02<00:00, 269.08it/s] \n",
      "100%|██████████| 20/20 [00:03<00:00,  5.76it/s]\n",
      "100%|██████████| 667/667 [00:00<00:00, 2238.20it/s]\n",
      "100%|██████████| 667/667 [00:02<00:00, 254.80it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.97it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Data_Processed/AMI-Spanish/\"\n",
    "MODEL_PATH = \"Saved_Models/AMI-Spanish/all_but_one/best_bert_xlm_roberta_1_all.pt\"\n",
    "\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "model = XLM_Roberta(model_args)\n",
    "\n",
    "metrics = one_shot_output(MODEL_PATH,DATA_PATH,model,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.6125,\n",
       " 'Test_mF1Score': 0.605381253486432,\n",
       " 'Test_f1Score': 0.5526684318693685,\n",
       " 'Test_auc': 0.6119005304875215,\n",
       " 'Test_precision': 0.6490872005047186,\n",
       " 'Test_recall': 0.48141229613610437,\n",
       " 'Test_non_hatef1Score': 0.6580940751034957,\n",
       " 'Test_non_recallScore': 0.7423887648389385,\n",
       " 'Test_non_precisionScore': 0.591081139642356,\n",
       " 'Test_avg_loss': 1.244543731212616}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 3266/3266 [00:02<00:00, 1228.08it/s]\n",
      "100%|██████████| 3266/3266 [00:56<00:00, 57.41it/s] \n",
      "100%|██████████| 102/102 [00:18<00:00,  5.45it/s]\n",
      "100%|██████████| 3266/3266 [00:02<00:00, 1142.75it/s]\n",
      "100%|██████████| 3266/3266 [00:56<00:00, 58.20it/s] \n",
      "100%|██████████| 102/102 [00:16<00:00,  6.02it/s]\n",
      "100%|██████████| 3266/3266 [00:02<00:00, 1090.24it/s]\n",
      "100%|██████████| 3266/3266 [00:53<00:00, 61.08it/s] \n",
      "100%|██████████| 102/102 [00:16<00:00,  6.00it/s]\n",
      "100%|██████████| 3266/3266 [00:02<00:00, 1091.21it/s]\n",
      "100%|██████████| 3266/3266 [00:52<00:00, 62.43it/s] \n",
      "100%|██████████| 102/102 [00:16<00:00,  6.01it/s]\n",
      "100%|██████████| 3271/3271 [00:02<00:00, 1136.67it/s]\n",
      "100%|██████████| 3271/3271 [00:52<00:00, 61.72it/s] \n",
      "100%|██████████| 102/102 [00:21<00:00,  4.65it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Data_Processed/Shared_Task_eng/\"\n",
    "MODEL_PATH = \"Saved_Models/Shared_Task_eng/all_but_one/best_bert_xlm_roberta_4_all.pt\"\n",
    "\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 8.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,8.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "model = XLM_Roberta(model_args)\n",
    "\n",
    "metrics = one_shot_output(MODEL_PATH,DATA_PATH,model,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.834375,\n",
       " 'Test_mF1Score': 0.6479554382504329,\n",
       " 'Test_f1Score': 0.3917966071193363,\n",
       " 'Test_auc': 0.6267542292292257,\n",
       " 'Test_precision': 0.5371065108473771,\n",
       " 'Test_recall': 0.3093664150634787,\n",
       " 'Test_non_hatef1Score': 0.9041142693815294,\n",
       " 'Test_non_recallScore': 0.9441420433949729,\n",
       " 'Test_non_precisionScore': 0.8673895257756673,\n",
       " 'Test_avg_loss': 1.865554614262838}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'roberta.embeddings.position_ids', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1047/1047 [00:00<00:00, 2864.79it/s]\n",
      "100%|██████████| 1047/1047 [00:04<00:00, 212.06it/s]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.84it/s]\n",
      "100%|██████████| 1047/1047 [00:00<00:00, 2409.66it/s]\n",
      "100%|██████████| 1047/1047 [00:04<00:00, 210.95it/s]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.89it/s]\n",
      "100%|██████████| 1047/1047 [00:00<00:00, 2726.46it/s]\n",
      "100%|██████████| 1047/1047 [00:04<00:00, 211.29it/s]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.87it/s]\n",
      "100%|██████████| 1047/1047 [00:00<00:00, 2685.16it/s]\n",
      "100%|██████████| 1047/1047 [00:04<00:00, 212.33it/s]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.86it/s]\n",
      "100%|██████████| 1052/1052 [00:00<00:00, 2849.73it/s]\n",
      "100%|██████████| 1052/1052 [00:05<00:00, 207.45it/s]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.84it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Data_Processed/Let-Mi_Translated/\"\n",
    "MODEL_PATH = \"Saved_Models/Shared_Task_eng/best_bert_xlm_roberta_4_all.pt\"\n",
    "\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "model = XLM_Roberta(model_args)\n",
    "\n",
    "metrics = one_shot_output(MODEL_PATH,DATA_PATH,model,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.5322265625,\n",
       " 'Test_mF1Score': 0.39914104790283056,\n",
       " 'Test_f1Score': 0.11637593692584827,\n",
       " 'Test_auc': 0.5275126310640526,\n",
       " 'Test_precision': 0.8916105177729203,\n",
       " 'Test_recall': 0.06236579701203314,\n",
       " 'Test_non_hatef1Score': 0.681906158879813,\n",
       " 'Test_non_recallScore': 0.992659465116072,\n",
       " 'Test_non_precisionScore': 0.5193420391499137,\n",
       " 'Test_avg_loss': 2.5144972018897533}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'roberta.embeddings.position_ids', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1983/1983 [00:00<00:00, 3494.87it/s]\n",
      "100%|██████████| 1983/1983 [00:17<00:00, 110.70it/s]\n",
      "100%|██████████| 61/61 [00:16<00:00,  3.78it/s]\n",
      "100%|██████████| 1983/1983 [00:00<00:00, 3485.99it/s]\n",
      "100%|██████████| 1983/1983 [00:17<00:00, 110.73it/s]\n",
      "100%|██████████| 61/61 [00:13<00:00,  4.42it/s]\n",
      "100%|██████████| 1983/1983 [00:00<00:00, 3460.89it/s]\n",
      "100%|██████████| 1983/1983 [00:17<00:00, 111.51it/s]\n",
      "100%|██████████| 61/61 [00:16<00:00,  3.77it/s]\n",
      "100%|██████████| 1983/1983 [00:00<00:00, 3327.40it/s]\n",
      "100%|██████████| 1983/1983 [00:17<00:00, 111.06it/s]\n",
      "100%|██████████| 61/61 [00:16<00:00,  3.78it/s]\n",
      "100%|██████████| 1990/1990 [00:00<00:00, 3416.36it/s]\n",
      "100%|██████████| 1990/1990 [00:18<00:00, 110.47it/s]\n",
      "100%|██████████| 62/62 [00:14<00:00,  4.39it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Data_Processed/AMI-2020_Translated/\"\n",
    "MODEL_PATH = \"Saved_Models/Shared_Task_eng/best_bert_xlm_roberta_4_all.pt\"\n",
    "\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "model = XLM_Roberta(model_args)\n",
    "\n",
    "metrics = one_shot_output(MODEL_PATH,DATA_PATH,model,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.533890798519302,\n",
       " 'Test_mF1Score': 0.5103910317290101,\n",
       " 'Test_f1Score': 0.4032377253327931,\n",
       " 'Test_auc': 0.5274122455562283,\n",
       " 'Test_precision': 0.5298040560388801,\n",
       " 'Test_recall': 0.3256028353734132,\n",
       " 'Test_non_hatef1Score': 0.617544338125227,\n",
       " 'Test_non_recallScore': 0.7292216557390432,\n",
       " 'Test_non_precisionScore': 0.535567571087997,\n",
       " 'Test_avg_loss': 2.253825451683456}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'roberta.embeddings.position_ids', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 660/660 [00:00<00:00, 2329.45it/s]\n",
      "100%|██████████| 660/660 [00:02<00:00, 323.53it/s] \n",
      "100%|██████████| 20/20 [00:05<00:00,  3.79it/s]\n",
      "100%|██████████| 660/660 [00:00<00:00, 2316.29it/s]\n",
      "100%|██████████| 660/660 [00:02<00:00, 307.60it/s] \n",
      "100%|██████████| 20/20 [00:05<00:00,  3.78it/s]\n",
      "100%|██████████| 660/660 [00:00<00:00, 2238.41it/s]\n",
      "100%|██████████| 660/660 [00:02<00:00, 323.47it/s] \n",
      "100%|██████████| 20/20 [00:05<00:00,  3.75it/s]\n",
      "100%|██████████| 660/660 [00:00<00:00, 2157.89it/s]\n",
      "100%|██████████| 660/660 [00:02<00:00, 323.58it/s] \n",
      "100%|██████████| 20/20 [00:05<00:00,  3.79it/s]\n",
      "100%|██████████| 667/667 [00:00<00:00, 2246.57it/s]\n",
      "100%|██████████| 667/667 [00:02<00:00, 314.58it/s] \n",
      "100%|██████████| 20/20 [00:05<00:00,  3.78it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Data_Processed/AMI-Spanish_Translated/\"\n",
    "MODEL_PATH = \"Saved_Models/Shared_Task_eng/best_bert_xlm_roberta_4_all.pt\"\n",
    "\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "model = XLM_Roberta(model_args)\n",
    "\n",
    "metrics = one_shot_output(MODEL_PATH,DATA_PATH,model,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.6743750000000001,\n",
       " 'Test_mF1Score': 0.6701567084088194,\n",
       " 'Test_f1Score': 0.7067346932684204,\n",
       " 'Test_auc': 0.674585344505144,\n",
       " 'Test_precision': 0.64112234595706,\n",
       " 'Test_recall': 0.7876304193398093,\n",
       " 'Test_non_hatef1Score': 0.6335787235492185,\n",
       " 'Test_non_recallScore': 0.5615402696704787,\n",
       " 'Test_non_precisionScore': 0.7276090793114998,\n",
       " 'Test_avg_loss': 1.3168907338380813}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Learning\n",
    "\n",
    "Here we are checking the performance of the model trained on the English Dataset on other Datasets and their translated versions with finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLM RoBERTa Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import random\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Tokeniser\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# Utility\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Optimiser\n",
    "from transformers import AdamW\n",
    "\n",
    "# Model\n",
    "\n",
    "import torch.nn as nn\n",
    "from models import weighted_Roberta\n",
    "\n",
    "\n",
    "class XLM_Roberta_fewShot:\n",
    "    def __init__(self,args):\n",
    "        # fix the random\n",
    "        random.seed(args['seed_val'])\n",
    "        np.random.seed(args['seed_val'])\n",
    "        torch.manual_seed(args['seed_val'])\n",
    "        torch.cuda.manual_seed_all(args['seed_val'])\n",
    "        \n",
    "        # set device\n",
    "        self.device = torch.device(args['device'])\n",
    "\n",
    "        self.weights=args['weights']\n",
    "        \n",
    "        # initiliase tokeniser\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case = True)\n",
    "\n",
    "        self.model_save_path = args['model_save_path']\n",
    "        self.name = args['name']\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Utility Functions -----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def encode(self,data,max_len):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sent in tqdm(data):\n",
    "            # use in-built tokeniser of Bert\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                            sent,\n",
    "                            add_special_tokens =True, # for [CLS] and [SEP]\n",
    "                            max_length = max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = True,\n",
    "#                             return_tensors = 'pt', # return pytorch tensors\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            # attention masks notify where padding has been added \n",
    "            # and where is the sentence\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            X_data = torch.tensor(input_ids)\n",
    "            attention_masks_data = torch.tensor(attention_masks)\n",
    "            \n",
    "        return [X_data,attention_masks_data]\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##------------------ Dataloader -----------------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def get_dataloader(self,samples, batch_size,is_train=False):\n",
    "        inputs,masks,labels = samples\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "#         inputs = torch.cat(inputs, dim=0)\n",
    "#         masks = torch.cat(masks, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # convert to dataset\n",
    "        data = TensorDataset(inputs,masks,labels)\n",
    "\n",
    "        if(is_train==False):\n",
    "            # use random sampler for training to shuffle\n",
    "            # train data\n",
    "            sampler = SequentialSampler(data)\n",
    "        else:\n",
    "            # order does not matter for validation as we just \n",
    "            # need the metrics\n",
    "            sampler = RandomSampler(data)  \n",
    "\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size,drop_last=True)\n",
    "\n",
    "        return dataloader\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Training Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def get_optimiser(self,learning_rate,model):\n",
    "        # using AdamW optimiser from transformers library\n",
    "        return AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    def get_scheduler(self,epochs,optimiser,train_dl):\n",
    "        total_steps = len(train_dl) * epochs\n",
    "        return get_linear_schedule_with_warmup(optimiser, \n",
    "                num_warmup_steps = 0, \n",
    "                num_training_steps = total_steps)\n",
    "    \n",
    "    def evalMetric(self, y_true, y_pred, prefix):\n",
    "        # calculate all the metrics and add prefix to them\n",
    "        # before saving in dictionary\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "        f1Score = f1_score(y_true, y_pred)\n",
    "        area_under_c = roc_auc_score(y_true, y_pred)\n",
    "        recallScore = recall_score(y_true, y_pred)\n",
    "        precisionScore = precision_score(y_true, y_pred)\n",
    "\n",
    "        nonhate_f1Score = f1_score(y_true, y_pred, pos_label=0)\n",
    "        non_recallScore = recall_score(y_true, y_pred, pos_label=0)\n",
    "        non_precisionScore = precision_score(y_true, y_pred, pos_label=0)\n",
    "        return {prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "            prefix+'f1Score': f1Score, prefix+'auc': area_under_c,\n",
    "            prefix+'precision': precisionScore, \n",
    "            prefix+'recall': recallScore, \n",
    "            prefix+'non_hatef1Score': nonhate_f1Score, \n",
    "            prefix+'non_recallScore': non_recallScore, \n",
    "            prefix+'non_precisionScore': non_precisionScore}\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##---------------- Different Train Loops --------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def evaluate(self,model,loader,which):\n",
    "        # to evaluate model on test and validation set\n",
    "\n",
    "        model.eval() # put model in eval mode\n",
    "\n",
    "        # maintain total loss to save in metrics\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(loader):\n",
    "            # separate input, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            with torch.no_grad(): # do not construct compute graph\n",
    "                outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            # output is always a tuple, thus we have to \n",
    "            # separate it manually\n",
    "            #loss = outputs[0]\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float).to(self.device))\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "\n",
    "            # add the current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # calculate true labels and convert it into numpy array\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "            \n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = self.evalMetric(y_true,y_pred,which+\"_\")\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_loss = total_eval_loss / len(loader)\n",
    "        # add it to the metric\n",
    "        metrics[which+'_avg_loss'] = avg_loss\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def run_train_loop(self,model,train_loader,optimiser,scheduler):\n",
    "\n",
    "        model.train() # put model in train mode\n",
    "\n",
    "        # maintain total loss to add to metric\n",
    "        total_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            # separate inputs, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            # Ref: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch#:~:text=In%20PyTorch%20%2C%20we%20need%20to,backward()%20call.\n",
    "            model.zero_grad()                \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "            # outputs is always returned as tuple\n",
    "            # Separate it manually\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float).to(self.device))\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "            \n",
    "            # calculate current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Back-propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # calculate true labels\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "            # clip gradient to prevent exploding gradient\n",
    "            # problems\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # gradient descent\n",
    "            optimiser.step()\n",
    "            \n",
    "            # schedule learning rate accordingly\n",
    "            scheduler.step()\n",
    "\n",
    "        # calculate avg loss \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # calculate metrics\n",
    "        train_metrics = self.evalMetric(y_true,y_pred,\"Train_\")\n",
    "        \n",
    "        # print results\n",
    "        print('avg_train_loss',avg_train_loss)\n",
    "        print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "        print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "        # add loss to metrics\n",
    "        train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "        return train_metrics\n",
    "    \n",
    "    \n",
    "    ##------------------------------------------------------------##\n",
    "    ##----------------- Main Train Loop --------------------------##\n",
    "    ##------------------------------------------------------------##\n",
    "    def train(self,model,data_loaders,optimiser,scheduler,epochs,save_model):\n",
    "        # save train stats per epoch\n",
    "        train_stats = []\n",
    "        train_loader,val_loader,test_loader = data_loaders\n",
    "        # maintain best mF1 Score to save best model\n",
    "        best_mf1Score=-1.0\n",
    "        for epoch_i in range(0, epochs):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            \n",
    "            print(\"\")\n",
    "            print('Training...')\n",
    "            # run trian loop\n",
    "            train_metrics = self.run_train_loop(model,train_loader,\n",
    "                                            optimiser,scheduler)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\") \n",
    "            # test on validation set\n",
    "            val_metrics = self.evaluate(model,val_loader,\"Val\")\n",
    "            \n",
    "            print(\"Validation Loss: \",val_metrics['Val_avg_loss'])\n",
    "            print(\"Validation Accuracy: \",val_metrics['Val_accuracy'])\n",
    "            \n",
    "            stats = {}\n",
    "\n",
    "            # save model where validation mF1Score is best\n",
    "            if(val_metrics['Val_mF1Score']>best_mf1Score):\n",
    "                best_mf1Score=val_metrics['Val_mF1Score']\n",
    "                if(save_model):\n",
    "                    torch.save(model.state_dict(), self.model_save_path+\n",
    "                        '/best_bert_'+self.name+'.pt')\n",
    "                # evaluate best model on test set\n",
    "                test_metrics = self.evaluate(model,test_loader,\"Test\")\n",
    "\n",
    "            stats['epoch']=epoch_i+1\n",
    "\n",
    "            # add train and val metrics of the epoch to \n",
    "            # same dictionary\n",
    "            stats.update(train_metrics)\n",
    "            stats.update(val_metrics)\n",
    "\n",
    "            train_stats.append(stats)\n",
    "\n",
    "        return train_stats,test_metrics\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------------- Main Pipeline ---------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run(self,args,df_train,df_val,df_test):\n",
    "        # get X and Y data points \n",
    "        X_train = df_train['Text'].values\n",
    "        Y_train = df_train['Label'].values\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "        X_val = df_val['Text'].values\n",
    "        Y_val = df_val['Label'].values\n",
    "        \n",
    "        # encode data\n",
    "        # returns list of data and attention masks\n",
    "        train_data = self.encode(X_train,args['max_len'])\n",
    "        val_data = self.encode(X_val,args['max_len'])\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "        \n",
    "        # add labels to data so that we can send them to\n",
    "        # dataloader function together\n",
    "        train_data.append(Y_train)\n",
    "        val_data.append(Y_val)\n",
    "        test_data.append(Y_test)\n",
    "        \n",
    "        # convert to dataloader\n",
    "        train_dl =self.get_dataloader(train_data,args['batch_size'],True)\n",
    "        val_dl =self.get_dataloader(val_data,args['batch_size'])                          \n",
    "        test_dl =self.get_dataloader(test_data,args['batch_size'])\n",
    "        \n",
    "        # intialise model\n",
    "#         model = weighted_Roberta.from_pretrained(\n",
    "#             'xlm-roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#             num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
    "#             params=args['params'],\n",
    "#         )\n",
    "        model = self.load_model(args['model_path'],args)\n",
    "        model.to(self.device)\n",
    "        \n",
    "        optimiser = self.get_optimiser(args['learning_rate'],model)\n",
    "        \n",
    "        scheduler = self.get_scheduler(args['epochs'],optimiser,train_dl)\n",
    "        \n",
    "        # Run train loop and evaluate on validation data set\n",
    "        # on each epoch. Store best model from all epochs \n",
    "        # (best mF1 Score on Val set) and evaluate it on\n",
    "        # test set\n",
    "        train_stats,train_metrics = self.train(model,[train_dl,val_dl,test_dl],\n",
    "                                optimiser,scheduler,args['epochs'],args['save_model'])\n",
    "        \n",
    "        return train_stats,train_metrics\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##-------------------- Other Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run_test(self,model,df_test,args):\n",
    "        # to evaluate test set on the final saved model\n",
    "        # to retrieve results if necessary\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "\n",
    "        test_data.append(Y_test)\n",
    "\n",
    "        test_dl =self.get_dataloader(test_data,32)\n",
    "\n",
    "        metrics = self.evaluate(model,test_dl,\"Test\")\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def load_model(self,path,args):\n",
    "        # load saved best model\n",
    "        saved_model = weighted_Roberta.from_pretrained(\n",
    "            'xlm-roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
    "            params=args['params'],\n",
    "        )\n",
    "        \n",
    "        saved_model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        return saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(args,index):\n",
    "    # initialise constants \n",
    "    path = args['data_path']\n",
    "    # read dataframes\n",
    "    df_train = pd.read_csv(path+'train_'+str(index)+'.csv')\n",
    "    df_val = pd.read_csv(path+'val_'+str(index)+'.csv')\n",
    "    df_test = pd.read_csv(path+'test_'+str(index)+'.csv')\n",
    "\n",
    "    # clean data\n",
    "    df_train=preprocess(df_train,args['isArabic'])\n",
    "    df_val=preprocess(df_val,args['isArabic'])\n",
    "    df_test=preprocess(df_test,args['isArabic'])\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,isArabic):\n",
    "    \n",
    "    X = df['Text']\n",
    "    X_new=[]\n",
    "    if(isArabic):\n",
    "        prep = ArabertPreprocessor('bert-base-arabertv02')\n",
    "        for text in tqdm(X):\n",
    "            text = prep.preprocess(text)\n",
    "            X_new.append(text)\n",
    "    else:\n",
    "        processer = Data_Preprocessing()\n",
    "        for text in tqdm(X):\n",
    "            text= processer.removeEmojis(text)\n",
    "            text = processer.removeUrls(text)\n",
    "            text=processer.removeSpecialChar(text)\n",
    "            X_new.append(text)\n",
    "\n",
    "    df['Text']=X_new\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(path,metrics,which):\n",
    "    df = pd.DataFrame(metrics)\n",
    "    df.to_csv(path+\"_\"+which+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed_val=42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, index,all_test_metrics,model_args):\n",
    "    model_name = args['model_name']\n",
    "    model_args['name']=model_name+'_'+str(index)+'_all'\n",
    "    print(\"\\tInitialising Model....\")\n",
    "    model = XLM_Roberta_fewShot(model_args)\n",
    "    print(\"\\tLoading Dataset....\")\n",
    "    df_train, df_val, df_test = load_dataset(args,index)\n",
    "    print(\"\\tTraining Starts....\")\n",
    "    train_metrics, test_metrics = model.run(model_args, \n",
    "                    df_train, df_val, df_test)\n",
    "\n",
    "    # Save train metrics after generating path\n",
    "    res_path=args['res_base_path']+model_name+'_'+model_args['name']\n",
    "    save_metrics(res_path,train_metrics,\"train\")\n",
    "    \n",
    "    all_test_metrics.append(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Run Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args,model_args):\n",
    "    all_test_metrics=[]\n",
    "    \n",
    "    for fold in [1, 2, 3, 4, 5]:\n",
    "        print(\"Fold: \",fold)\n",
    "        fix_random()\n",
    "        train(args,fold,all_test_metrics,model_args)\n",
    "        print(\"Saving Test Metrics....\")\n",
    "        save_metrics(args['res_base_path']+args['model_name']+'_all',\n",
    "                     all_test_metrics,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3670/3670 [00:00<00:00, 9679.43it/s]\n",
      "100%|██████████| 523/523 [00:00<00:00, 9002.94it/s]\n",
      "100%|██████████| 1047/1047 [00:00<00:00, 9759.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3670/3670 [01:07<00:00, 54.71it/s] \n",
      "100%|██████████| 523/523 [00:01<00:00, 368.54it/s] \n",
      "100%|██████████| 1047/1047 [00:05<00:00, 182.42it/s]\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'roberta.embeddings.position_ids', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [01:22<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5844816608175505\n",
      "train_f1Score 0.7332382310984308\n",
      "train_accuracy 0.7448144104803494\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:02<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.38593538667146976\n",
      "Validation Accuracy:  0.8403846153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:04<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [01:22<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.37574176873816545\n",
      "train_f1Score 0.8530566461020752\n",
      "train_accuracy 0.8569868995633187\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:02<00:00, 27.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.45340895171348866\n",
      "Validation Accuracy:  0.8461538461538461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:04<00:00, 27.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 200/458 [00:35<00:45,  5.71it/s]"
     ]
    }
   ],
   "source": [
    "run_args={\n",
    "    'model_name':'few_shot_xlm',\n",
    "    'data_path':'Data_Processed/Let-Mi/',\n",
    "    'train_cnt':'all',\n",
    "    'res_base_path': 'Results/Let-Mi/fewShot/',\n",
    "    'model_save_path': 'Saved_Models/Let-Mi/',\n",
    "    'isArabic': True,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_path': 'Saved_Models/Let-Mi/all_but_one/best_bert_xlm_roberta_3_all.pt',\n",
    "        'isArabic': True,\n",
    "        'model_save_path': '',\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'few_shot_xlm',\n",
    "    'data_path':'Data_Processed/AMI-Spanish/',\n",
    "    'train_cnt':'all',\n",
    "    'res_base_path': 'Results/AMI-Spanish/fewShot/',\n",
    "    'model_save_path': 'Saved_Models/AMI-Spanish/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_path': 'Saved_Models/AMI-Spanish/all_but_one/best_bert_xlm_roberta_1_all.pt',\n",
    "        'isArabic': False,\n",
    "        'model_save_path': '',\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':False,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':False,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'few_shot_xlm',\n",
    "    'data_path':'Data_Processed/Shared_Task_hin/',\n",
    "    'train_cnt':'all',\n",
    "    'res_base_path': 'Results/Shared_Task_hin/fewShot/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_hin/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 4.5],\n",
    "        'save_model': False,\n",
    "        'model_path': 'Saved_Models/Shared_Task_hin/all_but_one/best_bert_xlm_roberta_1_all.pt',\n",
    "        'isArabic': False,\n",
    "        'model_save_path': '',\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':False,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':False,\n",
    "            'weights':[1.0,4.5],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Less data points few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_part(train_cnt,args,index,seed):\n",
    "    # initialise constants \n",
    "    path = args['data_path']\n",
    "    # read dataframes\n",
    "    df_train = pd.read_csv(path+'train_'+str(index)+'.csv')\n",
    "    df_val = pd.read_csv(path+'val_'+str(index)+'.csv')\n",
    "    df_test = pd.read_csv(path+'test_'+str(index)+'.csv')\n",
    "    \n",
    "    # split train into hate and non-hate and take train_cnt\n",
    "    # samples of each\n",
    "    df_train_hate = df_train[df_train['Label'] == 1].sample(train_cnt,random_state=seed)\n",
    "    df_train_non_hate = df_train[df_train['Label'] == 0].sample(train_cnt,random_state=seed)\n",
    "    # concatenate hate and non_hate\n",
    "    df_train = pd.concat([df_train_hate, df_train_non_hate])\n",
    "    # shuffle the train data\n",
    "    df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # clean data\n",
    "    df_train=preprocess(df_train,args['isArabic'])\n",
    "    df_val=preprocess(df_val,args['isArabic'])\n",
    "    df_test=preprocess(df_test,args['isArabic'])\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part(args,train_cnt,run,index,all_test_metrics,model_args,seed):\n",
    "    model_name = args['model_name']\n",
    "    model_args['name']=model_name+'_'+str(index)+'_'+str(train_cnt)+'_'+str(run)\n",
    "    print(\"\\tInitialising Model....\")\n",
    "    model = XLM_Roberta_fewShot(model_args)\n",
    "    print(\"\\tLoading Dataset....\")\n",
    "    df_train, df_val, df_test = load_dataset_part(train_cnt,args,index,seed)\n",
    "    print(\"\\tTraining Starts....\")\n",
    "    train_metrics, test_metrics = model.run(model_args, \n",
    "                    df_train, df_val, df_test)\n",
    "\n",
    "    # Save train metrics after generating path\n",
    "    res_path=args['res_base_path']+model_name+'_'+model_args['name']\n",
    "    save_metrics(res_path,train_metrics,\"train\")\n",
    "    \n",
    "    all_test_metrics.append(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_part(run_args,model_args,train_cnt):\n",
    "    all_test_metrics=[]\n",
    "    seeds = [42,43,44]\n",
    "    for fold in [1, 2, 3, 4, 5]:\n",
    "        print(\"Fold: \",fold)\n",
    "        for run in [1,2,3]:\n",
    "            print(\"Run: \",run)\n",
    "            fix_random()\n",
    "            train_part(run_args,train_cnt,run,fold,all_test_metrics,model_args,seeds[run-1])\n",
    "            print(\"Saving Test Metrics....\")\n",
    "            save_metrics(run_args['res_base_path']+run_args['model_name']+\n",
    "                         '_'+str(train_cnt),all_test_metrics,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic few data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cnt:  256\n",
      "Fold:  1\n",
      "Run:  1\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:00<00:00, 9484.89it/s]\n",
      "100%|██████████| 523/523 [00:00<00:00, 10746.25it/s]\n",
      "100%|██████████| 1047/1047 [00:00<00:00, 7362.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:01<00:00, 297.33it/s] \n",
      "100%|██████████| 523/523 [00:01<00:00, 317.57it/s] \n",
      "100%|██████████| 1047/1047 [00:06<00:00, 157.69it/s]\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:14<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 1.091972861904651\n",
      "train_f1Score 0.663917525773196\n",
      "train_accuracy 0.681640625\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:03<00:00, 20.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.5773658773073783\n",
      "Validation Accuracy:  0.7096153846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:06<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:14<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.47084189497400075\n",
      "train_f1Score 0.7809917355371901\n",
      "train_accuracy 0.79296875\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:03<00:00, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.5416358647438196\n",
      "Validation Accuracy:  0.7865384615384615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:06<00:00, 20.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:14<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.3281706521374872\n",
      "train_f1Score 0.8787276341948309\n",
      "train_accuracy 0.880859375\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:03<00:00, 21.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.8024578670397974\n",
      "Validation Accuracy:  0.775\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:14<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.25167675940247136\n",
      "train_f1Score 0.9180952380952381\n",
      "train_accuracy 0.916015625\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:03<00:00, 19.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.1313557712871654\n",
      "Validation Accuracy:  0.7634615384615384\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:14<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.14241571135016784\n",
      "train_f1Score 0.9725490196078432\n",
      "train_accuracy 0.97265625\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:03<00:00, 20.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.4868718307120545\n",
      "Validation Accuracy:  0.7884615384615384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:06<00:00, 20.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:14<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.16598336707761518\n",
      "train_f1Score 0.9651162790697675\n",
      "train_accuracy 0.96484375\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:03<00:00, 20.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.6880386180089912\n",
      "Validation Accuracy:  0.7615384615384615\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:14<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.10756652033956016\n",
      "train_f1Score 0.9788053949903661\n",
      "train_accuracy 0.978515625\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:03<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.4385452045879972\n",
      "Validation Accuracy:  0.7769230769230769\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 21/64 [00:04<00:09,  4.52it/s]"
     ]
    }
   ],
   "source": [
    "run_args={\n",
    "    'model_name':'few_shot_xlm',\n",
    "    'data_path':'Data_Processed/Let-Mi/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Let-Mi/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Let-Mi/',\n",
    "    'isArabic': True,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda:1',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_path': 'Saved_Models/Let-Mi/all_but_one/best_bert_xlm_roberta_3_all.pt',\n",
    "        'isArabic': True,\n",
    "        'model_save_path': '',\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "for train_cnt in [256,512]:\n",
    "    print(\"Train cnt: \",train_cnt)\n",
    "    run_args['train_cnt']=train_cnt\n",
    "    run_part(run_args,model_args,train_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian few Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'few_shot_xlm',\n",
    "    'data_path':'Data_Processed/AMI-2020/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/AMI-2020/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/AMI-2020/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda:1',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_path': 'Saved_Models/AMI-2020/all_but_one/best_bert_xlm_roberta_2_all.pt',\n",
    "        'isArabic': False,\n",
    "        'model_save_path': '',\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':False,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':False,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "for train_cnt in [32,64,128,256,512]:\n",
    "    print(\"Train cnt: \",train_cnt)\n",
    "    run_args['train_cnt']=train_cnt\n",
    "    run_part(run_args,model_args,train_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish Few Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'few_shot_xlm',\n",
    "    'data_path':'Data_Processed/AMI-Spanish/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/AMI-Spanish/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/AMI-Spanish/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda:1',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_path': 'Saved_Models/AMI-2020/all_but_one/best_bert_xlm_roberta_2_all.pt',\n",
    "        'isArabic': False,\n",
    "        'model_save_path': '',\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':False,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':False,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "for train_cnt in [128,256,512]:\n",
    "    print(\"Train cnt: \",train_cnt)\n",
    "    run_args['train_cnt']=train_cnt\n",
    "    run_part(run_args,model_args,train_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindi Few Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'few_shot_xlm',\n",
    "    'data_path':'Data_Processed/Shared_Task_hin/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Shared_Task_hin/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_hin/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda:1',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_path': 'Saved_Models/Shared_Task_hin/all_but_one/best_bert_xlm_roberta_1_all.pt',\n",
    "        'isArabic': False,\n",
    "        'model_save_path': '',\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':False,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':False,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "for train_cnt in [32,64,128,256,512]:\n",
    "    print(\"Train cnt: \",train_cnt)\n",
    "    run_args['train_cnt']=train_cnt\n",
    "    run_part(run_args,model_args,train_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bengali Few Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'few_shot_xlm',\n",
    "    'data_path':'Data_Processed/Shared_Task_iben/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Shared_Task_iben/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_iben/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda:1',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_path': 'Saved_Models/Shared_Task_iben/all_but_one/best_bert_xlm_roberta_5_all.pt',\n",
    "        'isArabic': False,\n",
    "        'model_save_path': '',\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':False,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':False,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "for train_cnt in [64,128,256,512]:\n",
    "    print(\"Train cnt: \",train_cnt)\n",
    "    run_args['train_cnt']=train_cnt\n",
    "    run_part(run_args,model_args,train_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'few_shot_xlm',\n",
    "    'data_path':'Data_Processed/Shared_Task_eng/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Shared_Task_eng/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_eng/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda:1',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_path': 'Saved_Models/Shared_Task_eng/all_but_one/best_bert_xlm_roberta_4_all.pt',\n",
    "        'isArabic': False,\n",
    "        'model_save_path': '',\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':False,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':False,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "for train_cnt in [32,64,128,256,512]:\n",
    "    print(\"Train cnt: \",train_cnt)\n",
    "    run_args['train_cnt']=train_cnt\n",
    "    run_part(run_args,model_args,train_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
