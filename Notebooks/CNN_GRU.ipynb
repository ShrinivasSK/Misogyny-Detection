{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uW_ieSgOdaUx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ubMyN3zJcJUU"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"Data/Shared Task/\"\n",
    "TRAIN_FILE_NAME = \"eng/trac2_eng_train.csv\"\n",
    "VAL_FILE_NAME = \"eng/trac2_eng_dev.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VDhEkEhYcbTb"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_PATH = \"Embeddings/wiki.multi.en.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kdG7vwucvnv"
   },
   "source": [
    "## Load MUSE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p-Hn9qxWcs32"
   },
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_E6m4CNdHIZ"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "J1fXRZR5cy2u"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH+TRAIN_FILE_NAME)\n",
    "df_val = pd.read_csv(DATA_PATH+VAL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "IQbTQ4jCeeZi",
    "outputId": "0ab9def8-7f37-4d86-9e19-43a3b9fba34d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sub-task A</th>\n",
       "      <th>Sub-task B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>C4.743</td>\n",
       "      <td>Kabir Singh' inspired man kills girl\\n&lt;http://...</td>\n",
       "      <td>CAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>C33.384</td>\n",
       "      <td>I m from pakistan  \\nKoi pakistan se girl ho t...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>C7.2173</td>\n",
       "      <td>You Reated 9/10  \\nWow üòç  \\nI Reated 10 / 10</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>C4.2034</td>\n",
       "      <td>There is no one character of doctor like kabir...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>C4.2182</td>\n",
       "      <td>I dunno, but I had this gut feeling inside of ...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>C7.2288</td>\n",
       "      <td>I read a blog post of India Today they were tr...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>C59.1200</td>\n",
       "      <td>great show</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>C7.2595.5</td>\n",
       "      <td>LKMKA, L - Leftist.these should be kich out</td>\n",
       "      <td>OAG</td>\n",
       "      <td>GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>C4.1160</td>\n",
       "      <td>sir, please don't stop making video</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>C25.583.1</td>\n",
       "      <td>Yes so ppl like u can take dowry with an open ...</td>\n",
       "      <td>CAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                               Text Sub-task A  \\\n",
       "4120     C4.743  Kabir Singh' inspired man kills girl\\n<http://...        CAG   \n",
       "1562    C33.384  I m from pakistan  \\nKoi pakistan se girl ho t...        NAG   \n",
       "2046    C7.2173       You Reated 9/10  \\nWow üòç  \\nI Reated 10 / 10        NAG   \n",
       "1547    C4.2034  There is no one character of doctor like kabir...        NAG   \n",
       "882     C4.2182  I dunno, but I had this gut feeling inside of ...        NAG   \n",
       "396     C7.2288  I read a blog post of India Today they were tr...        OAG   \n",
       "3883   C59.1200                                         great show        NAG   \n",
       "1586  C7.2595.5        LKMKA, L - Leftist.these should be kich out        OAG   \n",
       "3006    C4.1160                sir, please don't stop making video        NAG   \n",
       "1793  C25.583.1  Yes so ppl like u can take dowry with an open ...        CAG   \n",
       "\n",
       "     Sub-task B  \n",
       "4120       NGEN  \n",
       "1562       NGEN  \n",
       "2046       NGEN  \n",
       "1547       NGEN  \n",
       "882        NGEN  \n",
       "396        NGEN  \n",
       "3883       NGEN  \n",
       "1586        GEN  \n",
       "3006       NGEN  \n",
       "1793       NGEN  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3954, 309)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train[df_train['Sub-task B']=='NGEN']),len(df_train[df_train['Sub-task B']=='GEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCZFesPzdqg6"
   },
   "source": [
    "## Encode Data\n",
    "- Encode text to embeddings\n",
    "- Encode labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QVrAC34cdYoM"
   },
   "outputs": [],
   "source": [
    "vector,id2word,word2id = load_vec(EMBEDDING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D50aVKFqdz4x"
   },
   "outputs": [],
   "source": [
    "def encode_data(df,word2id,textColName,labelColName):\n",
    "        max_len=0\n",
    "        for index,row in tqdm(df.iterrows(),total=len(df)):\n",
    "            \n",
    "            if(max_len<len(row[textColName].split(' '))):\n",
    "                max_len=len(row[textColName].split(' '))\n",
    "        \n",
    "        new_data=[]\n",
    "        \n",
    "        \n",
    "        for index,row in df.iterrows():\n",
    "            list_token_id=[]\n",
    "            words=row[textColName].split(' ')\n",
    "            for word in words:\n",
    "                try:\n",
    "                    index=word2id[word]\n",
    "                except KeyError:\n",
    "                    index=len(list(word2id.keys()))\n",
    "                list_token_id.append(index)\n",
    "            with_padding_text=list(pad(list_token_id, max_len, len(list(word2id.keys()))+1))\n",
    "            new_data.append([with_padding_text,row[labelColName],row[textColName]])\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-4PYVPw1evtV"
   },
   "outputs": [],
   "source": [
    "from itertools import chain, repeat, islice\n",
    "def pad_infinite(iterable, padding=None):\n",
    "       return chain(iterable, repeat(padding))\n",
    "\n",
    "def pad(iterable, size, padding=None):\n",
    "       return islice(pad_infinite(iterable, padding), size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vSy-kUfd_mj",
    "outputId": "b9618e1e-be2b-4ab0-d655-14f0dc9a891d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4263/4263 [00:00<00:00, 15645.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1066/1066 [00:00<00:00, 16898.52it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data=encode_data(df_train,word2id,\"Text\",\"Sub-task B\")\n",
    "val_data=encode_data(df_val,word2id,\"Text\",\"Sub-task B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6YqwcOVeJby",
    "outputId": "4870c2a1-5874-47d1-c12b-d7f324bccac7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4263"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9F9v1DH2gR6Y"
   },
   "source": [
    "Data is a list of the form [word_vectors,label,'Next Part']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "leAOe7OTgn7p",
    "outputId": "1e7db778-7c65-4f98-fbd9-683f94ad9754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector length:  779\n"
     ]
    }
   ],
   "source": [
    "print(\"Word Vector length: \",len(train_data[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHvMKjGHiKx3"
   },
   "source": [
    "### Get Vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "c1w0GmU9fjl6"
   },
   "outputs": [],
   "source": [
    "pad_vec=np.random.randn(1,300) \n",
    "unk_vec=np.random.randn(1,300)\n",
    "merged_vec=np.append(vector, unk_vec, axis=0)\n",
    "merged_vec=np.append(merged_vec, pad_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3r7Ic__fy5A",
    "outputId": "bbf050d2-0328-4325-cf18-a75730811350"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50002, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2ueYdm2Ff0Bk"
   },
   "outputs": [],
   "source": [
    "vocab_size = merged_vec.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP7E1CH5iM77"
   },
   "source": [
    "### Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3Q82yekiiRBu"
   },
   "outputs": [],
   "source": [
    "def encodeLabels(data,misogynyLabel,nonmisogynyLabel):\n",
    "  for sample in data:\n",
    "    if(sample[1]==misogynyLabel):\n",
    "      sample[1]=1;\n",
    "    elif(sample[1]==nonmisogynyLabel):\n",
    "      sample[1]=0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vg1s2JiMintB"
   },
   "outputs": [],
   "source": [
    "encodeLabels(train_data,\"GEN\",\"NGEN\")\n",
    "encodeLabels(val_data,\"GEN\",\"NGEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlZL_Hcwf_9O"
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pFihEaLMjvBS"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "tYFZoIJEf9hs"
   },
   "outputs": [],
   "source": [
    "def return_cnngru_dataloader(samples, batch_size,is_train=False):\n",
    "  inputs = [ele[0] for ele in samples]\n",
    "  labels = [ele[1] for ele in samples]\n",
    "\n",
    "  inputs = torch.tensor(inputs)\n",
    "  labels = torch.tensor(labels,dtype=torch.long)\n",
    "\n",
    "  data = TensorDataset(inputs,labels)\n",
    "\n",
    "  if(is_train==False):\n",
    "      sampler = SequentialSampler(data)\n",
    "  else:\n",
    "      sampler = RandomSampler(data)  \n",
    "  \n",
    "  dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "zoIeS6oej-V7"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "O7606Bh0jrIO"
   },
   "outputs": [],
   "source": [
    "train_dataloader = return_cnngru_dataloader(train_data,BATCH_SIZE,True)\n",
    "validation_dataloader=return_cnngru_dataloader(val_data,BATCH_SIZE,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az-YVOw0kM-Y"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bENT09RokgKQ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "id": "EdVXvtb2kyMq"
   },
   "outputs": [],
   "source": [
    "args= {\n",
    "    'train_embed': False,\n",
    "    'weights': [1.0,10.0],\n",
    "    'vocab_size': vocab_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "id": "uof90RZ9kDiv"
   },
   "outputs": [],
   "source": [
    "def global_max_pooling(tensor, dim, topk):\n",
    "    \"\"\"Global max pooling\"\"\"\n",
    "    ret, _ = torch.topk(tensor, topk, dim)\n",
    "    return ret\n",
    "\n",
    "class CNN_GRU(nn.Module):\n",
    "    def __init__(self,args,vector):\n",
    "        super(CNN_GRU, self).__init__()\n",
    "        self.embedsize = vector.shape[1]\n",
    "        self.conv1 = nn.Conv1d(self.embedsize,100, 2)\n",
    "        self.conv2 = nn.Conv1d(self.embedsize,100, 3,padding=1)\n",
    "        self.conv3 = nn.Conv1d(self.embedsize,100, 4,padding=2)\n",
    "        self.maxpool1D = nn.MaxPool1d(4, stride=4)\n",
    "        self.seq_model = nn.GRU(100, 100, bidirectional=False, batch_first=True)\n",
    "        self.embedding = nn.Embedding(args[\"vocab_size\"], self.embedsize)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(vector.astype(np.float32), dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = args[\"train_embed\"]\n",
    "        self.num_labels=2\n",
    "        self.weights=args['weights']\n",
    "        self.out = nn.Linear(100, self.num_labels)\n",
    "\n",
    "        \n",
    "    def forward(self,x,labels=None):\n",
    "        batch_size=x.size(0)\n",
    "        h_embedding = self.embedding(x)\n",
    "        new_conv1=self.maxpool1D(self.conv1(h_embedding.permute(0,2,1)))\n",
    "        new_conv2=self.maxpool1D(self.conv2(h_embedding.permute(0,2,1)))\n",
    "        new_conv3=self.maxpool1D(self.conv3(h_embedding.permute(0,2,1)))\n",
    "        concat=self.maxpool1D(torch.cat([new_conv1, new_conv2,new_conv3], dim=2))\n",
    "        h_seq, _ = self.seq_model(concat.permute(0,2,1))\n",
    "        global_h_seq=torch.squeeze(global_max_pooling(h_seq, 1, 1)) \n",
    "        output=self.out(global_h_seq)\n",
    "        \n",
    "        if labels is not None:\n",
    "        \tloss_fct = nn.CrossEntropyLoss(weight=torch.tensor(self.weights,dtype=torch.float))\n",
    "        \tloss = loss_fct(output.view(-1, self.num_labels), labels.view(-1))\n",
    "        \treturn loss,output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "hvm4X2Kolgjw"
   },
   "outputs": [],
   "source": [
    "model = CNN_GRU(args,merged_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_mZRH0woIX2"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhVzOjKClyF5",
    "outputId": "0d4636e5-2a67-4e51-dda1-6a191c3a7365"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "id": "-zaSfzDJoK0N"
   },
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "id": "PLdd0xaXonro"
   },
   "outputs": [],
   "source": [
    " optimizer = AdamW(model.parameters(),\n",
    "                  lr = LR, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "id": "jbbMISRdoq5b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def fix_the_random(seed_val = 42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "id": "J_p1MStyqQKX"
   },
   "outputs": [],
   "source": [
    "fix_the_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8678257465362549\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0.03125\n",
      "0.06060606060606061\n",
      "0.7170535922050476\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0.09375\n",
      "0.06451612903225806\n",
      "0.6959130764007568\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
      "0.84375\n",
      "0.0\n",
      "0.5491247773170471\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0.96875\n",
      "0.0\n",
      "0.5842214822769165\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0.9375\n",
      "0.0\n",
      "0.7776606678962708\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      "0.84375\n",
      "0.0\n",
      "0.7294902801513672\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      "0.875\n",
      "0.0\n",
      "0.8387044072151184\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "0.84375\n",
      "0.0\n",
      "0.4248593747615814\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0.96875\n",
      "0.0\n",
      "0.22827929258346558\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for batch in train_dataloader:\n",
    "    i+=1\n",
    "    inputs= batch[0].to(device)\n",
    "    labels = batch[1].to(device)\n",
    "    \n",
    "#     print(inputs.shape)\n",
    "#     print(labels.shape)\n",
    "    \n",
    "    output = model(inputs,labels)\n",
    "    \n",
    "#     print(output.shape)\n",
    "    \n",
    "    loss = output[0]\n",
    "    logits = output[1]\n",
    "    \n",
    "    print(loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    y_true = labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "    y_pred = torch.max(logits,1)[1]\n",
    "    \n",
    "    print(y_pred),print(y_true)\n",
    "    y_pred = y_pred.cpu().data.squeeze().numpy()\n",
    "    \n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    print(f1_score(y_true, y_pred, labels = np.unique(y_pred)))\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if(i==10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLSa7Cjuq1-_"
   },
   "source": [
    "### Main Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "id": "6VNbY74dq986"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "id": "dktgCRnctXoL"
   },
   "outputs": [],
   "source": [
    "def evalMetric(y_true, y_pred,prefix):\n",
    "   accuracy = accuracy_score(y_true, y_pred)\n",
    "   mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "   f1Score  = f1_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "   fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "   area_under_c = auc(fpr, tpr)\n",
    "   recallScore = recall_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "   precisionScore = precision_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "   return dict({prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "                prefix+'f1Score': f1Score, prefix+'precision': precisionScore, \n",
    "                prefix+'recall': recallScore})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "id": "k0WNk_G6vR6C"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "id": "NY8_kLJSuQXF"
   },
   "outputs": [],
   "source": [
    "def EvaluateOnData(model,loader):\n",
    "    \n",
    "    model.eval() # put model in eval mode\n",
    "    \n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    y_pred = np.zeros(shape=(0),dtype='int')\n",
    "    y_true = np.empty(shape=(0),dtype='int')\n",
    "    \n",
    "    for batch in loader:\n",
    "        b_inputs = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad(): # do not construct compute graph\n",
    "            outputs = model(b_inputs,b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "        b_y_pred = torch.max(logits,1)[1]\n",
    "        b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "        y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "        y_true = np.concatenate((y_true,b_y_true))\n",
    "        \n",
    "    metrics = evalMetric(y_true,y_pred,\"Val_\")\n",
    "\n",
    "    print(\" Validation Accuracy: {0:.2f}\".format(metrics['Val_accuracy']))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_loss = total_eval_loss / len(loader)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_loss))\n",
    "\n",
    "    metrics['Val_avg_loss'] = avg_loss\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "id": "hkbiUppru2sG"
   },
   "outputs": [],
   "source": [
    "def runTrainLoop(model,train_loader,t0,optimiser):\n",
    "    print(\"\")\n",
    "    print('Training...')\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    model.train() # put model in train mode\n",
    "\n",
    "    y_pred = np.zeros(shape=(0),dtype='int')\n",
    "    y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in tqdm(enumerate(train_loader)):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        b_inputs = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "        \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_inputs,b_labels)\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print('batch_loss',loss.item())\n",
    "\n",
    "        #Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # Compute True and predicted labels to get\n",
    "        # train metrics\n",
    "        b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "        b_y_pred = torch.max(logits,1)[1]\n",
    "        b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "        \n",
    "        # accumulate b_y_pred and b_y_true for each batch\n",
    "        # and evaluate at once\n",
    "        y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "        y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        #scheduler.step()\n",
    "        \n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    train_metrics = evalMetric(y_true,y_pred,\"Train_\")\n",
    "\n",
    "    print('avg_train_loss',avg_train_loss)\n",
    "    print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "    print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "    train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "    return train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "id": "bf-1Mwvtqa8Z"
   },
   "outputs": [],
   "source": [
    "def train(model,train_loader,val_loader,optimiser,epochs):\n",
    "    train_stats = []\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "\n",
    "          # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        train_metrics = runTrainLoop(model,train_loader,t0,optimiser)\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\") \n",
    "        val_metrics = EvaluateOnData(model,val_loader)\n",
    "        \n",
    "        stats = {}\n",
    "        \n",
    "        stats['epoch']=epoch_i+1\n",
    "        \n",
    "        stats.update(train_metrics)\n",
    "        stats.update(val_metrics)\n",
    "\n",
    "        train_stats.append(stats)\n",
    "    \n",
    "    return train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "id": "UrOpvjeVxOll"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5896029472351074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4902842044830322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6900213360786438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6553460544169839\n",
      "train_f1Score 0.16276477146042362\n",
      "train_accuracy 0.8238329814684494\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.80\n",
      "  Validation Loss: 0.62\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4612290561199188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.8244192600250244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7256960868835449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:26,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6395130186383404\n",
      "train_f1Score 0.19722901385493075\n",
      "train_accuracy 0.7689420595824537\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.76\n",
      "  Validation Loss: 0.62\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.9179143309593201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7374975085258484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4540960192680359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6338299973242318\n",
      "train_f1Score 0.20802377414561662\n",
      "train_accuracy 0.749941355852686\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.91\n",
      "  Validation Loss: 0.63\n",
      "\n",
      "======== Epoch 4 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7036627531051636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6168279051780701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5486738085746765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6367299923701073\n",
      "train_f1Score 0.18600682593856652\n",
      "train_accuracy 0.7762139338494018\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.74\n",
      "  Validation Loss: 0.62\n",
      "\n",
      "======== Epoch 5 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:08,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7423107624053955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6425331830978394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5328797698020935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:26,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6295516281875212\n",
      "train_f1Score 0.20036652412950517\n",
      "train_accuracy 0.6929392446633826\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.73\n",
      "  Validation Loss: 0.63\n",
      "\n",
      "======== Epoch 6 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5604899525642395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.518089771270752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.730506420135498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6340348800616478\n",
      "train_f1Score 0.20361990950226244\n",
      "train_accuracy 0.7522871217452498\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.64\n",
      "  Validation Loss: 0.63\n",
      "\n",
      "======== Epoch 7 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:08,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.8015630841255188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5658503770828247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5749363303184509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:26,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6344919333707041\n",
      "train_f1Score 0.20726072607260723\n",
      "train_accuracy 0.7182735163030729\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.65\n",
      "  Validation Loss: 0.63\n",
      "\n",
      "======== Epoch 8 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7022523283958435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5381507277488708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.49218177795410156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6278990818493402\n",
      "train_f1Score 0.2051282051282051\n",
      "train_accuracy 0.72366877785597\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.67\n",
      "  Validation Loss: 0.63\n",
      "\n",
      "======== Epoch 9 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.612427830696106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6621097326278687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5450568199157715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6292072372205222\n",
      "train_f1Score 0.19568151147098517\n",
      "train_accuracy 0.7203847056063805\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.81\n",
      "  Validation Loss: 0.62\n",
      "\n",
      "======== Epoch 10 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6330550909042358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5309252142906189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:22,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7789674401283264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6297627407223431\n",
      "train_f1Score 0.18772563176895307\n",
      "train_accuracy 0.7361013370865588\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.57\n",
      "  Validation Loss: 0.65\n",
      "\n",
      "======== Epoch 11 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7399698495864868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5843307375907898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6415930390357971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:26,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6311814974048244\n",
      "train_f1Score 0.1973030518097942\n",
      "train_accuracy 0.7346938775510204\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.76\n",
      "  Validation Loss: 0.62\n",
      "\n",
      "======== Epoch 12 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.46096330881118774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5134145617485046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5325857996940613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6255035502697105\n",
      "train_f1Score 0.20875420875420878\n",
      "train_accuracy 0.7243725076237392\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.79\n",
      "  Validation Loss: 0.61\n",
      "\n",
      "======== Epoch 13 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:08,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6289860606193542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:16,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.8440609574317932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6486392021179199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:26,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6218367196730713\n",
      "train_f1Score 0.2115830115830116\n",
      "train_accuracy 0.7604973023692235\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.66\n",
      "  Validation Loss: 0.62\n",
      "\n",
      "======== Epoch 14 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:08,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5124680399894714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.49595698714256287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7312994599342346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6275608942135057\n",
      "train_f1Score 0.21536252692031588\n",
      "train_accuracy 0.7436077879427633\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.71\n",
      "  Validation Loss: 0.62\n",
      "\n",
      "======== Epoch 15 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:08,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.8137422800064087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7655908465385437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6766626834869385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:26,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6237769055722365\n",
      "train_f1Score 0.2041958041958042\n",
      "train_accuracy 0.7330518414262257\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.58\n",
      "  Validation Loss: 0.64\n",
      "\n",
      "======== Epoch 16 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4436948895454407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7013877630233765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5792557001113892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:26,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6217894934451402\n",
      "train_f1Score 0.2068502350570853\n",
      "train_accuracy 0.7229650480882008\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.75\n",
      "  Validation Loss: 0.61\n",
      "\n",
      "======== Epoch 17 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5777749419212341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.9759151935577393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6877971887588501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.61810189530031\n",
      "train_f1Score 0.20870767104353835\n",
      "train_accuracy 0.731409805301431\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.73\n",
      "  Validation Loss: 0.62\n",
      "\n",
      "======== Epoch 18 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:08,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5116952657699585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5719243288040161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5590273141860962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:26,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6228241175413132\n",
      "train_f1Score 0.20849933598937584\n",
      "train_accuracy 0.7203847056063805\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.68\n",
      "  Validation Loss: 0.63\n",
      "\n",
      "======== Epoch 19 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:08,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7105596661567688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.41482114791870117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7235344648361206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6207500965292774\n",
      "train_f1Score 0.20383036935704515\n",
      "train_accuracy 0.7269528501055594\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.63\n",
      "  Validation Loss: 0.63\n",
      "\n",
      "======== Epoch 20 / 20 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7106671333312988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:15,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.46385622024536133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:23,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6183617115020752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:25,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6242387127965244\n",
      "train_f1Score 0.2154273801250869\n",
      "train_accuracy 0.7351630307295332\n",
      "\n",
      "Running Validation...\n",
      " Validation Accuracy: 0.64\n",
      "  Validation Loss: 0.63\n"
     ]
    }
   ],
   "source": [
    "train_stats = train(model,train_dataloader,validation_dataloader,optimizer,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18454935622317595"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats[19]['Val_f1Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import io  #open embedding file\n",
    "import random  #fix random\n",
    "\n",
    "# Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Utility\n",
    "from tqdm import tqdm #progress-bar\n",
    "from itertools import chain, repeat, islice #padding\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Model\n",
    "import torch.nn as nn\n",
    "\n",
    "# Optimiser\n",
    "from transformers import AdamW\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_max_pooling(tensor, dim, topk):\n",
    "    \"\"\"Global max pooling\"\"\"\n",
    "    ret, _ = torch.topk(tensor, topk, dim)\n",
    "    return ret\n",
    "\n",
    "class CNN_GRU_Model(nn.Module):\n",
    "    def __init__(self,args,vector):\n",
    "        super(CNN_GRU_Model, self).__init__()\n",
    "        self.embedsize = vector.shape[1]\n",
    "        self.conv1 = nn.Conv1d(self.embedsize,100, 2)\n",
    "        self.conv2 = nn.Conv1d(self.embedsize,100, 3,padding=1)\n",
    "        self.conv3 = nn.Conv1d(self.embedsize,100, 4,padding=2)\n",
    "        self.maxpool1D = nn.MaxPool1d(4, stride=4)\n",
    "        self.seq_model = nn.GRU(100, 100, bidirectional=False, batch_first=True)\n",
    "        self.embedding = nn.Embedding(args[\"vocab_size\"], self.embedsize)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(vector.astype(np.float32), dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = args[\"train_embed\"]\n",
    "        self.num_labels=2\n",
    "        self.weights=args['weights']\n",
    "        self.out = nn.Linear(100, self.num_labels)\n",
    "\n",
    "        \n",
    "    def forward(self,x,labels=None):\n",
    "        batch_size=x.size(0)\n",
    "        h_embedding = self.embedding(x)\n",
    "        new_conv1=self.maxpool1D(self.conv1(h_embedding.permute(0,2,1)))\n",
    "        new_conv2=self.maxpool1D(self.conv2(h_embedding.permute(0,2,1)))\n",
    "        new_conv3=self.maxpool1D(self.conv3(h_embedding.permute(0,2,1)))\n",
    "        concat=self.maxpool1D(torch.cat([new_conv1, new_conv2,new_conv3], dim=2))\n",
    "        h_seq, _ = self.seq_model(concat.permute(0,2,1))\n",
    "        global_h_seq=torch.squeeze(global_max_pooling(h_seq, 1, 1)) \n",
    "        output=self.out(global_h_seq)\n",
    "        \n",
    "        if labels is not None:\n",
    "        \tloss_fct = nn.CrossEntropyLoss(weight=torch.tensor(self.weights,dtype=torch.float))\n",
    "        \tloss = loss_fct(output.view(-1, self.num_labels), labels.view(-1))\n",
    "        \treturn loss,output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_GRU:\n",
    "    def __init__(self,args):\n",
    "        # fix the random\n",
    "        random.seed(args['seed_val'])\n",
    "        np.random.seed(args['seed_val'])\n",
    "        torch.manual_seed(args['seed_val'])\n",
    "        torch.cuda.manual_seed_all(args['seed_val'])\n",
    "        \n",
    "        self.vector,id2word,self.word2id = self.load_vec(args['embedding_path'])\n",
    "        \n",
    "        self.device = torch.device(args['device'])\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##------------------ Utility Functions ----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def load_vec(self,emb_path, nmax=50000):\n",
    "        vectors = []\n",
    "        word2id = {}\n",
    "        with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vect = line.rstrip().split(' ', 1)\n",
    "                vect = np.fromstring(vect, sep=' ')\n",
    "                assert word not in word2id, 'word found twice'\n",
    "                vectors.append(vect)\n",
    "                word2id[word] = len(word2id)\n",
    "                if len(word2id) == nmax:\n",
    "                    break\n",
    "        id2word = {v: k for k, v in word2id.items()}\n",
    "        embeddings = np.vstack(vectors)\n",
    "        return embeddings, id2word, word2id\n",
    "    \n",
    "    \n",
    "    def pad_infinite(self,iterable, padding=None):\n",
    "        return chain(iterable, repeat(padding))\n",
    "    \n",
    "\n",
    "    def pad(self,iterable, size, padding=None):\n",
    "        return islice(self.pad_infinite(iterable, padding), size)\n",
    "    \n",
    "    \n",
    "    def encode_data(self,df,word2id):\n",
    "        max_len=0\n",
    "        for index,row in tqdm(df.iterrows(),total=len(df)):\n",
    "            \n",
    "            if(max_len<len(row['Text'].split(' '))):\n",
    "                max_len=len(row['Text'].split(' '))\n",
    "        \n",
    "        new_data=[]\n",
    "        \n",
    "        \n",
    "        for index,row in df.iterrows():\n",
    "            list_token_id=[]\n",
    "            words=row['Text'].split(' ')\n",
    "            for word in words:\n",
    "                try:\n",
    "                    index=word2id[word]\n",
    "                except KeyError:\n",
    "                    index=len(list(word2id.keys()))\n",
    "                list_token_id.append(index)\n",
    "            with_padding_text=list(self.pad(list_token_id, max_len, len(list(word2id.keys()))+1))\n",
    "            new_data.append([with_padding_text,row['Label'],row['Text']])\n",
    "        return new_data\n",
    "    \n",
    "    \n",
    "    def add_pad_unk(self,vector):\n",
    "        pad_vec=np.random.randn(1,300) \n",
    "        unk_vec=np.random.randn(1,300)\n",
    "        \n",
    "        merged_vec=np.append(vector, unk_vec, axis=0)\n",
    "        merged_vec=np.append(merged_vec, pad_vec, axis=0)\n",
    "        \n",
    "        return merged_vec\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##------------------ Dataloader -----------------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    \n",
    "    def get_dataloader(self,samples, batch_size,is_train=False):\n",
    "        inputs = [ele[0] for ele in samples]\n",
    "        labels = [ele[1] for ele in samples]\n",
    "\n",
    "        inputs = torch.tensor(inputs)\n",
    "        labels = torch.tensor(labels,dtype=torch.long)\n",
    "\n",
    "        data = TensorDataset(inputs,labels)\n",
    "\n",
    "        if(is_train==False):\n",
    "            sampler = SequentialSampler(data)\n",
    "        else:\n",
    "            sampler = RandomSampler(data)  \n",
    "\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "        return dataloader\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Training Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------##  \n",
    "    \n",
    "    def get_optimiser(self,learning_rate,model):\n",
    "         return AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8\n",
    "                )\n",
    "        \n",
    "    def evalMetric(self,y_true, y_pred,prefix):\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "        f1Score  = f1_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        area_under_c = auc(fpr, tpr)\n",
    "        recallScore = recall_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "        precisionScore = precision_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "        return dict({prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "                        prefix+'f1Score': f1Score, prefix+'precision': precisionScore, \n",
    "                        prefix+'recall': recallScore})\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##---------------- Different Train Loops --------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    \n",
    "    def evaluate(self,model,loader,which):\n",
    "    \n",
    "        model.eval() # put model in eval mode\n",
    "\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in loader:\n",
    "            b_inputs = batch[0].to(self.device)\n",
    "            b_labels = batch[1].to(self.device)\n",
    "\n",
    "            with torch.no_grad(): # do not construct compute graph\n",
    "                outputs = model(b_inputs,b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        metrics = self.evalMetric(y_true,y_pred,which+\"_\")\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_loss = total_eval_loss / len(loader)\n",
    "\n",
    "        metrics[which+'_avg_loss'] = avg_loss\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def run_train_loop(self,model,train_loader,optimiser):\n",
    "        \n",
    "        total_loss = 0\n",
    "        model.train() # put model in train mode\n",
    "\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for step, batch in tqdm(enumerate(train_loader)):\n",
    "\n",
    "            b_inputs = batch[0].to(self.device)\n",
    "            b_labels = batch[1].to(self.device)\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            outputs = model(b_inputs,b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimiser.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        train_metrics = self.evalMetric(y_true,y_pred,\"Train_\")\n",
    "\n",
    "        print('avg_train_loss',avg_train_loss)\n",
    "        print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "        print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "        train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "        return train_metrics\n",
    "    \n",
    "    \n",
    "    ##------------------------------------------------------------##\n",
    "    ##----------------- Main Train Loop --------------------------##\n",
    "    ##------------------------------------------------------------##\n",
    "    \n",
    "    def train(self,model,data_loaders,optimiser,epochs):\n",
    "        train_stats = []\n",
    "        train_loader,val_loader,test_loader = data_loaders\n",
    "        for epoch_i in range(0, epochs):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            \n",
    "            print(\"\")\n",
    "            print('Training...')\n",
    "            train_metrics = self.run_train_loop(model,train_loader,optimiser)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\") \n",
    "            val_metrics = self.evaluate(model,val_loader,\"Val\")\n",
    "            \n",
    "            print(\"Validation Loss: \",val_metrics['Val_avg_loss'])\n",
    "            print(\"Validation Accuracy: \",val_metrics['Val_accuracy'])\n",
    "            \n",
    "            stats = {}\n",
    "\n",
    "            stats['epoch']=epoch_i+1\n",
    "\n",
    "            stats.update(train_metrics)\n",
    "            stats.update(val_metrics)\n",
    "\n",
    "            train_stats.append(stats)\n",
    "\n",
    "        return train_stats\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##------------------------ The Pipeline ---------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run(self,args,df_train,df_val,df_test):\n",
    "        train_data=self.encode_data(df_train,self.word2id)\n",
    "        val_data=self.encode_data(df_val,self.word2id)\n",
    "        test_data=self.encode_data(df_test,self.word2id)\n",
    "        \n",
    "        merged_vec = self.add_pad_unk(self.vector)\n",
    "        \n",
    "        args['model']['vocab_size'] = merged_vec.shape[1]\n",
    "        \n",
    "        train_dl = self.get_dataloader(train_data,args['batch_size'],True)\n",
    "        val_dl = self.get_dataloader(val_data,args['batch_size'],False)\n",
    "        test_dl = self.get_dataloader(test_data,args['batch_size'],False)\n",
    "        \n",
    "        model = CNN_GRU_Model(args['model'],merged_vec)\n",
    "        \n",
    "        optimiser=self.get_optimiser(args['learning_rate'],model)\n",
    "        \n",
    "        train_stats = self.train(model,[train_dl,val_dl,test_dl],\n",
    "                            optimiser,args['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"Data_Processed/Shared_Task_eng/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_FOLDER+\"train_1.csv\")\n",
    "df_val = pd.read_csv(DATA_FOLDER+\"val_1.csv\")\n",
    "df_test = pd.read_csv(DATA_FOLDER+\"test_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(inplace=True)\n",
    "df_val.dropna(inplace=True)\n",
    "df_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>953</td>\n",
       "      <td>954</td>\n",
       "      <td>Fuck yourself Feminism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3413</td>\n",
       "      <td>3414</td>\n",
       "      <td>I subscribe to that propaganda, for real.\\r\\nI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>728</td>\n",
       "      <td>729</td>\n",
       "      <td>You are genuinely struggling with how approach...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1322</td>\n",
       "      <td>1323</td>\n",
       "      <td>Please do review on joker which is based on ev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2096</td>\n",
       "      <td>2097</td>\n",
       "      <td>But 99% liberals are praising this movie. Only...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    ID                                               Text  Label\n",
       "0         953   954                             Fuck yourself Feminism      1\n",
       "1        3413  3414  I subscribe to that propaganda, for real.\\r\\nI...      0\n",
       "2         728   729  You are genuinely struggling with how approach...      1\n",
       "3        1322  1323  Please do review on joker which is based on ev...      0\n",
       "4        2096  2097  But 99% liberals are praising this movie. Only...      0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "    'seed_val':42,\n",
    "    'embedding_path': \"Embeddings/wiki.multi.en.vec\",\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-4,\n",
    "    'epochs': 10,\n",
    "    'device':'cpu',\n",
    "    'model':{\n",
    "        'train_embed': False,\n",
    "        'weights': [1.0,8.0],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_gru = CNN_GRU(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9161/9161 [00:00<00:00, 15493.18it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1308/1308 [00:00<00:00, 16681.77it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2615/2615 [00:00<00:00, 17030.19it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [03:31,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6221524381471428\n",
      "train_f1Score 0.27088305489260145\n",
      "train_accuracy 0.7332168977185897\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.6043016016483307\n",
      "Validation Accuracy:  0.7431192660550459\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:03,  1.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-8c2c18a3caa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_gru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-3c807a2b53df>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, args, df_train, df_val, df_test)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_optimiser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         train_stats = self.train(model,[train_dl,val_dl,test_dl],\n\u001b[0m\u001b[1;32m    269\u001b[0m                             optimiser,args['epochs'])\n",
      "\u001b[0;32m<ipython-input-75-3c807a2b53df>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, data_loaders, optimiser, epochs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-3c807a2b53df>\u001b[0m in \u001b[0;36mrun_train_loop\u001b[0;34m(self, model, train_loader, optimiser)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mb_y_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn_gru.run(args,df_train,df_val,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CNN-GRU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
