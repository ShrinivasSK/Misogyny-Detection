{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import Data_Preprocessing\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "from transformers import RobertaPreTrainedModel,RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import random\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Tokeniser\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# Utility\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Optimiser\n",
    "from transformers import AdamW\n",
    "\n",
    "# Model\n",
    "\n",
    "import torch.nn as nn\n",
    "from models import weighted_Roberta\n",
    "\n",
    "\n",
    "class XLM_Roberta:\n",
    "    def __init__(self,args):\n",
    "        # fix the random\n",
    "        random.seed(args['seed_val'])\n",
    "        np.random.seed(args['seed_val'])\n",
    "        torch.manual_seed(args['seed_val'])\n",
    "        torch.cuda.manual_seed_all(args['seed_val'])\n",
    "        \n",
    "        # set device\n",
    "        self.device = torch.device(args['device'])\n",
    "\n",
    "        self.weights=args['weights']\n",
    "        \n",
    "        # initiliase tokeniser\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case = True)\n",
    "\n",
    "        self.model_save_path = args['model_save_path']\n",
    "        self.name = args['name']\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Utility Functions -----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def encode(self,data,max_len):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sent in tqdm(data):\n",
    "            # use in-built tokeniser of Bert\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                            sent,\n",
    "                            add_special_tokens =True, # for [CLS] and [SEP]\n",
    "                            max_length = max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = True,\n",
    "#                             return_tensors = 'pt', # return pytorch tensors\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            # attention masks notify where padding has been added \n",
    "            # and where is the sentence\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            X_data = torch.tensor(input_ids)\n",
    "            attention_masks_data = torch.tensor(attention_masks)\n",
    "            \n",
    "        return [X_data,attention_masks_data]\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##------------------ Dataloader -----------------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def get_dataloader(self,samples, batch_size,is_train=False):\n",
    "        inputs,masks,labels = samples\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "#         inputs = torch.cat(inputs, dim=0)\n",
    "#         masks = torch.cat(masks, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # convert to dataset\n",
    "        data = TensorDataset(inputs,masks,labels)\n",
    "\n",
    "        if(is_train==False):\n",
    "            # use random sampler for training to shuffle\n",
    "            # train data\n",
    "            sampler = SequentialSampler(data)\n",
    "        else:\n",
    "            # order does not matter for validation as we just \n",
    "            # need the metrics\n",
    "            sampler = RandomSampler(data)  \n",
    "\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size,drop_last=True)\n",
    "\n",
    "        return dataloader\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Training Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def get_optimiser(self,learning_rate,model):\n",
    "        # using AdamW optimiser from transformers library\n",
    "        return AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    def get_scheduler(self,epochs,optimiser,train_dl):\n",
    "        total_steps = len(train_dl) * epochs\n",
    "        return get_linear_schedule_with_warmup(optimiser, \n",
    "                num_warmup_steps = 0, \n",
    "                num_training_steps = total_steps)\n",
    "    \n",
    "    def evalMetric(self, y_true, y_pred, prefix):\n",
    "        # calculate all the metrics and add prefix to them\n",
    "        # before saving in dictionary\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "        f1Score = f1_score(y_true, y_pred)\n",
    "        area_under_c = roc_auc_score(y_true, y_pred)\n",
    "        recallScore = recall_score(y_true, y_pred)\n",
    "        precisionScore = precision_score(y_true, y_pred)\n",
    "\n",
    "        nonhate_f1Score = f1_score(y_true, y_pred, pos_label=0)\n",
    "        non_recallScore = recall_score(y_true, y_pred, pos_label=0)\n",
    "        non_precisionScore = precision_score(y_true, y_pred, pos_label=0)\n",
    "        return {prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "            prefix+'f1Score': f1Score, prefix+'auc': area_under_c,\n",
    "            prefix+'precision': precisionScore, \n",
    "            prefix+'recall': recallScore, \n",
    "            prefix+'non_hatef1Score': nonhate_f1Score, \n",
    "            prefix+'non_recallScore': non_recallScore, \n",
    "            prefix+'non_precisionScore': non_precisionScore}\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##---------------- Different Train Loops --------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def evaluate(self,model,loader,which):\n",
    "        # to evaluate model on test and validation set\n",
    "\n",
    "        model.eval() # put model in eval mode\n",
    "\n",
    "        # maintain total loss to save in metrics\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(loader):\n",
    "            # separate input, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            with torch.no_grad(): # do not construct compute graph\n",
    "                outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            # output is always a tuple, thus we have to \n",
    "            # separate it manually\n",
    "            #loss = outputs[0]\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float).to(self.device))\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "\n",
    "            # add the current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # calculate true labels and convert it into numpy array\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "            \n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = self.evalMetric(y_true,y_pred,which+\"_\")\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_loss = total_eval_loss / len(loader)\n",
    "        # add it to the metric\n",
    "        metrics[which+'_avg_loss'] = avg_loss\n",
    "\n",
    "        return metrics,y_pred\n",
    "    \n",
    "    \n",
    "    def run_train_loop(self,model,train_loader,optimiser,scheduler):\n",
    "\n",
    "        model.train() # put model in train mode\n",
    "\n",
    "        # maintain total loss to add to metric\n",
    "        total_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            # separate inputs, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            # Ref: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch#:~:text=In%20PyTorch%20%2C%20we%20need%20to,backward()%20call.\n",
    "            model.zero_grad()                \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "            # outputs is always returned as tuple\n",
    "            # Separate it manually\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float).to(self.device))\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "            \n",
    "            # calculate current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Back-propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # calculate true labels\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "            # clip gradient to prevent exploding gradient\n",
    "            # problems\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # gradient descent\n",
    "            optimiser.step()\n",
    "            \n",
    "            # schedule learning rate accordingly\n",
    "            scheduler.step()\n",
    "\n",
    "        # calculate avg loss \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # calculate metrics\n",
    "        train_metrics = self.evalMetric(y_true,y_pred,\"Train_\")\n",
    "        \n",
    "        # print results\n",
    "        print('avg_train_loss',avg_train_loss)\n",
    "        print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "        print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "        # add loss to metrics\n",
    "        train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "        return train_metrics\n",
    "    \n",
    "    \n",
    "    ##------------------------------------------------------------##\n",
    "    ##----------------- Main Train Loop --------------------------##\n",
    "    ##------------------------------------------------------------##\n",
    "    def train(self,model,data_loaders,optimiser,scheduler,epochs,save_model):\n",
    "        # save train stats per epoch\n",
    "        train_stats = []\n",
    "        train_loader,val_loader,test_loader = data_loaders\n",
    "        # maintain best mF1 Score to save best model\n",
    "        best_mf1Score=-1.0\n",
    "        for epoch_i in range(0, epochs):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            \n",
    "            print(\"\")\n",
    "            print('Training...')\n",
    "            # run trian loop\n",
    "            train_metrics = self.run_train_loop(model,train_loader,\n",
    "                                            optimiser,scheduler)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\") \n",
    "            # test on validation set\n",
    "            val_metrics,_ = self.evaluate(model,val_loader,\"Val\")\n",
    "            \n",
    "            print(\"Validation Loss: \",val_metrics['Val_avg_loss'])\n",
    "            print(\"Validation Accuracy: \",val_metrics['Val_accuracy'])\n",
    "            \n",
    "            stats = {}\n",
    "\n",
    "            # save model where validation mF1Score is best\n",
    "            if(val_metrics['Val_mF1Score']>best_mf1Score):\n",
    "                best_mf1Score=val_metrics['Val_mF1Score']\n",
    "                if(save_model):\n",
    "                    torch.save(model.state_dict(), self.model_save_path+\n",
    "                        '/best_bert_'+self.name+'.pt')\n",
    "                # evaluate best model on test set\n",
    "                test_metrics = self.evaluate(model,test_loader,\"Test\")\n",
    "\n",
    "            stats['epoch']=epoch_i+1\n",
    "\n",
    "            # add train and val metrics of the epoch to \n",
    "            # same dictionary\n",
    "            stats.update(train_metrics)\n",
    "            stats.update(val_metrics)\n",
    "\n",
    "            train_stats.append(stats)\n",
    "\n",
    "        return train_stats,test_metrics\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------------- Main Pipeline ---------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run(self,args,df_train,df_val,df_test):\n",
    "        # get X and Y data points \n",
    "        X_train = df_train['Text'].values\n",
    "        Y_train = df_train['Label'].values\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "        X_val = df_val['Text'].values\n",
    "        Y_val = df_val['Label'].values\n",
    "        \n",
    "        # encode data\n",
    "        # returns list of data and attention masks\n",
    "        train_data = self.encode(X_train,args['max_len'])\n",
    "        val_data = self.encode(X_val,args['max_len'])\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "        \n",
    "        # add labels to data so that we can send them to\n",
    "        # dataloader function together\n",
    "        train_data.append(Y_train)\n",
    "        val_data.append(Y_val)\n",
    "        test_data.append(Y_test)\n",
    "        \n",
    "        # convert to dataloader\n",
    "        train_dl =self.get_dataloader(train_data,args['batch_size'],True)\n",
    "        val_dl =self.get_dataloader(val_data,args['batch_size'])                          \n",
    "        test_dl =self.get_dataloader(test_data,args['batch_size'])\n",
    "        \n",
    "        # intialise model\n",
    "        model = weighted_Roberta.from_pretrained(\n",
    "            'xlm-roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
    "            params=args['params'],\n",
    "        )\n",
    "        model.to(self.device)\n",
    "        \n",
    "        optimiser = self.get_optimiser(args['learning_rate'],model)\n",
    "        \n",
    "        scheduler = self.get_scheduler(args['epochs'],optimiser,train_dl)\n",
    "        \n",
    "        # Run train loop and evaluate on validation data set\n",
    "        # on each epoch. Store best model from all epochs \n",
    "        # (best mF1 Score on Val set) and evaluate it on\n",
    "        # test set\n",
    "        train_stats,train_metrics = self.train(model,[train_dl,val_dl,test_dl],\n",
    "                                optimiser,scheduler,args['epochs'],args['save_model'])\n",
    "        \n",
    "        return train_stats,train_metrics\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##-------------------- Other Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run_test(self,model,df_test,args):\n",
    "        # to evaluate test set on the final saved model\n",
    "        # to retrieve results if necessary\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "\n",
    "        test_data.append(Y_test)\n",
    "\n",
    "        test_dl =self.get_dataloader(test_data,32)\n",
    "\n",
    "        metrics,preds = self.evaluate(model,test_dl,\"Test\")\n",
    "\n",
    "        return metrics,preds\n",
    "    \n",
    "    def load_model(self,path,args):\n",
    "        # load saved best model\n",
    "        saved_model = weighted_Roberta.from_pretrained(\n",
    "            'xlm-roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
    "            params=args['params'],\n",
    "        )\n",
    "        \n",
    "        saved_model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        return saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda:1',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,isArabic):\n",
    "    X = df['Text']\n",
    "    X_new=[]\n",
    "    if(isArabic):\n",
    "        prep = ArabertPreprocessor('bert-base-arabertv02')\n",
    "        for text in tqdm(X):\n",
    "            text = prep.preprocess(text)\n",
    "            X_new.append(text)\n",
    "    else:\n",
    "        processer = Data_Preprocessing()\n",
    "        for text in tqdm(X):\n",
    "            text= processer.removeEmojis(text)\n",
    "            text = processer.removeUrls(text)\n",
    "            text=processer.removeSpecialChar(text)\n",
    "            X_new.append(text)\n",
    "\n",
    "    df['Text']=X_new\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 16335/16335 [00:13<00:00, 1231.37it/s]\n"
     ]
    }
   ],
   "source": [
    "obj = XLM_Roberta(args)\n",
    "\n",
    "model_path = \"Saved_Models/Shared_Task_eng/best_bert_xlm_roberta_4_all.pt\"\n",
    "saved_model = obj.load_model(model_path,args)\n",
    "saved_model.to(obj.device)\n",
    "\n",
    "data_path = \"Data_Processed/Shared_Task_eng/all.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = preprocess(df,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16335/16335 [19:22<00:00, 14.06it/s]\n",
      "100%|██████████| 510/510 [01:03<00:00,  8.07it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics, preds = obj.run_test(saved_model,df,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df['Label'].values\n",
    "y_true = y_true[0: len(preds)]\n",
    "\n",
    "hate_correct = []\n",
    "hate_incorrect = []\n",
    "non_hate_correct=[]\n",
    "non_hate_incorrect=[]\n",
    "\n",
    "for index in range(len(preds)):\n",
    "    pred = preds[index]\n",
    "    true = y_true[index]\n",
    "    if(pred==true):\n",
    "        if(true==0):\n",
    "            non_hate_correct.append(index)\n",
    "        else:\n",
    "            hate_correct.append(index)\n",
    "    else:\n",
    "        if(true==0):\n",
    "            non_hate_incorrect.append(index)\n",
    "        else:\n",
    "            hate_incorrect.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If youre a man women cant be trusted Theyll turn against you its in their nature 16252\n",
      "\n",
      "Why WomenBoycottTwitter because you act like Ms Whore 14446\n",
      "\n",
      "DylanFry10 landenmartin Lmao Your stupid quick talking shit that you cant back up and meet me bitch 14022\n",
      "\n",
      "Exolkaiwife88 EXOpassion  this whore is damn sexy baekhyunsexyass tbh this girl knew how to name her acc  14692\n",
      "\n",
      "Fahima66819353 your pussy very big maximum 5 cock 15523\n",
      "\n",
      "5 I was 20 and ostracised from my friendship group and branded a slut because a boy with a girlfriend made unwanted advances on me MeToo 15186\n",
      "\n",
      "This is bullshit and horseshit\r\n",
      "\r\n",
      "Both this bitch and judge need to be shot and killed 8254\n",
      "\n",
      "Fuck boy    \n",
      " 1457\n",
      "\n",
      "You shouldve ruined her life in any way possible with access to her accounts\r\n",
      "\r\n",
      "Come on dude Bitches who dont act right need to be punished 11619\n",
      "\n",
      "Thats not really gender specific though I think most men want to settle down with the best woman they can too\r\n",
      "\r\n",
      "In monogamous societies there is less room for different marital behaviors between sexes because nobody can have more than one spouse\r\n",
      "\r\n",
      "I think the thing that comes closest to hypergamy is human societies with polygamy where a woman may prefer to be a wealthy mans fourth wife over a poor mans first wife Or in premarital sex culture where a woman on tinder may prefer to be a players hundredth lay than a virgins first lay Those are the main places where obvious gender asymmetries show up 9772\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(hate_correct,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_correct_indices = [1184, 7984, 14647, 15538, 8254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They want our money and peace of mind I say come and get it 6713\n",
      "\n",
      "Army need man not LGBT 1581\n",
      "\n",
      "When you first get into a relationship with a crazy girl vs after you get out of it  14710\n",
      "\n",
      "Hey alauddins maybe you deserve a dick up your ass since your profile picture is a selfie as well   15718\n",
      "\n",
      "Good decision bipin rawat sir  \n",
      "Gay will blow the job of male army 2083\n",
      "\n",
      "Yea and everyone on rfemaledatingstrategy uses men 8469\n",
      "\n",
      "Arundhati is a traitor and a kutiyaa 1681\n",
      "\n",
      "When you take her home drunk vs what you wake up to  13539\n",
      "\n",
      "Comparing contributions of men to those of women reveals that women have made almost no significant contributions in the history of man 13573\n",
      "\n",
      "Slay the gay 3078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(hate_incorrect,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_incorrect_indices = [15206, 10100, 5291, 16251, 1681]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feminism ruined marriage and american people hate feminism 3763\n",
      "\n",
      "Female student Loses marks in essay specifically for using the word mankind because it is sexist according to her feminist professor Dr Anne Scott  This is how pervasive feminism is in university Why would not say education has become feminised 11431\n",
      "\n",
      "Classic Ed Kempercel  6567\n",
      "\n",
      "krish veera there is a reason why most of the women who are into feminism\n",
      "actually go really mad it really triggers the internet is completely filled\n",
      "with people who has blamed feminism for everything it comes to a point where\n",
      "your brain gets triggered when you see comments like this  \n",
      "the other side is that the people who blame feminism end up not listening but\n",
      "they turn the same comment into different story by narrating it to their\n",
      "friends saying duuude i said something so logical and this lady in comment\n",
      "section just went berserk feminists are asshole  \n",
      "this happens 6000\n",
      "\n",
      "Just want to say thank you to kobebryant and DukeMLAX for their great work in the past inspiration 15048\n",
      "\n",
      "spindIeweed I am not sure how much you have seen about this but this is a pretty strange take on it to say rape is the main plot point is dismissive 16192\n",
      "\n",
      "What part of it is creepy He just did 2 questions How is that insisting dude 8126\n",
      "\n",
      "This made me screech like a pterodactyl thank you 10489\n",
      "\n",
      "Sir you got to review Mukkabaz 2205\n",
      "\n",
      "Super man 4211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(non_hate_correct,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hate_correct_indices = [3005, 4429, 1884, 3946, 15048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes I saw in reality how protective lesbian girl is when it comes to her\n",
      "girlfriend Salute to that girlslgbt rocks 98\n",
      "\n",
      "bdotchristian BheeScott Bitch stop tryna be hard hoe and just tell me you love me 15229\n",
      "\n",
      "How can you not engage As enraging as it all is fuck it all off I say Turn off the power Reset Reboot Escape the cunt booth 14677\n",
      "\n",
      "D E L E T I N G   M Y   A C C T 12631\n",
      "\n",
      "I used to be the sweetest girl ever but I met the wrong mfs and they turned me into a bitch Funny how the world can change you 15095\n",
      "\n",
      "augietwt just stop before you make it worse eat your cock and pussy 14218\n",
      "\n",
      "The poor kid wanted a fuck and got a bitch prego and now is fucked for life hahahahaha dumb cunts 13674\n",
      "\n",
      "SUSSANE arundhati roy 285\n",
      "\n",
      "horrific message and worst portrayal of gay people nobody has the right to\n",
      "take other persons life whether gay or straight  maybe its cuz the\n",
      "people behind it want to create a negative propaganda around LGBTQ\n",
      "community  \n",
      "  \n",
      "Totally disappointed  4484\n",
      "\n",
      "Ive done the same If Im already getting pussy why would I wanna fuck my female friends Not every hole is a goal honestly Honestly  15689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(non_hate_incorrect,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hate_incorrect_indices = [14632, 13173, 13490, 15478, 15095]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbabilities(sentences):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in tqdm(sentences):\n",
    "        # use in-built tokeniser of Bert\n",
    "        encoded_dict = obj.tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens =True, # for [CLS] and [SEP]\n",
    "                        max_length = 128,\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt', # return pytorch tensors\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        # attention masks notify where padding has been added \n",
    "        # and where is the sentence\n",
    "        attention_masks.append(encoded_dict['attention_mask']) \n",
    "        \n",
    "    masks = torch.cat(attention_masks, dim=0)\n",
    "    inputs = torch.cat(input_ids, dim=0)\n",
    "    \n",
    "    with torch.no_grad(): # do not construct compute graph\n",
    "        outputs = saved_model(inputs, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=masks,\n",
    "                           labels=None)\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    \n",
    "    preds = logits.cpu().data.squeeze().numpy()\n",
    "    \n",
    "    return softmax(preds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 16335/16335 [00:13<00:00, 1246.04it/s]\n"
     ]
    }
   ],
   "source": [
    "args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cpu',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "obj = XLM_Roberta(args)\n",
    "\n",
    "model_path = \"Saved_Models/Shared_Task_eng/best_bert_xlm_roberta_4_all.pt\"\n",
    "saved_model = obj.load_model(model_path,args)\n",
    "saved_model.to(obj.device)\n",
    "\n",
    "data_path = \"Data_Processed/Shared_Task_eng/all.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = preprocess(df,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=['non_hate','hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6224.20it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 11143.51it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 9214.61it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7458.79it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7113.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(hate_correct_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "                                     num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/Shared_Task_eng/hate_correct_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6978.41it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7458.40it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7416.20it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 9417.36it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7880.92it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(hate_incorrect_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "                                     num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/Shared_Task_eng/hate_incorrect_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 7423.15it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7816.74it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7936.99it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8521.37it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7513.98it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(non_hate_correct_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "                                     num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/Shared_Task_eng/non_hate_correct_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 7070.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 6017.65it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7409.65it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 9309.09it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 6166.73it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(non_hate_incorrect_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "                                     num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/Shared_Task_eng/non_hate_incorrect_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda:1',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 3307/3307 [00:01<00:00, 1869.49it/s]\n"
     ]
    }
   ],
   "source": [
    "obj = XLM_Roberta(args)\n",
    "\n",
    "model_path = \"Saved_Models/AMI-Spanish/best_bert_xlm_roberta_1_all.pt\"\n",
    "saved_model = obj.load_model(model_path,args)\n",
    "saved_model.to(obj.device)\n",
    "\n",
    "data_path = \"Data_Processed/AMI-Spanish/all.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = preprocess(df,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3307/3307 [00:48<00:00, 68.73it/s] \n",
      "100%|██████████| 103/103 [00:12<00:00,  8.05it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics, preds = obj.run_test(saved_model,df,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df['Label'].values\n",
    "y_true = y_true[0: len(preds)]\n",
    "\n",
    "hate_correct = []\n",
    "hate_incorrect = []\n",
    "non_hate_correct=[]\n",
    "non_hate_incorrect=[]\n",
    "\n",
    "for index in range(len(preds)):\n",
    "    pred = preds[index]\n",
    "    true = y_true[index]\n",
    "    if(pred==true):\n",
    "        if(true==0):\n",
    "            non_hate_correct.append(index)\n",
    "        else:\n",
    "            hate_correct.append(index)\n",
    "    else:\n",
    "        if(true==0):\n",
    "            non_hate_incorrect.append(index)\n",
    "        else:\n",
    "            hate_incorrect.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mira rolitranco el mamagueva tu eres una puta y eso vas a seguir siendo toda tu maldita y miserable vida y punto y he dicho Fin del comunicado 655\n",
      "\n",
      "TowandaRebels A m s que sola corregirme puta por punta pero he pasado demasiado rato en Twitter y ya pasa 1557\n",
      "\n",
      "mwesner1 Te mereces todo lo que est bien que lindo verte as perra  85\n",
      "\n",
      "suarezfeelings callate perra arrepentida 591\n",
      "\n",
      "Ojal la Puta sevillista de tu novia te mande donde te mereces antonioadan13 2016\n",
      "\n",
      "Antonin anda de zorra regalada y seguro hasta hace gritar de placer a su mejor amigo Cario t mereces mucho ms que eso   Porque soy un idiota pero un idiota feliz por el momento Siempre optimista annimo siempre op  1700\n",
      "\n",
      "laurabozzo Callate la boca vieja loca ven lame mi axila perra 665\n",
      "\n",
      "detailiant no mereces entrar a mi choza perra 1819\n",
      "\n",
      "yungpichaku 3adawiyaaa tu eres mi problema puta x 2747\n",
      "\n",
      "eduardoc60 RuiDeAzevedo411 Rui si tu eres una perra enamorada 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(hate_correct,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_correct_indices = [1976, 830, 2393, 254, 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BerserkerSaiyan CellMagnifico Tu callate TREMENDA PINCHE PUTA no tienes derecho de opinar porque tu user es una mierda chupa vergas de caballo 2545\n",
      "\n",
      "Puta wna Cllate Si s pero tu sab po encuentro tierno hasta cuando chocan los nios entre ellos Y esto esto no s es una mezcla de emociones me dej sin palabras po hasta incoherente idk solo quiero que lo abracen   2270\n",
      "\n",
      "aurelionuno lopezobrador Cllate pendejo de mierda Mentiroso hijo de la gran puta Culero cobarde puto e hijo de tu repinche madre 1707\n",
      "\n",
      "xSuzuHopkins gimme5yens El de mi polla a granel Que te calles virgen 3272\n",
      "\n",
      "Tambin te estresa todo hermana y bueno gil callate la puta madre 1193\n",
      "\n",
      "danieljadue MarianaLaActriz Callate concha de tu madre comunacho hijo de puta que nacio en cuna de oro 1814\n",
      "\n",
      "nsascast MarHuidobro menuda cara tienessi tienes verguenza cllate la puta boca que menuda mierda has parido tia 2622\n",
      "\n",
      "ignaciodover comemela hso y si cojemos ahi nomas no pasa nada ahre 1985\n",
      "\n",
      "JMLugones arielholanDT Independiente Cllate caradura hijo de puta Vos tens q cuidar al hincha de independiente no pegarle a la salida de la cancha 2805\n",
      "\n",
      "Puto mentiroso de mierda te has acostado con chicas maldito hijo de perra Te mereces todo lo que te est pasando kingxfthieves  1875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(hate_incorrect,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_incorrect_indices = [2530, 2196, 3255, 305, 1193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 Me encantan los memes y las tonterias de twitter y Facebook me parto la polla 3083\n",
      "\n",
      "LA PUTA MADRE TE MERECES TODO LO BUENO florvigna TODOS LOS AOS DEMOSTRAS QUE SOS LA MEJOR  Showmatch2017 MANDA BAILA FLOR AL 13013 2195\n",
      "\n",
      "GetawayCar78 Hierba mala nunca muere dicen Es un pedazo de imbecil que ojala le cerraran el canal Es el LoganJake paul espaol Escoria 1988\n",
      "\n",
      "Iros de una puta vez Zidane y Benzema Ya no merecis llevar esa camiseta ni ese escudo 2331\n",
      "\n",
      "hezmick Y t eres un autntico cateto No hables de lo que no tienes ni puta idea payaso  2740\n",
      "\n",
      "aaronscoot Laura99CM NiDarrak Es que aunque no lo creas responder eso suena a no llevas razn cllate la puta boca porque no quiero escuchar una verdad aunque no sea lo que quieras expresar 2574\n",
      "\n",
      "BlueeVeins que te calles la puta boca hombre hecho con el teclado ese que va a voz sabes no se como se llama pero ese 837\n",
      "\n",
      "Esa gente insultando y condenando al asesino de Diana Quer pero que luego le quitan importancia al acoso violacin y maltrato a la mujer 100\n",
      "\n",
      "No tengo ninguna bronca sobre las actrices hablalndo del machismo acoso y violacin en Hollywood como una manera de exponenciar la situacin en otros mbitos de trabajo pero Y las consecuencias a los perpetradradores Metieron a WeinsteinFilms a la crcel Accountability 1142\n",
      "\n",
      "samantavillar dicaro101 cielikolindo TradxAbolicion NIKITALYFM 1620emaraa AbolicionProst El siendo puta por un da pa cundo 957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(non_hate_correct,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hate_correct_indices = [1326, 2133, 615, 770, 2331]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pambisita1568 s0ymia Sucio coges sobra zorra novia jajajajajaj eres muy imbcil wey osea que asco das nunca en tu vida has visto un diccionario o que 659\n",
      "\n",
      "Que te pasa mardito yo soy puta pq quiero ser puta no pq me pas algo malo Cllate  2858\n",
      "\n",
      "es una puta gilipollez alarmarse asi enserio que hay de malo no esta desnuda no tiene nada que ensear por que tanto revuelo Para eso puedes poner ropa a tu pj si tanto te molesta verle desnudo un poquito de logica 2668\n",
      "\n",
      "la familia de mi novia es lo mejor qu hay la puta madre  1062\n",
      "\n",
      "Tu puta madre pinche chamaco mejor cllate 2078\n",
      "\n",
      "Te gustara participar en una peli porno buscamos tu perfil una tia guarra con ojos bonitos y no muy guapa  Hostia no te has esforzado mucho sigue intentndolo  1348\n",
      "\n",
      "AndreiusArdilla Me flipa Me tienes que ensear a hacer pelo me cago en la puta JAJAJA 2765\n",
      "\n",
      "roman tu eres la puta de vince v RAW 1501\n",
      "\n",
      "cual es el vehculo de una mujer la aspiradora 835\n",
      "\n",
      "Slo dentro de los lmites del humor quiz es esta la solucin compensatoria para ellas en el tema de la carne roja  789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(non_hate_incorrect,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hate_incorrect_indices = [2858, 1062, 2078, 1501, 835]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 3307/3307 [00:01<00:00, 2389.85it/s]\n"
     ]
    }
   ],
   "source": [
    "args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cpu',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "obj = XLM_Roberta(args)\n",
    "\n",
    "model_path = \"Saved_Models/AMI-Spanish/best_bert_xlm_roberta_1_all.pt\"\n",
    "saved_model = obj.load_model(model_path,args)\n",
    "saved_model.to(obj.device)\n",
    "\n",
    "data_path = \"Data_Processed/AMI-Spanish/all.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = preprocess(df,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbabilities(sentences):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in tqdm(sentences):\n",
    "        # use in-built tokeniser of Bert\n",
    "        encoded_dict = obj.tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens =True, # for [CLS] and [SEP]\n",
    "                        max_length = 128,\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt', # return pytorch tensors\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        # attention masks notify where padding has been added \n",
    "        # and where is the sentence\n",
    "        attention_masks.append(encoded_dict['attention_mask']) \n",
    "        \n",
    "    masks = torch.cat(attention_masks, dim=0)\n",
    "    inputs = torch.cat(input_ids, dim=0)\n",
    "    \n",
    "    with torch.no_grad(): # do not construct compute graph\n",
    "        outputs = saved_model(inputs, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=masks,\n",
    "                           labels=None)\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    \n",
    "    preds = logits.cpu().data.squeeze().numpy()\n",
    "    \n",
    "    return softmax(preds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=['non_hate','hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4728.75it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8430.25it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7378.10it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7293.81it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7200.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(hate_correct_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "                                     num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/AMI-Spanish/hate_correct_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4739.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7661.95it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7690.47it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7205.59it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8087.28it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(hate_incorrect_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "                                     num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/AMI-Spanish/hate_incorrect_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 8080.58it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7315.43it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7333.60it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8358.02it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7002.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(non_hate_correct_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "                                     num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/AMI-Spanish/non_hate_correct_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 7395.14it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8518.43it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8278.67it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7059.81it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8360.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(non_hate_incorrect_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "               a                      num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/AMI-Spanish/non_hate_incorrect_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 5240/5240 [00:00<00:00, 10888.38it/s]\n"
     ]
    }
   ],
   "source": [
    "obj = XLM_Roberta(args)\n",
    "\n",
    "model_path = \"Saved_Models/Let-Mi/best_bert_xlm_roberta_3_all.pt\"\n",
    "saved_model = obj.load_model(model_path,args)\n",
    "saved_model.to(obj.device)\n",
    "\n",
    "data_path = \"Data_Processed/Let-Mi/all.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = preprocess(df,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5240/5240 [02:01<00:00, 43.02it/s] \n",
      "100%|██████████| 163/163 [00:20<00:00,  8.06it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics, preds = obj.run_test(saved_model,df,args)\n",
    "\n",
    "y_true = df['Label'].values\n",
    "y_true = y_true[0: len(preds)]\n",
    "\n",
    "hate_correct = []\n",
    "hate_incorrect = []\n",
    "non_hate_correct=[]\n",
    "non_hate_incorrect=[]\n",
    "\n",
    "for index in range(len(preds)):\n",
    "    pred = preds[index]\n",
    "    true = y_true[index]\n",
    "    if(pred==true):\n",
    "        if(true==0):\n",
    "            non_hate_correct.append(index)\n",
    "        else:\n",
    "            hate_correct.append(index)\n",
    "    else:\n",
    "        if(true==0):\n",
    "            non_hate_incorrect.append(index)\n",
    "        else:\n",
    "            hate_incorrect.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[مستخدم] بلا اخلاق بزيادة عليكي هالكلمة 3341\n",
      "\n",
      "[مستخدم] يازعرا انت هه 130\n",
      "\n",
      "[مستخدم] اوقح البشر انت 1451\n",
      "\n",
      "RT [مستخدم] : داعشية الاعلام نانسي السبع تتبع اساليب الخبث والحقارة في الاعلام 1201\n",
      "\n",
      "[مستخدم] [مستخدم] انضبي يا شرحوحه 3828\n",
      "\n",
      "[مستخدم] [مستخدم] # طعمرك شو مسخرة 3519\n",
      "\n",
      "[مستخدم] واطية رخيصة فاسدة اكتر من معلمينك # ديما _ الواطية 4637\n",
      "\n",
      "[مستخدم] بتحكي اد فهماتك وانت ما بتفهمي اسكتي احسن 481\n",
      "\n",
      "[مستخدم] أكلي زباله يا زباله 2235\n",
      "\n",
      "[مستخدم] نفايات الكترونية و انت وحدة منن 155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(hate_correct,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_correct_indices = [3341,4637, 155, 1201, 481 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[مستخدم] قلتلك 100 مرة المطبخ احسن لك . . الولد الشقي بجيب مسبات للعبة # السيد _ نصر _ الله _ ثقتنا 800\n",
      "\n",
      "[مستخدم] يمنى فواز ، ارحمي حالك ، اشفقي على حالك واسكتي . ما بعرف اذا ممكن اي شخص اشتغل صحافة بيوم من الايام ، اي ول … [رابط] 2630\n",
      "\n",
      "[مستخدم] ما هي الرسالة التي توجه ؟ سخافة متل سخافة [مستخدم] 128\n",
      "\n",
      "[مستخدم] [مستخدم] عنجد كان معهن حق يقولوا عنك اشيا ما بتسوا 720\n",
      "\n",
      "[مستخدم] طز فيكي وبالرينغ قرفتونا بالرينغ وغير الرينج [رابط] 3370\n",
      "\n",
      "[مستخدم] [مستخدم] [مستخدم] بول البعير كتير عليك 2832\n",
      "\n",
      "[مستخدم] قلتلك 100 مرة المطبخ احسن لك . . الولد الشقي بجيب مسبات للعبة # السيد _ نصر _ الله _ ثقتنا 800\n",
      "\n",
      "[مستخدم] لو في دوله بتحترم حالها كنتي انت وأمثالك من صناع الفتن بمستشفى الأمراض العقلية او حبس روميه من زمان . 3165\n",
      "\n",
      "[مستخدم] ديما بتسوى صرمايه بايام الغلى 4227\n",
      "\n",
      "[مستخدم] [مستخدم] اي نحن ما منقبلها صرماية بإجرنا . . مبروك . ع راسكم 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(hate_incorrect,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_incorrect_indices = [128,720, 800, 3165, 3370]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[مستخدم] شفنا الأعداد يا ست يمنى كلهم على بعضهم ما بيجوا 100 ألف بتسميها انت ثورة ؟ 4222\n",
      "\n",
      "[مستخدم] اليوم . . بكرا . . بعد شهر . . بعد سنة . . ألف سنة . . ستسقطون ! ! انشاء الله ما تكوني فكرتي كتير قبل ما تكتبيها : D 3821\n",
      "\n",
      "[مستخدم] اقسم انك اقوى مرا بالكون 2793\n",
      "\n",
      "كل الحب الكن [مستخدم] [مستخدم] [رابط] 5078\n",
      "\n",
      "# كلنا _ ديما _ صادق أكيد الجيش الالكتروني السوري ، كلاب # بشار _ الأسد فلتانين و عينبحوا لمساعدة # حسن _ نصر _ الله و حزبو ضد صوت حر متل [مستخدم] 1948\n",
      "\n",
      "[مستخدم] [مستخدم] [مستخدم] [مستخدم] يا ست ام ديما حضرتك ست محترمة بس ليش ما عرفت تربي ديما 2856\n",
      "\n",
      "[مستخدم] أخلاقيا سقطوا من زمان وسقطو من عيون الناس 4346\n",
      "\n",
      "[مستخدم] شكرا ديما صادق من القلب 3223\n",
      "\n",
      "[مستخدم] اي نسيلنا نصرالله وركزوا عالاساس يرضى عليكي # كلنا _ ديما _ صادق 4513\n",
      "\n",
      "[مستخدم] انتي بطلة بكل صراحة كل الحب والاحترام 2992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(non_hate_correct,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hate_correct_indices = [3666, 1721, 476, 501, 4222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[مستخدم] ولا ابوخ من هيك قرطة 5118\n",
      "\n",
      "[مستخدم] ما انتي وحدة منن نزلتي معن بل انتخابات او نسيتي حالكو قريبة كتير منن ف ما تركبي موجة لعبتك مكشوفة . . و … [رابط] 3167\n",
      "\n",
      "[مستخدم] [مستخدم] [مستخدم] [مستخدم] احد الممولين لها والداعمين كمشناكي يا آدمية 2808\n",
      "\n",
      "[مستخدم] خلص ما تزعلي هلق تحسينو بزدلا ياهن شوي قال ثورة قال روحوا انضبوا 3377\n",
      "\n",
      "[مستخدم] ديما روحي غطي الحدث ب ادلب بيمشي حالك 4164\n",
      "\n",
      "[مستخدم] انت المصاري فوقك وتحتك شو فارقة معك ؟ ! ! 3938\n",
      "\n",
      "[مستخدم] [مستخدم] [مستخدم] صاير لسانك طويل الله يلعن الغرور 879\n",
      "\n",
      "[مستخدم] هني الواطيين حثالة المجتمع ما تهكلي للهم 139\n",
      "\n",
      "[مستخدم] [مستخدم] يا زلمي ما بتنطاق لا عل الوج ولا عالقفى 1264\n",
      "\n",
      "[مستخدم] [مستخدم] [مستخدم] [مستخدم] [مستخدم] [مستخدم] نسبية على دايرة وحدة 1043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = random.choices(non_hate_incorrect,k=10)\n",
    "for index in samples:\n",
    "    print(df.iloc[index]['Text'],index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hate_incorrect_indices = [1043, 3075, 576, 2808, 3938]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 5240/5240 [00:01<00:00, 3405.85it/s]\n"
     ]
    }
   ],
   "source": [
    "args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cpu',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "obj = XLM_Roberta(args)\n",
    "\n",
    "model_path = \"Saved_Models/Let-Mi/best_bert_xlm_roberta_3_all.pt\"\n",
    "saved_model = obj.load_model(model_path,args)\n",
    "saved_model.to(obj.device)\n",
    "\n",
    "data_path = \"Data_Processed/Let-Mi/all.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = preprocess(df,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbabilities(sentences):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in tqdm(sentences):\n",
    "        # use in-built tokeniser of Bert\n",
    "        encoded_dict = obj.tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens =True, # for [CLS] and [SEP]\n",
    "                        max_length = 128,\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt', # return pytorch tensors\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        # attention masks notify where padding has been added \n",
    "        # and where is the sentence\n",
    "        attention_masks.append(encoded_dict['attention_mask']) \n",
    "        \n",
    "    masks = torch.cat(attention_masks, dim=0)\n",
    "    inputs = torch.cat(input_ids, dim=0)\n",
    "    \n",
    "    with torch.no_grad(): # do not construct compute graph\n",
    "        outputs = saved_model(inputs, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=masks,\n",
    "                           labels=None)\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    \n",
    "    preds = logits.cpu().data.squeeze().numpy()\n",
    "    \n",
    "    return softmax(preds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=['non_hate','hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 8349.53it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 10787.26it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 10632.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 9307.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8287.17it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(hate_correct_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "                                     num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/Let-Mi/hate_correct_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 8928.61it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8060.54it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7830.31it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8466.84it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 10247.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(hate_incorrect_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "                                     num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/Let-Mi/hate_incorrect_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 8091.02it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8787.38it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 9573.20it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 10179.36it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 9281.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(non_hate_correct_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "                                     num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/Let-Mi/non_hate_correct_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 7214.15it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8837.56it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 10712.33it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 6886.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8962.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(non_hate_incorrect_indices):\n",
    "    exp = explainer.explain_instance(df.iloc[index]['Text'],\n",
    "                                    getProbabilities, num_samples=100,\n",
    "                                     num_features=20)\n",
    "    exp.save_to_file(\"Outputs_xlm/Let-Mi/non_hate_incorrect_\"+str(i+1)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
