{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import Data_Preprocessing\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 27 21:53:59 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    31W / 250W |   8300MiB / 12198MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:DB:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    38W / 250W |   3088MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1423      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A     33770      C   ...onda/envs/py38/bin/python     3237MiB |\n",
      "|    0   N/A  N/A   1334586      C   ...hal/miniconda3/bin/python     5057MiB |\n",
      "|    1   N/A  N/A      1423      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A     33770      C   ...onda/envs/py38/bin/python     1653MiB |\n",
      "|    1   N/A  N/A   1363277      C   python3                          1429MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed_val=42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(args,index):\n",
    "    # initialise constants \n",
    "    path = args['data_path']\n",
    "    train_cnt = args['train_cnt']\n",
    "    model_name = args['model_name']\n",
    "    # read dataframes\n",
    "    df_train = pd.read_csv(path+'train_'+str(index)+'.csv')\n",
    "    df_val = pd.read_csv(path+'val_'+str(index)+'.csv')\n",
    "    df_test = pd.read_csv(path+'test_'+str(index)+'.csv')\n",
    "\n",
    "    # clean data\n",
    "    df_train=preprocess(df_train,args['isArabic'])\n",
    "    df_val=preprocess(df_val,args['isArabic'])\n",
    "    df_test=preprocess(df_test,args['isArabic'])\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,isArabic):\n",
    "    \n",
    "    X = df['Text']\n",
    "    X_new=[]\n",
    "    if(isArabic):\n",
    "        prep = ArabertPreprocessor('bert-base-arabertv02')\n",
    "        for text in tqdm(X):\n",
    "            text = prep.preprocess(text)\n",
    "            X_new.append(text)\n",
    "    else:\n",
    "        processer = Data_Preprocessing()\n",
    "        for text in tqdm(X):\n",
    "            text= processer.removeEmojis(text)\n",
    "            text = processer.removeUrls(text)\n",
    "            text=processer.removeSpecialChar(text)\n",
    "            X_new.append(text)\n",
    "\n",
    "    df['Text']=X_new\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(path,metrics,which):\n",
    "    df = pd.DataFrame(metrics)\n",
    "    df.to_csv(path+\"_\"+which+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, index,all_test_metrics,model_args):\n",
    "    model_name = args['model_name']\n",
    "    model_args['name']=model_name+'_'+str(index)+'_all'\n",
    "    print(\"\\tInitialising Model....\")\n",
    "    model = BERT(model_args)\n",
    "    print(\"\\tLoading Dataset....\")\n",
    "    df_train, df_val, df_test = load_dataset(args,index)\n",
    "    print(\"\\tTraining Starts....\")\n",
    "    train_metrics, test_metrics = model.run(model_args, \n",
    "                    df_train, df_val, df_test)\n",
    "    test_metrics['name']=model_args['name']\n",
    "\n",
    "    # Save train metrics after generating path\n",
    "    res_path=args['res_base_path']+model_name+'_'+model_args['name']\n",
    "    save_metrics(res_path,train_metrics,\"train\")\n",
    "    \n",
    "    all_test_metrics.append(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args,model_args):\n",
    "    all_test_metrics=[]\n",
    "    \n",
    "    for fold in [1, 2, 3, 4, 5]:\n",
    "        print(\"Fold: \",fold)\n",
    "        fix_random()\n",
    "        train(args,fold,all_test_metrics,model_args)\n",
    "        print(\"Saving Test Metrics....\")\n",
    "        save_metrics(args['res_base_path']+args['model_name']+\n",
    "             '_all',all_test_metrics,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import random\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Tokeniser\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Utility\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Optimiser\n",
    "from transformers import AdamW\n",
    "\n",
    "# Model\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERT:\n",
    "    def __init__(self,args):\n",
    "        # fix the random\n",
    "        random.seed(args['seed_val'])\n",
    "        np.random.seed(args['seed_val'])\n",
    "        torch.manual_seed(args['seed_val'])\n",
    "        torch.cuda.manual_seed_all(args['seed_val'])\n",
    "        \n",
    "        # set device\n",
    "        self.device = torch.device(args['device'])\n",
    "\n",
    "        self.weights=args['weights']\n",
    "        \n",
    "        # initiliase tokeniser\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(args['bert_model'])\n",
    "\n",
    "        self.model_save_path = args['model_save_path']\n",
    "        self.name = args['name']\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Utility Functions -----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def encode(self,data,max_len):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sent in tqdm(data):\n",
    "            # use in-built tokeniser of Bert\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                            sent,\n",
    "                            add_special_tokens =True, # for [CLS] and [SEP]\n",
    "                            max_length = max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt', # return pytorch tensors\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            # attention masks notify where padding has been added \n",
    "            # and where is the sentence\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "        return [input_ids,attention_masks]\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##------------------ Dataloader -----------------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def get_dataloader(self,samples, batch_size,is_train=False):\n",
    "        inputs,masks,labels = samples\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "        inputs = torch.cat(inputs, dim=0)\n",
    "        masks = torch.cat(masks, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # convert to dataset\n",
    "        data = TensorDataset(inputs,masks,labels)\n",
    "\n",
    "        if(is_train==False):\n",
    "            # use random sampler for training to shuffle\n",
    "            # train data\n",
    "            sampler = SequentialSampler(data)\n",
    "        else:\n",
    "            # order does not matter for validation as we just \n",
    "            # need the metrics\n",
    "            sampler = RandomSampler(data)  \n",
    "\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size,drop_last=True)\n",
    "\n",
    "        return dataloader\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Training Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def get_optimiser(self,learning_rate,model):\n",
    "        # using AdamW optimiser from transformers library\n",
    "        return AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    def get_scheduler(self,epochs,optimiser,train_dl):\n",
    "        total_steps = len(train_dl) * epochs\n",
    "        return get_linear_schedule_with_warmup(optimiser, \n",
    "                num_warmup_steps = 0, \n",
    "                num_training_steps = total_steps)\n",
    "    \n",
    "    def evalMetric(self, y_true, y_pred, prefix):\n",
    "        # calculate all the metrics and add prefix to them\n",
    "        # before saving in dictionary\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "        f1Score = f1_score(y_true, y_pred)\n",
    "        area_under_c = roc_auc_score(y_true, y_pred)\n",
    "        recallScore = recall_score(y_true, y_pred)\n",
    "        precisionScore = precision_score(y_true, y_pred)\n",
    "\n",
    "        nonhate_f1Score = f1_score(y_true, y_pred, pos_label=0)\n",
    "        non_recallScore = recall_score(y_true, y_pred, pos_label=0)\n",
    "        non_precisionScore = precision_score(y_true, y_pred, pos_label=0)\n",
    "        return {prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "            prefix+'f1Score': f1Score, prefix+'auc': area_under_c,\n",
    "            prefix+'precision': precisionScore, \n",
    "            prefix+'recall': recallScore, \n",
    "            prefix+'non_hatef1Score': nonhate_f1Score, \n",
    "            prefix+'non_recallScore': non_recallScore, \n",
    "            prefix+'non_precisionScore': non_precisionScore}\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##---------------- Different Train Loops --------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def evaluate(self,model,loader,which):\n",
    "        # to evaluate model on test and validation set\n",
    "\n",
    "        model.eval() # put model in eval mode\n",
    "\n",
    "        # maintain total loss to save in metrics\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(loader):\n",
    "            # separate input, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            with torch.no_grad(): # do not construct compute graph\n",
    "                outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            # output is always a tuple, thus we have to \n",
    "            # separate it manually\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "            # add the current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # calculate true labels and convert it into numpy array\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "            \n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = self.evalMetric(y_true,y_pred,which+\"_\")\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_loss = total_eval_loss / len(loader)\n",
    "        # add it to the metric\n",
    "        metrics[which+'_avg_loss'] = avg_loss\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def run_train_loop(self,model,train_loader,optimiser,scheduler):\n",
    "\n",
    "        model.train() # put model in train mode\n",
    "\n",
    "        # maintain total loss to add to metric\n",
    "        total_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            # separate inputs, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            # Ref: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch#:~:text=In%20PyTorch%20%2C%20we%20need%20to,backward()%20call.\n",
    "            model.zero_grad()                \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "            # outputs is always returned as tuple\n",
    "            # Separate it manually\n",
    "            logits = outputs[1]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float)).to(self.device)\n",
    "            \n",
    "            loss = loss_fct(logits,b_labels)\n",
    "            \n",
    "            # calculate current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Back-propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # calculate true labels\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "            # clip gradient to prevent exploding gradient\n",
    "            # problems\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # gradient descent\n",
    "            optimiser.step()\n",
    "            \n",
    "            # schedule learning rate accordingly\n",
    "            scheduler.step()\n",
    "\n",
    "        # calculate avg loss \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # calculate metrics\n",
    "        train_metrics = self.evalMetric(y_true,y_pred,\"Train_\")\n",
    "        \n",
    "        # print results\n",
    "        print('avg_train_loss',avg_train_loss)\n",
    "        print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "        print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "        # add loss to metrics\n",
    "        train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "        return train_metrics\n",
    "    \n",
    "    \n",
    "    ##------------------------------------------------------------##\n",
    "    ##----------------- Main Train Loop --------------------------##\n",
    "    ##------------------------------------------------------------##\n",
    "    def train(self,model,data_loaders,optimiser,scheduler,epochs,save_model):\n",
    "        # save train stats per epoch\n",
    "        train_stats = []\n",
    "        train_loader,val_loader,test_loader = data_loaders\n",
    "        # maintain best mF1 Score to save best model\n",
    "        best_mf1Score=-1.0\n",
    "        for epoch_i in range(0, epochs):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            \n",
    "            print(\"\")\n",
    "            print('Training...')\n",
    "            # run trian loop\n",
    "            train_metrics = self.run_train_loop(model,train_loader,\n",
    "                                            optimiser,scheduler)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\") \n",
    "            # test on validation set\n",
    "            val_metrics = self.evaluate(model,val_loader,\"Val\")\n",
    "            \n",
    "            print(\"Validation Loss: \",val_metrics['Val_avg_loss'])\n",
    "            print(\"Validation Accuracy: \",val_metrics['Val_accuracy'])\n",
    "            \n",
    "            stats = {}\n",
    "\n",
    "            # save model where validation mF1Score is best\n",
    "            if(val_metrics['Val_mF1Score']>best_mf1Score):\n",
    "                best_mf1Score=val_metrics['Val_mF1Score']\n",
    "                if(save_model):\n",
    "                    torch.save(model.state_dict(), self.model_save_path+\n",
    "                        '/best_bert_'+self.name+'.pt')\n",
    "                # evaluate best model on test set\n",
    "                test_metrics = self.evaluate(model,test_loader,\"Test\")\n",
    "\n",
    "            stats['epoch']=epoch_i+1\n",
    "\n",
    "            # add train and val metrics of the epoch to \n",
    "            # same dictionary\n",
    "            stats.update(train_metrics)\n",
    "            stats.update(val_metrics)\n",
    "\n",
    "            train_stats.append(stats)\n",
    "\n",
    "        return train_stats,test_metrics\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------------- Main Pipeline ---------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run(self,args,df_train,df_val,df_test):\n",
    "        # get X and Y data points \n",
    "        X_train = df_train['Text'].values\n",
    "        Y_train = df_train['Label'].values\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "        X_val = df_val['Text'].values\n",
    "        Y_val = df_val['Label'].values\n",
    "        \n",
    "        # encode data\n",
    "        # returns list of data and attention masks\n",
    "        train_data = self.encode(X_train,args['max_len'])\n",
    "        val_data = self.encode(X_val,args['max_len'])\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "        \n",
    "        # add labels to data so that we can send them to\n",
    "        # dataloader function together\n",
    "        train_data.append(Y_train)\n",
    "        val_data.append(Y_val)\n",
    "        test_data.append(Y_test)\n",
    "        \n",
    "        # convert to dataloader\n",
    "        train_dl =self.get_dataloader(train_data,args['batch_size'],True)\n",
    "        val_dl =self.get_dataloader(val_data,args['batch_size'])                          \n",
    "        test_dl =self.get_dataloader(test_data,args['batch_size'])\n",
    "        \n",
    "        # intialise model\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "                args['bert_model'], \n",
    "                num_labels = 2, \n",
    "                output_attentions = False, # Whether the model returns attentions weights.\n",
    "                output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "            )\n",
    "        model.to(self.device)\n",
    "        \n",
    "        optimiser = self.get_optimiser(args['learning_rate'],model)\n",
    "        \n",
    "        scheduler = self.get_scheduler(args['epochs'],optimiser,train_dl)\n",
    "        \n",
    "        # Run train loop and evaluate on validation data set\n",
    "        # on each epoch. Store best model from all epochs \n",
    "        # (best mF1 Score on Val set) and evaluate it on\n",
    "        # test set\n",
    "        train_stats,train_metrics = self.train(model,[train_dl,val_dl,test_dl],\n",
    "                                optimiser,scheduler,args['epochs'],args['save_model'])\n",
    "        \n",
    "        return train_stats,train_metrics\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##-------------------- Other Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run_test(self,model,df_test,args):\n",
    "        # to evaluate test set on the final saved model\n",
    "        # to retrieve results if necessary\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "\n",
    "        test_data.append(Y_test)\n",
    "\n",
    "        test_dl =self.get_dataloader(test_data,32)\n",
    "\n",
    "        metrics = self.evaluate(model,test_dl,\"Test\")\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def load_model(self,path,args):\n",
    "        # load saved best model\n",
    "        saved_model = BertForSequenceClassification.from_pretrained(\n",
    "                args['bert_model'], \n",
    "                num_labels = 2, \n",
    "                output_attentions = False, # Whether the model returns attentions weights.\n",
    "                output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "            )\n",
    "        \n",
    "        saved_model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        return saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "\tInitialising Model....\n",
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29202/29202 [00:02<00:00, 10843.85it/s]\n",
      "100%|██████████| 4171/4171 [00:00<00:00, 10037.27it/s]\n",
      "100%|██████████| 8343/8343 [00:00<00:00, 10872.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29202/29202 [00:10<00:00, 2915.23it/s]\n",
      "100%|██████████| 4171/4171 [00:01<00:00, 2623.49it/s]\n",
      "100%|██████████| 8343/8343 [00:02<00:00, 2916.27it/s]\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 3575/3650 [05:56<00:07,  9.92it/s]"
     ]
    }
   ],
   "source": [
    "run_args={\n",
    "    'model_name':'bert',\n",
    "    'data_path':'Data_Processed/all_but_one/Let-Mi/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Let-Mi/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Let-Mi/all_but_one/',\n",
    "    'isArabic': True,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'seed_val': 42,\n",
    "    'batch_size': 8,\n",
    "#         'bert_model': \"aubmindlab/bert-base-arabertv02\",\n",
    "    'bert_model': \"bert-base-multilingual-cased\",\n",
    "#         'bert_model': \"bert-base-uncased\",\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 10,\n",
    "    'max_len': 128,\n",
    "    'device': 'cuda',\n",
    "    'weights': [1.0, 1.0],\n",
    "    'save_model': True,\n",
    "    'model_save_path': 'Saved_Models/Let-Mi/all_but_one/',\n",
    "}\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'bert',\n",
    "    'data_path':'Data_Processed/all_but_one/AMI-2020/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/AMI-2020/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/AMI-2020/all_but_one/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'seed_val': 42,\n",
    "    'batch_size': 8,\n",
    "#         'bert_model': \"aubmindlab/bert-base-arabertv02\",\n",
    "    'bert_model': \"bert-base-multilingual-cased\",\n",
    "#         'bert_model': \"bert-base-uncased\",\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 10,\n",
    "    'max_len': 128,\n",
    "    'device': 'cuda',\n",
    "    'weights': [1.0, 1.0],\n",
    "    'save_model': True,\n",
    "    'model_save_path': 'Saved_Models/AMI-2020/all_but_one/',\n",
    "}\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'bert',\n",
    "    'data_path':'Data_Processed/all_but_one/AMI-Spanish/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/AMI-Spanish/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/AMI-Spanish/all_but_one/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'seed_val': 42,\n",
    "    'batch_size': 8,\n",
    "#         'bert_model': \"aubmindlab/bert-base-arabertv02\",\n",
    "    'bert_model': \"bert-base-multilingual-cased\",\n",
    "#         'bert_model': \"bert-base-uncased\",\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 10,\n",
    "    'max_len': 128,\n",
    "    'device': 'cuda',\n",
    "    'weights': [1.0, 1.0],\n",
    "    'save_model': True,\n",
    "    'model_save_path': 'Saved_Models/AMI-Spanish/all_but_one/',\n",
    "}\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'bert',\n",
    "    'data_path':'Data_Processed/all_but_one/Shared_Task_hin/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Shared_Task_hin/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_hin/all_but_one/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'seed_val': 42,\n",
    "    'batch_size': 8,\n",
    "#         'bert_model': \"aubmindlab/bert-base-arabertv02\",\n",
    "    'bert_model': \"bert-base-multilingual-cased\",\n",
    "#         'bert_model': \"bert-base-uncased\",\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 10,\n",
    "    'max_len': 128,\n",
    "    'device': 'cuda',\n",
    "    'weights': [1.0, 4.5],\n",
    "    'save_model': True,\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_hin/all_but_one/',\n",
    "}\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'bert',\n",
    "    'data_path':'Data_Processed/all_but_one/Shared_Task_iben/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Shared_Task_iben/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_iben/all_but_one/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'seed_val': 42,\n",
    "    'batch_size': 8,\n",
    "#         'bert_model': \"aubmindlab/bert-base-arabertv02\",\n",
    "    'bert_model': \"bert-base-multilingual-cased\",\n",
    "#         'bert_model': \"bert-base-uncased\",\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 10,\n",
    "    'max_len': 128,\n",
    "    'device': 'cuda',\n",
    "    'weights': [1.0, 6.0],\n",
    "    'save_model': True,\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_iben/all_but_one/',\n",
    "}\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'bert',\n",
    "    'data_path':'Data_Processed/all_but_one/Shared_Task_eng/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Shared_Task_eng/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_eng/all_but_one/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'seed_val': 42,\n",
    "    'batch_size': 8,\n",
    "#         'bert_model': \"aubmindlab/bert-base-arabertv02\",\n",
    "    'bert_model': \"bert-base-multilingual-cased\",\n",
    "#         'bert_model': \"bert-base-uncased\",\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 10,\n",
    "    'max_len': 128,\n",
    "    'device': 'cuda',\n",
    "    'weights': [1.0, 8.0],\n",
    "    'save_model': True,\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_eng/all_but_one/',\n",
    "}\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import random\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Tokeniser\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# Utility\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Optimiser\n",
    "from transformers import AdamW\n",
    "\n",
    "# Model\n",
    "\n",
    "import torch.nn as nn\n",
    "from models import weighted_Roberta\n",
    "\n",
    "\n",
    "class XLM_Roberta:\n",
    "    def __init__(self,args):\n",
    "        # fix the random\n",
    "        random.seed(args['seed_val'])\n",
    "        np.random.seed(args['seed_val'])\n",
    "        torch.manual_seed(args['seed_val'])\n",
    "        torch.cuda.manual_seed_all(args['seed_val'])\n",
    "        \n",
    "        # set device\n",
    "        self.device = torch.device(args['device'])\n",
    "\n",
    "        self.weights=args['weights']\n",
    "        \n",
    "        # initiliase tokeniser\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case = True)\n",
    "\n",
    "        self.model_save_path = args['model_save_path']\n",
    "        self.name = args['name']\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Utility Functions -----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def encode(self,data,max_len):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sent in tqdm(data):\n",
    "            # use in-built tokeniser of Bert\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                            sent,\n",
    "                            add_special_tokens =True, # for [CLS] and [SEP]\n",
    "                            max_length = max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = True,\n",
    "#                             return_tensors = 'pt', # return pytorch tensors\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            # attention masks notify where padding has been added \n",
    "            # and where is the sentence\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            X_data = torch.tensor(input_ids)\n",
    "            attention_masks_data = torch.tensor(attention_masks)\n",
    "            \n",
    "        return [X_data,attention_masks_data]\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##------------------ Dataloader -----------------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def get_dataloader(self,samples, batch_size,is_train=False):\n",
    "        inputs,masks,labels = samples\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "        inputs = torch.cat(inputs, dim=0)\n",
    "        masks = torch.cat(masks, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # convert to dataset\n",
    "        data = TensorDataset(inputs,masks,labels)\n",
    "\n",
    "        if(is_train==False):\n",
    "            # use random sampler for training to shuffle\n",
    "            # train data\n",
    "            sampler = SequentialSampler(data)\n",
    "        else:\n",
    "            # order does not matter for validation as we just \n",
    "            # need the metrics\n",
    "            sampler = RandomSampler(data)  \n",
    "\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size,drop_last=True)\n",
    "\n",
    "        return dataloader\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Training Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def get_optimiser(self,learning_rate,model):\n",
    "        # using AdamW optimiser from transformers library\n",
    "        return AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    def get_scheduler(self,epochs,optimiser,train_dl):\n",
    "        total_steps = len(train_dl) * epochs\n",
    "        return get_linear_schedule_with_warmup(optimiser, \n",
    "                num_warmup_steps = 0, \n",
    "                num_training_steps = total_steps)\n",
    "    \n",
    "    def evalMetric(self, y_true, y_pred, prefix):\n",
    "        # calculate all the metrics and add prefix to them\n",
    "        # before saving in dictionary\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "        f1Score = f1_score(y_true, y_pred)\n",
    "        area_under_c = roc_auc_score(y_true, y_pred)\n",
    "        recallScore = recall_score(y_true, y_pred)\n",
    "        precisionScore = precision_score(y_true, y_pred)\n",
    "\n",
    "        nonhate_f1Score = f1_score(y_true, y_pred, pos_label=0)\n",
    "        non_recallScore = recall_score(y_true, y_pred, pos_label=0)\n",
    "        non_precisionScore = precision_score(y_true, y_pred, pos_label=0)\n",
    "        return {prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "            prefix+'f1Score': f1Score, prefix+'auc': area_under_c,\n",
    "            prefix+'precision': precisionScore, \n",
    "            prefix+'recall': recallScore, \n",
    "            prefix+'non_hatef1Score': nonhate_f1Score, \n",
    "            prefix+'non_recallScore': non_recallScore, \n",
    "            prefix+'non_precisionScore': non_precisionScore}\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##---------------- Different Train Loops --------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def evaluate(self,model,loader,which):\n",
    "        # to evaluate model on test and validation set\n",
    "\n",
    "        model.eval() # put model in eval mode\n",
    "\n",
    "        # maintain total loss to save in metrics\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(loader):\n",
    "            # separate input, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            with torch.no_grad(): # do not construct compute graph\n",
    "                outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            # output is always a tuple, thus we have to \n",
    "            # separate it manually\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "            # add the current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # calculate true labels and convert it into numpy array\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "            \n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = self.evalMetric(y_true,y_pred,which+\"_\")\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_loss = total_eval_loss / len(loader)\n",
    "        # add it to the metric\n",
    "        metrics[which+'_avg_loss'] = avg_loss\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def run_train_loop(self,model,train_loader,optimiser,scheduler):\n",
    "\n",
    "        model.train() # put model in train mode\n",
    "\n",
    "        # maintain total loss to add to metric\n",
    "        total_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            # separate inputs, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            # Ref: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch#:~:text=In%20PyTorch%20%2C%20we%20need%20to,backward()%20call.\n",
    "            model.zero_grad()                \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "            # outputs is always returned as tuple\n",
    "            # Separate it manually\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float).to(self.device))\n",
    "            \n",
    "            loss = loss_fct(logits,b_labels)\n",
    "            \n",
    "            # calculate current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Back-propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # calculate true labels\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "            # clip gradient to prevent exploding gradient\n",
    "            # problems\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # gradient descent\n",
    "            optimiser.step()\n",
    "            \n",
    "            # schedule learning rate accordingly\n",
    "            scheduler.step()\n",
    "\n",
    "        # calculate avg loss \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # calculate metrics\n",
    "        train_metrics = self.evalMetric(y_true,y_pred,\"Train_\")\n",
    "        \n",
    "        # print results\n",
    "        print('avg_train_loss',avg_train_loss)\n",
    "        print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "        print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "        # add loss to metrics\n",
    "        train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "        return train_metrics\n",
    "    \n",
    "    \n",
    "    ##------------------------------------------------------------##\n",
    "    ##----------------- Main Train Loop --------------------------##\n",
    "    ##------------------------------------------------------------##\n",
    "    def train(self,model,data_loaders,optimiser,scheduler,epochs,save_model):\n",
    "        # save train stats per epoch\n",
    "        train_stats = []\n",
    "        train_loader,val_loader,test_loader = data_loaders\n",
    "        # maintain best mF1 Score to save best model\n",
    "        best_mf1Score=-1.0\n",
    "        for epoch_i in range(0, epochs):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            \n",
    "            print(\"\")\n",
    "            print('Training...')\n",
    "            # run trian loop\n",
    "            train_metrics = self.run_train_loop(model,train_loader,\n",
    "                                            optimiser,scheduler)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\") \n",
    "            # test on validation set\n",
    "            val_metrics = self.evaluate(model,val_loader,\"Val\")\n",
    "            \n",
    "            print(\"Validation Loss: \",val_metrics['Val_avg_loss'])\n",
    "            print(\"Validation Accuracy: \",val_metrics['Val_accuracy'])\n",
    "            \n",
    "            stats = {}\n",
    "\n",
    "            # save model where validation mF1Score is best\n",
    "            if(val_metrics['Val_mF1Score']>best_mf1Score):\n",
    "                best_mf1Score=val_metrics['Val_mF1Score']\n",
    "                if(save_model):\n",
    "                    torch.save(model.state_dict(), self.model_save_path+\n",
    "                        '/best_bert_'+self.name+'.pt')\n",
    "                # evaluate best model on test set\n",
    "                test_metrics = self.evaluate(model,test_loader,\"Test\")\n",
    "\n",
    "            stats['epoch']=epoch_i+1\n",
    "\n",
    "            # add train and val metrics of the epoch to \n",
    "            # same dictionary\n",
    "            stats.update(train_metrics)\n",
    "            stats.update(val_metrics)\n",
    "\n",
    "            train_stats.append(stats)\n",
    "\n",
    "        return train_stats,test_metrics\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------------- Main Pipeline ---------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run(self,args,df_train,df_val,df_test):\n",
    "        # get X and Y data points \n",
    "        X_train = df_train['Text'].values\n",
    "        Y_train = df_train['Label'].values\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "        X_val = df_val['Text'].values\n",
    "        Y_val = df_val['Label'].values\n",
    "        \n",
    "        # encode data\n",
    "        # returns list of data and attention masks\n",
    "        train_data = self.encode(X_train,args['max_len'])\n",
    "        val_data = self.encode(X_val,args['max_len'])\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "        \n",
    "        # add labels to data so that we can send them to\n",
    "        # dataloader function together\n",
    "        train_data.append(Y_train)\n",
    "        val_data.append(Y_val)\n",
    "        test_data.append(Y_test)\n",
    "        \n",
    "        # convert to dataloader\n",
    "        train_dl =self.get_dataloader(train_data,args['batch_size'],True)\n",
    "        val_dl =self.get_dataloader(val_data,args['batch_size'])                          \n",
    "        test_dl =self.get_dataloader(test_data,args['batch_size'])\n",
    "        \n",
    "        # intialise model\n",
    "        model = weighted_Roberta.from_pretrained(\n",
    "            'xlm-roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
    "            params=args['params'],\n",
    "        )\n",
    "        model.to(self.device)\n",
    "        \n",
    "        optimiser = self.get_optimiser(args['learning_rate'],model)\n",
    "        \n",
    "        scheduler = self.get_scheduler(args['epochs'],optimiser,train_dl)\n",
    "        \n",
    "        # Run train loop and evaluate on validation data set\n",
    "        # on each epoch. Store best model from all epochs \n",
    "        # (best mF1 Score on Val set) and evaluate it on\n",
    "        # test set\n",
    "        train_stats,train_metrics = self.train(model,[train_dl,val_dl,test_dl],\n",
    "                                optimiser,scheduler,args['epochs'],args['save_model'])\n",
    "        \n",
    "        return train_stats,train_metrics\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##-------------------- Other Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run_test(self,model,df_test,args):\n",
    "        # to evaluate test set on the final saved model\n",
    "        # to retrieve results if necessary\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "\n",
    "        test_data.append(Y_test)\n",
    "\n",
    "        test_dl =self.get_dataloader(test_data,32)\n",
    "\n",
    "        metrics = self.evaluate(model,test_dl,\"Test\")\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def load_model(self,path,args):\n",
    "        # load saved best model\n",
    "        model = XLM_RobertaModel.from_pretrained(\n",
    "            args['bert_model'], \n",
    "            num_labels = 2, # The number of output labels--2 for binary classification                \n",
    "            args=args,\n",
    "        )\n",
    "        \n",
    "        saved_model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        return saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xlm(args, index,all_test_metrics,model_args):\n",
    "    model_name = args['model_name']\n",
    "    model_args['name']=model_name+'_'+str(index)+'_all'\n",
    "    print(\"\\tInitialising Model....\")\n",
    "    model = XLM_Roberta(model_args)\n",
    "    print(\"\\tLoading Dataset....\")\n",
    "    df_train, df_val, df_test = load_dataset(args,index)\n",
    "    print(\"\\tTraining Starts....\")\n",
    "    train_metrics, test_metrics = model.run(model_args, \n",
    "                    df_train, df_val, df_test)\n",
    "    test_metrics['name']=model_args['name']\n",
    "\n",
    "    # Save train metrics after generating path\n",
    "    res_path=args['res_base_path']+model_name+'_'+model_args['name']\n",
    "    save_metrics(res_path,train_metrics,\"train\")\n",
    "    \n",
    "    all_test_metrics.append(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xlm(args,model_args):\n",
    "    all_test_metrics=[]\n",
    "    \n",
    "    for fold in [1, 2, 3, 4, 5]:\n",
    "        print(\"Fold: \",fold)\n",
    "        fix_random()\n",
    "        train_xlm(args,fold,all_test_metrics,model_args)\n",
    "        print(\"Saving Test Metrics....\")\n",
    "        save_metrics(args['res_base_path']+args['model_name']+\n",
    "             '_all',all_test_metrics,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29202/29202 [00:04<00:00, 6788.76it/s]\n",
      "100%|██████████| 4171/4171 [00:00<00:00, 6339.59it/s]\n",
      "100%|██████████| 8343/8343 [00:01<00:00, 6234.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29202/29202 [1:16:52<00:00,  6.33it/s]\n",
      "100%|██████████| 4171/4171 [01:26<00:00, 48.39it/s] \n",
      "100%|██████████| 8343/8343 [05:44<00:00, 24.25it/s] \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cat() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-71f626712ac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mrun_xlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-321d9958cbc4>\u001b[0m in \u001b[0;36mrun_xlm\u001b[0;34m(args, model_args)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fold: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfix_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtrain_xlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_test_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving Test Metrics....\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         save_metrics(args['res_base_path']+args['model_name']+\n",
      "\u001b[0;32m<ipython-input-11-b96059121067>\u001b[0m in \u001b[0;36mtrain_xlm\u001b[0;34m(args, index, all_test_metrics, model_args)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tTraining Starts....\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     train_metrics, test_metrics = model.run(model_args, \n\u001b[0m\u001b[1;32m     10\u001b[0m                     df_train, df_val, df_test)\n\u001b[1;32m     11\u001b[0m     \u001b[0mtest_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-27d6f690b246>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, args, df_train, df_val, df_test)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# convert to dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0mval_dl\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mtest_dl\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-27d6f690b246>\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(self, samples, batch_size, is_train)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Convert the lists into tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cat() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/all_but_one/Let-Mi/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Let-Mi/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Let-Mi/all_but_one/',\n",
    "    'isArabic': True,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'seed_val': 42,\n",
    "    'batch_size': 8,\n",
    "#         'bert_model': \"aubmindlab/bert-base-arabertv02\",\n",
    "    'bert_model': \"xlm-roberta-base\",\n",
    "#         'bert_model': \"bert-base-uncased\",\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 10,\n",
    "    'max_len': 128,\n",
    "    'device': 'cuda:1',\n",
    "    'weights': [1.0, 1.0],\n",
    "    'save_model': True,\n",
    "    'model_save_path': 'Saved_Models/Let-Mi/all_but_one/',\n",
    "}\n",
    "\n",
    "run_xlm(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_berta',\n",
    "    'data_path':'Data_Processed/all_but_one/AMI-2020/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/AMI-2020/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/AMI-2020/all_but_one/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'seed_val': 42,\n",
    "    'batch_size': 8,\n",
    "#         'bert_model': \"aubmindlab/bert-base-arabertv02\",\n",
    "    'bert_model': \"xlm-roberta-base\",\n",
    "#         'bert_model': \"bert-base-uncased\",\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 10,\n",
    "    'max_len': 128,\n",
    "    'device': 'cuda:1',\n",
    "    'weights': [1.0, 1.0],\n",
    "    'save_model': True,\n",
    "    'model_save_path': 'Saved_Models/AMI-2020/all_but_one/',\n",
    "}\n",
    "\n",
    "run_xlm(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/all_but_one/AMI-Spanish/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/AMI-Spanish/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/AMI-Spanish/all_but_one/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'seed_val': 42,\n",
    "    'batch_size': 8,\n",
    "#         'bert_model': \"aubmindlab/bert-base-arabertv02\",\n",
    "    'bert_model': \"xlm-roberta-base\",\n",
    "#         'bert_model': \"bert-base-uncased\",\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 10,\n",
    "    'max_len': 128,\n",
    "    'device': 'cuda:1',\n",
    "    'weights': [1.0, 1.0],\n",
    "    'save_model': True,\n",
    "    'model_save_path': 'Saved_Models/AMI-Spanish/all_but_one/',\n",
    "}\n",
    "\n",
    "run_xlm(run_args,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
