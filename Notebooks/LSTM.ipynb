{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"Data_Processed/Shared_Task_eng/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = pd.read_csv(DATA_FOLDER+'val_1.csv')\n",
    "data_test = pd.read_csv(DATA_FOLDER+'test_1.csv')\n",
    "data_train = pd.read_csv(DATA_FOLDER+'train_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  1309 non-null   int64 \n",
      " 1   ID          1309 non-null   int64 \n",
      " 2   Text        1308 non-null   object\n",
      " 3   Label       1309 non-null   int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 41.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val.dropna(inplace=True)\n",
    "data_test.dropna(inplace=True)\n",
    "data_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9161"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(880, 8281)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train[data_train['Label']==1]),len(data_train[data_train['Label']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "- Ref: https://medium.com/mlearning-ai/load-pre-trained-glove-embeddings-in-torch-nn-embedding-layer-in-under-2-minutes-f5af8f57416a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idx,embeddings = {},[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_PATH = \"Embeddings/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EMBEDDING_PATH,'rt') as f:\n",
    "    full_content = f.read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,line in enumerate(full_content):\n",
    "    word = line.split(' ')[0]\n",
    "    embedding = [float(val) for val in line.split(' ')[1:]]\n",
    "    vocab2idx[word]=i\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 400000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab2idx),len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embs_np = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding for '<pad>' token: 0s\n",
    "pad_emb_np = np.zeros((1,embs_np.shape[1]))   \n",
    "#embedding for '<unk>' token: mean\n",
    "unk_emb_np = np.mean(embs_np,axis=0,keepdims=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert these embeddings at the top of embs_npa\n",
    "embs_np = np.vstack((embs_np,pad_emb_np,unk_emb_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400002, 100)\n"
     ]
    }
   ],
   "source": [
    "print(embs_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs_np[400000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idx['<unk>']=400001\n",
    "vocab2idx['<pad>']=400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400002"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data_train.iloc[101]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bro thats needy as fuck'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text,max_len=MAX_LEN):\n",
    "    encoded=[]\n",
    "    for word in text.split(' '):\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            idx = vocab2idx[word]\n",
    "        except:\n",
    "            idx = vocab2idx['<unk>']\n",
    "        encoded.append(idx)\n",
    "    if(len(encoded)<max_len):\n",
    "        padding = [vocab2idx['<pad>']]*(max_len-len(encoded))\n",
    "        encoded.extend(padding)\n",
    "    else:\n",
    "        encoded=encoded[:max_len]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = encode(sample,MAX_LEN)\n",
    "len(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_np).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 100])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.LongTensor(encoded)\n",
    "emb_layer(input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Embedding npa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPA_PATH = \"Embeddings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NPA_PATH+'embs_np.npy','wb') as f:\n",
    "    np.save(f,embs_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train['Text'].values\n",
    "Y_train = data_train['Label'].values\n",
    "X_test = data_test['Text'].values\n",
    "Y_test = data_test['Label'].values\n",
    "X_val = data_val['Text'].values\n",
    "Y_val = data_val['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [encode(text) for text in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9161, 9161)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs),len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(X,Y,batch_size,is_train=False):\n",
    "    inputs = [encode(text) for text in X]\n",
    "    labels = Y\n",
    "    \n",
    "    inputs = torch.tensor(inputs)\n",
    "    labels = torch.tensor(labels,dtype=torch.long)\n",
    "    \n",
    "    data = TensorDataset(inputs,labels)\n",
    "    \n",
    "    if(is_train==False):\n",
    "        sampler = SequentialSampler(data)\n",
    "    else:\n",
    "        sampler = RandomSampler(data) \n",
    "        \n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloader(X_train,Y_train,BATCH_SIZE,True)\n",
    "val_loader = get_dataloader(X_val,Y_val,BATCH_SIZE)\n",
    "test_loader = get_dataloader(X_test,Y_test,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, weights,dimension=128):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.weights = weights\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embs_np).float())\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=100,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc = nn.Linear(2*dimension, 2)\n",
    "\n",
    "    def forward(self, text,labels):\n",
    "        text_len = 1024\n",
    "\n",
    "        text_emb = self.embedding(text)\n",
    "\n",
    "        #packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.lstm(text_emb)\n",
    "        #output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "        out_reverse = output[:, 0, self.dimension:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        output = self.fc(text_fea)\n",
    "        #text_fea = torch.squeeze(text_fea, 1)\n",
    "        #text_out = torch.sigmoid(text_fea)\n",
    "    \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(self.weights,dtype=torch.float))\n",
    "        \n",
    "        #oss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fct(output,labels)\n",
    "        return loss,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =LSTM(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    " optimizer = AdamW(model.parameters(),\n",
    "                  lr = LR, \n",
    "                  eps = 1e-8\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def fix_the_random(seed_val = 42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_the_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test output and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6860517859458923\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1])\n",
      "[0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "0.71875\n",
      "0.18181818181818182\n",
      "0.6916463375091553\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "0.75\n",
      "0.0\n",
      "0.6878390908241272\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1])\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "0.78125\n",
      "0.0\n",
      "0.691704273223877\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "0.65625\n",
      "0.0\n",
      "0.6998888850212097\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0]\n",
      "0.65625\n",
      "0.15384615384615383\n",
      "0.677577555179596\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0.78125\n",
      "0.0\n",
      "0.6838791966438293\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0.8125\n",
      "0.0\n",
      "0.6567132472991943\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "0.875\n",
      "0.3333333333333333\n",
      "0.6779935956001282\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "0.84375\n",
      "0.0\n",
      "0.6899706721305847\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0])\n",
      "[0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0.78125\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for batch in train_loader:\n",
    "    i+=1\n",
    "    inputs= batch[0].to(device)\n",
    "    labels = batch[1].to(device)\n",
    "    \n",
    "#     print(inputs)\n",
    "    \n",
    "    output = model(inputs,labels)\n",
    "    \n",
    "    loss = output[0]\n",
    "    logits = output[1]\n",
    "    \n",
    "    print(loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    y_true = labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "    y_pred = torch.max(logits,1)[1]\n",
    "    \n",
    "    print(y_pred),print(y_true)\n",
    "    y_pred = y_pred.cpu().data.squeeze().numpy()\n",
    "    \n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    print(f1_score(y_true, y_pred, labels = np.unique(y_pred)))\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if(i==10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "        0, 1, 1, 1, 1, 0, 1, 1]\n",
    "y_true=[0,1,0,1,0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMetric(y_true, y_pred,prefix):\n",
    "   accuracy = accuracy_score(y_true, y_pred)\n",
    "   mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "   f1Score  = f1_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "   fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "   area_under_c = auc(fpr, tpr)\n",
    "   recallScore = recall_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "   precisionScore = precision_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "   return dict({prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "                prefix+'f1Score': f1Score, prefix+'precision': precisionScore, \n",
    "                prefix+'recall': recallScore})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateOnData(model,loader):\n",
    "    \n",
    "    model.eval() # put model in eval mode\n",
    "    \n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    y_pred = np.zeros(shape=(0),dtype='int')\n",
    "    y_true = np.empty(shape=(0),dtype='int')\n",
    "    \n",
    "    for batch in loader:\n",
    "        b_inputs = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad(): # do not construct compute graph\n",
    "            outputs = model(b_inputs,b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "        b_y_pred = torch.max(logits,1)[1]\n",
    "        b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "        y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "        y_true = np.concatenate((y_true,b_y_true))\n",
    "        \n",
    "    metrics = evalMetric(y_true,y_pred,\"Val_\")\n",
    "\n",
    "    print(\" Validation Accuracy: {0:.2f}\".format(metrics['Val_accuracy']))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_loss = total_eval_loss / len(loader)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_loss))\n",
    "\n",
    "    metrics['Val_avg_loss'] = avg_loss\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTrainLoop(model,train_loader,t0,optimiser):\n",
    "    print(\"\")\n",
    "    print('Training...')\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    model.train() # put model in train mode\n",
    "\n",
    "    y_pred = np.zeros(shape=(0),dtype='int')\n",
    "    y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in tqdm(enumerate(train_loader)):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        b_inputs = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "        \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_inputs,b_labels)\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print('batch_loss',loss.item())\n",
    "\n",
    "        #Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # Compute True and predicted labels to get\n",
    "        # train metrics\n",
    "        b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "        b_y_pred = torch.max(logits,1)[1]\n",
    "        b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "        \n",
    "        # accumulate b_y_pred and b_y_true for each batch\n",
    "        # and evaluate at once\n",
    "        y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "        y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        #scheduler.step()\n",
    "        \n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    train_metrics = evalMetric(y_true,y_pred,\"Train_\")\n",
    "\n",
    "    print('avg_train_loss',avg_train_loss)\n",
    "    print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "    print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "    train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "    return train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,val_loader,optimiser,epochs):\n",
    "    train_stats = []\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "\n",
    "          # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        train_metrics = runTrainLoop(model,train_loader,t0,optimiser)\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\") \n",
    "        val_metrics = EvaluateOnData(model,val_loader)\n",
    "        \n",
    "        stats['epoch']=epoch_i+1\n",
    "        \n",
    "        stats.update(train_metrics)\n",
    "        stats.update(val_metrics)\n",
    "\n",
    "        train_stats.append(stats)\n",
    "    \n",
    "    return train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:21,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.694582998752594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:45,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.713119626045227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [04:10,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.636775553226471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [05:41,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.734338104724884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [07:14,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4468008875846863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [08:54,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6707899570465088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [10:39,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6639588475227356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [10:56,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6365118128497426\n",
      "train_f1Score 0.015590200445434297\n",
      "train_accuracy 0.903503984281192\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.63\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:38,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5842412710189819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [03:25,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5210427045822144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [04:54,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.42139768600463867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [06:20,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5073403120040894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [07:52,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.45949965715408325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [09:16,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5583552122116089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [10:46,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5163238048553467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [10:58,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5784645129371603\n",
      "train_f1Score 0.19504643962848298\n",
      "train_accuracy 0.8864752756249318\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.73\n",
      "  Validation Loss: 0.55\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:29,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4915522038936615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [03:05,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.43567827343940735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [04:41,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5196518898010254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [06:19,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5538341403007507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [07:55,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6837435364723206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [09:31,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5436350107192993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [11:09,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6906737089157104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [11:23,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5419966364052238\n",
      "train_f1Score 0.3194125745754933\n",
      "train_accuracy 0.8381181093767056\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.81\n",
      "  Validation Loss: 0.54\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:34,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.3729540705680847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [03:14,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7452590465545654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [04:58,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.2327832579612732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [06:38,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6020908951759338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [08:20,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5579232573509216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [10:03,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.45141440629959106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [11:45,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.3598862290382385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [12:02,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5197686439814884\n",
      "train_f1Score 0.34669067987393065\n",
      "train_accuracy 0.8416111778190154\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.77\n",
      "  Validation Loss: 0.53\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:41,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.40852102637290955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [03:26,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.334728866815567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [05:08,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.794390857219696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [06:54,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.38498279452323914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [08:42,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4041173458099365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [10:29,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5076082944869995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [12:18,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.3602437973022461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [12:35,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5044918974102166\n",
      "train_f1Score 0.37775735294117646\n",
      "train_accuracy 0.852199541534767\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.88\n",
      "  Validation Loss: 0.55\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:45,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.26938101649284363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [03:33,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6233281493186951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [05:23,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6074079871177673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [07:13,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5119422078132629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [09:02,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5399578809738159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [10:51,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4891956150531769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [12:41,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7480677962303162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [12:59,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5038676430643227\n",
      "train_f1Score 0.38548307994114756\n",
      "train_accuracy 0.8632245388058072\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.80\n",
      "  Validation Loss: 0.52\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:46,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.43802499771118164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [03:35,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5172142386436462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [05:23,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.3982948958873749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [07:13,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.3961125612258911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [09:03,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7021610736846924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [10:57,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.8751559853553772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [12:47,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.8478622436523438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [13:05,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.49057138532088607\n",
      "train_f1Score 0.3899596593455849\n",
      "train_accuracy 0.8514354328130117\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.86\n",
      "  Validation Loss: 0.53\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:47,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.34622833132743835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [03:40,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.8159197568893433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [05:35,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.36838939785957336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [07:27,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.33635732531547546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [09:25,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.43092402815818787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [11:18,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4542291760444641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [13:11,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.3616260588169098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [13:27,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.48380928499565723\n",
      "train_f1Score 0.40814393939393934\n",
      "train_accuracy 0.8635520139722738\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.84\n",
      "  Validation Loss: 0.52\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:39,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5086437463760376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [03:18,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.29981809854507446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [05:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4857662618160248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [06:46,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5236760973930359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [08:27,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.13514329493045807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [10:14,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4792105555534363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [12:00,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.6411433219909668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [12:17,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.482620335102912\n",
      "train_f1Score 0.40517626059794737\n",
      "train_accuracy 0.8544918677000327\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.80\n",
      "  Validation Loss: 0.52\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:38,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.7466442584991455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [03:23,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5486990809440613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [05:09,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.615241289138794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [06:52,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.40957292914390564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [08:34,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.5289916396141052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [10:16,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.44440916180610657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [12:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 0.4740843176841736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [12:17,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.4726619808204498\n",
      "train_f1Score 0.4121878967414304\n",
      "train_accuracy 0.8483789979259906\n",
      "\n",
      "Running Validation...\n",
      " Validation Accuracy: 0.84\n",
      "  Validation Loss: 0.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'train metrics': {'accuracy': 0.903503984281192,\n",
       "   'mF1Score': 0.48242779076449854,\n",
       "   'f1Score': 0.015590200445434297,\n",
       "   'precision': 0.3888888888888889,\n",
       "   'recall': 0.007954545454545454,\n",
       "   'avg_train_loss': 0.6365118128497426},\n",
       "  'val_metrics': {'accuracy': 0.9029051987767585,\n",
       "   'mF1Score': 0.4822191675783844,\n",
       "   'f1Score': 0.015503875968992248,\n",
       "   'precision': 0.25,\n",
       "   'recall': 0.008,\n",
       "   'avg_val_loss': 0.6279183124623647}},\n",
       " {'epoch': 2,\n",
       "  'train metrics': {'accuracy': 0.8864752756249318,\n",
       "   'mF1Score': 0.5669888686692033,\n",
       "   'f1Score': 0.19504643962848298,\n",
       "   'precision': 0.3058252427184466,\n",
       "   'recall': 0.1431818181818182,\n",
       "   'avg_train_loss': 0.5784645129371603},\n",
       "  'val_metrics': {'accuracy': 0.7270642201834863,\n",
       "   'mF1Score': 0.565886831744379,\n",
       "   'f1Score': 0.30136986301369867,\n",
       "   'precision': 0.19948186528497408,\n",
       "   'recall': 0.616,\n",
       "   'avg_val_loss': 0.5529958758412338}},\n",
       " {'epoch': 3,\n",
       "  'train metrics': {'accuracy': 0.8381181093767056,\n",
       "   'mF1Score': 0.6137730654578513,\n",
       "   'f1Score': 0.3194125745754933,\n",
       "   'precision': 0.2678983833718245,\n",
       "   'recall': 0.39545454545454545,\n",
       "   'avg_train_loss': 0.5419966364052238},\n",
       "  'val_metrics': {'accuracy': 0.8111620795107034,\n",
       "   'mF1Score': 0.6189346125311533,\n",
       "   'f1Score': 0.34828496042216356,\n",
       "   'precision': 0.25984251968503935,\n",
       "   'recall': 0.528,\n",
       "   'avg_val_loss': 0.5423922451531015}},\n",
       " {'epoch': 4,\n",
       "  'train metrics': {'accuracy': 0.8416111778190154,\n",
       "   'mF1Score': 0.6282860268508217,\n",
       "   'f1Score': 0.34669067987393065,\n",
       "   'precision': 0.2870991797166294,\n",
       "   'recall': 0.4375,\n",
       "   'avg_train_loss': 0.5197686439814884},\n",
       "  'val_metrics': {'accuracy': 0.7668195718654435,\n",
       "   'mF1Score': 0.5942659839004124,\n",
       "   'f1Score': 0.32967032967032966,\n",
       "   'precision': 0.22727272727272727,\n",
       "   'recall': 0.6,\n",
       "   'avg_val_loss': 0.5303168107823628}},\n",
       " {'epoch': 5,\n",
       "  'train metrics': {'accuracy': 0.852199541534767,\n",
       "   'mF1Score': 0.6469487867146115,\n",
       "   'f1Score': 0.37775735294117646,\n",
       "   'precision': 0.31712962962962965,\n",
       "   'recall': 0.46704545454545454,\n",
       "   'avg_train_loss': 0.5044918974102166},\n",
       "  'val_metrics': {'accuracy': 0.8753822629969419,\n",
       "   'mF1Score': 0.6408379168316164,\n",
       "   'f1Score': 0.35059760956175295,\n",
       "   'precision': 0.3492063492063492,\n",
       "   'recall': 0.352,\n",
       "   'avg_val_loss': 0.5513924912708562}},\n",
       " {'epoch': 6,\n",
       "  'train metrics': {'accuracy': 0.8632245388058072,\n",
       "   'mF1Score': 0.6542658291064823,\n",
       "   'f1Score': 0.38548307994114756,\n",
       "   'precision': 0.33908541846419327,\n",
       "   'recall': 0.4465909090909091,\n",
       "   'avg_train_loss': 0.5038676430643227},\n",
       "  'val_metrics': {'accuracy': 0.7958715596330275,\n",
       "   'mF1Score': 0.6176590049429886,\n",
       "   'f1Score': 0.3566265060240964,\n",
       "   'precision': 0.25517241379310346,\n",
       "   'recall': 0.592,\n",
       "   'avg_val_loss': 0.5234039542151661}},\n",
       " {'epoch': 7,\n",
       "  'train metrics': {'accuracy': 0.8514354328130117,\n",
       "   'mF1Score': 0.6526891081514451,\n",
       "   'f1Score': 0.3899596593455849,\n",
       "   'precision': 0.3219837157660992,\n",
       "   'recall': 0.4943181818181818,\n",
       "   'avg_train_loss': 0.49057138532088607},\n",
       "  'val_metrics': {'accuracy': 0.8631498470948012,\n",
       "   'mF1Score': 0.6431648974677467,\n",
       "   'f1Score': 0.36298932384341637,\n",
       "   'precision': 0.3269230769230769,\n",
       "   'recall': 0.408,\n",
       "   'avg_val_loss': 0.529910690900756}},\n",
       " {'epoch': 8,\n",
       "  'train metrics': {'accuracy': 0.8635520139722738,\n",
       "   'mF1Score': 0.6655155230590918,\n",
       "   'f1Score': 0.40814393939393934,\n",
       "   'precision': 0.34983766233766234,\n",
       "   'recall': 0.48977272727272725,\n",
       "   'avg_train_loss': 0.48380928499565723},\n",
       "  'val_metrics': {'accuracy': 0.8363914373088684,\n",
       "   'mF1Score': 0.6308632076466989,\n",
       "   'f1Score': 0.355421686746988,\n",
       "   'precision': 0.28502415458937197,\n",
       "   'recall': 0.472,\n",
       "   'avg_val_loss': 0.5227955608833127}},\n",
       " {'epoch': 9,\n",
       "  'train metrics': {'accuracy': 0.8544918677000327,\n",
       "   'mF1Score': 0.6611417028379949,\n",
       "   'f1Score': 0.40517626059794737,\n",
       "   'precision': 0.33357825128581925,\n",
       "   'recall': 0.5159090909090909,\n",
       "   'avg_train_loss': 0.482620335102912},\n",
       "  'val_metrics': {'accuracy': 0.8012232415902141,\n",
       "   'mF1Score': 0.6163357400722023,\n",
       "   'f1Score': 0.35,\n",
       "   'precision': 0.2545454545454545,\n",
       "   'recall': 0.56,\n",
       "   'avg_val_loss': 0.5201039634099821}},\n",
       " {'epoch': 10,\n",
       "  'train metrics': {'accuracy': 0.8483789979259906,\n",
       "   'mF1Score': 0.662576184099771,\n",
       "   'f1Score': 0.4121878967414304,\n",
       "   'precision': 0.32838840188806473,\n",
       "   'recall': 0.553409090909091,\n",
       "   'avg_train_loss': 0.4726619808204498},\n",
       "  'val_metrics': {'accuracy': 0.8386850152905199,\n",
       "   'mF1Score': 0.638822982849889,\n",
       "   'f1Score': 0.3701492537313432,\n",
       "   'precision': 0.29523809523809524,\n",
       "   'recall': 0.496,\n",
       "   'avg_val_loss': 0.5321731847233888}}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model,train_loader,val_loader,optimizer,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = [{'epoch': 1,\n",
    "  'train metrics': {'accuracy': 0.903503984281192,\n",
    "   'mF1Score': 0.48242779076449854,\n",
    "   'f1Score': 0.015590200445434297,\n",
    "   'precision': 0.3888888888888889,\n",
    "   'recall': 0.007954545454545454,\n",
    "   'avg_train_loss': 0.6365118128497426},\n",
    "  'val_metrics': {'accuracy': 0.9029051987767585,\n",
    "   'mF1Score': 0.4822191675783844,\n",
    "   'f1Score': 0.015503875968992248,\n",
    "   'precision': 0.25,\n",
    "   'recall': 0.008,\n",
    "   'avg_val_loss': 0.6279183124623647}},\n",
    " {'epoch': 2,\n",
    "  'train metrics': {'accuracy': 0.8864752756249318,\n",
    "   'mF1Score': 0.5669888686692033,\n",
    "   'f1Score': 0.19504643962848298,\n",
    "   'precision': 0.3058252427184466,\n",
    "   'recall': 0.1431818181818182,\n",
    "   'avg_train_loss': 0.5784645129371603},\n",
    "  'val_metrics': {'accuracy': 0.7270642201834863,\n",
    "   'mF1Score': 0.565886831744379,\n",
    "   'f1Score': 0.30136986301369867,\n",
    "   'precision': 0.19948186528497408,\n",
    "   'recall': 0.616,\n",
    "   'avg_val_loss': 0.5529958758412338}},\n",
    " {'epoch': 3,\n",
    "  'train metrics': {'accuracy': 0.8381181093767056,\n",
    "   'mF1Score': 0.6137730654578513,\n",
    "   'f1Score': 0.3194125745754933,\n",
    "   'precision': 0.2678983833718245,\n",
    "   'recall': 0.39545454545454545,\n",
    "   'avg_train_loss': 0.5419966364052238},\n",
    "  'val_metrics': {'accuracy': 0.8111620795107034,\n",
    "   'mF1Score': 0.6189346125311533,\n",
    "   'f1Score': 0.34828496042216356,\n",
    "   'precision': 0.25984251968503935,\n",
    "   'recall': 0.528,\n",
    "   'avg_val_loss': 0.5423922451531015}},\n",
    " {'epoch': 4,\n",
    "  'train metrics': {'accuracy': 0.8416111778190154,\n",
    "   'mF1Score': 0.6282860268508217,\n",
    "   'f1Score': 0.34669067987393065,\n",
    "   'precision': 0.2870991797166294,\n",
    "   'recall': 0.4375,\n",
    "   'avg_train_loss': 0.5197686439814884},\n",
    "  'val_metrics': {'accuracy': 0.7668195718654435,\n",
    "   'mF1Score': 0.5942659839004124,\n",
    "   'f1Score': 0.32967032967032966,\n",
    "   'precision': 0.22727272727272727,\n",
    "   'recall': 0.6,\n",
    "   'avg_val_loss': 0.5303168107823628}},\n",
    " {'epoch': 5,\n",
    "  'train metrics': {'accuracy': 0.852199541534767,\n",
    "   'mF1Score': 0.6469487867146115,\n",
    "   'f1Score': 0.37775735294117646,\n",
    "   'precision': 0.31712962962962965,\n",
    "   'recall': 0.46704545454545454,\n",
    "   'avg_train_loss': 0.5044918974102166},\n",
    "  'val_metrics': {'accuracy': 0.8753822629969419,\n",
    "   'mF1Score': 0.6408379168316164,\n",
    "   'f1Score': 0.35059760956175295,\n",
    "   'precision': 0.3492063492063492,\n",
    "   'recall': 0.352,\n",
    "   'avg_val_loss': 0.5513924912708562}},\n",
    " {'epoch': 6,\n",
    "  'train metrics': {'accuracy': 0.8632245388058072,\n",
    "   'mF1Score': 0.6542658291064823,\n",
    "   'f1Score': 0.38548307994114756,\n",
    "   'precision': 0.33908541846419327,\n",
    "   'recall': 0.4465909090909091,\n",
    "   'avg_train_loss': 0.5038676430643227},\n",
    "  'val_metrics': {'accuracy': 0.7958715596330275,\n",
    "   'mF1Score': 0.6176590049429886,\n",
    "   'f1Score': 0.3566265060240964,\n",
    "   'precision': 0.25517241379310346,\n",
    "   'recall': 0.592,\n",
    "   'avg_val_loss': 0.5234039542151661}},\n",
    " {'epoch': 7,\n",
    "  'train metrics': {'accuracy': 0.8514354328130117,\n",
    "   'mF1Score': 0.6526891081514451,\n",
    "   'f1Score': 0.3899596593455849,\n",
    "   'precision': 0.3219837157660992,\n",
    "   'recall': 0.4943181818181818,\n",
    "   'avg_train_loss': 0.49057138532088607},\n",
    "  'val_metrics': {'accuracy': 0.8631498470948012,\n",
    "   'mF1Score': 0.6431648974677467,\n",
    "   'f1Score': 0.36298932384341637,\n",
    "   'precision': 0.3269230769230769,\n",
    "   'recall': 0.408,\n",
    "   'avg_val_loss': 0.529910690900756}},\n",
    " {'epoch': 8,\n",
    "  'train metrics': {'accuracy': 0.8635520139722738,\n",
    "   'mF1Score': 0.6655155230590918,\n",
    "   'f1Score': 0.40814393939393934,\n",
    "   'precision': 0.34983766233766234,\n",
    "   'recall': 0.48977272727272725,\n",
    "   'avg_train_loss': 0.48380928499565723},\n",
    "  'val_metrics': {'accuracy': 0.8363914373088684,\n",
    "   'mF1Score': 0.6308632076466989,\n",
    "   'f1Score': 0.355421686746988,\n",
    "   'precision': 0.28502415458937197,\n",
    "   'recall': 0.472,\n",
    "   'avg_val_loss': 0.5227955608833127}},\n",
    " {'epoch': 9,\n",
    "  'train metrics': {'accuracy': 0.8544918677000327,\n",
    "   'mF1Score': 0.6611417028379949,\n",
    "   'f1Score': 0.40517626059794737,\n",
    "   'precision': 0.33357825128581925,\n",
    "   'recall': 0.5159090909090909,\n",
    "   'avg_train_loss': 0.482620335102912},\n",
    "  'val_metrics': {'accuracy': 0.8012232415902141,\n",
    "   'mF1Score': 0.6163357400722023,\n",
    "   'f1Score': 0.35,\n",
    "   'precision': 0.2545454545454545,\n",
    "   'recall': 0.56,\n",
    "   'avg_val_loss': 0.5201039634099821}},\n",
    " {'epoch': 10,\n",
    "  'train metrics': {'accuracy': 0.8483789979259906,\n",
    "   'mF1Score': 0.662576184099771,\n",
    "   'f1Score': 0.4121878967414304,\n",
    "   'precision': 0.32838840188806473,\n",
    "   'recall': 0.553409090909091,\n",
    "   'avg_train_loss': 0.4726619808204498},\n",
    "  'val_metrics': {'accuracy': 0.8386850152905199,\n",
    "   'mF1Score': 0.638822982849889,\n",
    "   'f1Score': 0.3701492537313432,\n",
    "   'precision': 0.29523809523809524,\n",
    "   'recall': 0.496,\n",
    "   'avg_val_loss': 0.5321731847233888}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processStats(stats):\n",
    "    data={}\n",
    "    cols = ['Epoch','Train_accuracy','Train_mF1Score',\n",
    "    'Train_f1Score','Train_precision','Train_recall',\n",
    "    'Train_avg_train_loss',\n",
    "    'Val_accuracy','Val_mF1Score',\n",
    "    'Val_f1Score','Val_precision','Val_recall',\n",
    "    'Val_avg_val_loss'\n",
    "    ]\n",
    "\n",
    "    for col in cols:\n",
    "        data[col]=[]\n",
    "    \n",
    "    for stat in stats:\n",
    "        data['Epoch'].append(stat['epoch'])\n",
    "\n",
    "        for key1,val1 in stat['train metrics'].items():\n",
    "            data['Train_'+key1].append(val1)\n",
    "\n",
    "        for key1,val1 in stat['val_metrics'].items():\n",
    "            data['Val_'+key1].append(val1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = processStats(train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.903503984281192,\n",
       " 'mF1Score': 0.48242779076449854,\n",
       " 'f1Score': 0.015590200445434297,\n",
       " 'precision': 0.3888888888888889,\n",
       " 'recall': 0.007954545454545454,\n",
       " 'avg_train_loss': 0.6365118128497426}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats[0]['train metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train_accuracy</th>\n",
       "      <th>Train_mF1Score</th>\n",
       "      <th>Train_f1Score</th>\n",
       "      <th>Train_precision</th>\n",
       "      <th>Train_recall</th>\n",
       "      <th>Train_avg_train_loss</th>\n",
       "      <th>Val_accuracy</th>\n",
       "      <th>Val_mF1Score</th>\n",
       "      <th>Val_f1Score</th>\n",
       "      <th>Val_precision</th>\n",
       "      <th>Val_recall</th>\n",
       "      <th>Val_avg_val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.903504</td>\n",
       "      <td>0.482428</td>\n",
       "      <td>0.015590</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.007955</td>\n",
       "      <td>0.636512</td>\n",
       "      <td>0.902905</td>\n",
       "      <td>0.482219</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.627918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.886475</td>\n",
       "      <td>0.566989</td>\n",
       "      <td>0.195046</td>\n",
       "      <td>0.305825</td>\n",
       "      <td>0.143182</td>\n",
       "      <td>0.578465</td>\n",
       "      <td>0.727064</td>\n",
       "      <td>0.565887</td>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.199482</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.552996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.838118</td>\n",
       "      <td>0.613773</td>\n",
       "      <td>0.319413</td>\n",
       "      <td>0.267898</td>\n",
       "      <td>0.395455</td>\n",
       "      <td>0.541997</td>\n",
       "      <td>0.811162</td>\n",
       "      <td>0.618935</td>\n",
       "      <td>0.348285</td>\n",
       "      <td>0.259843</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.542392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.841611</td>\n",
       "      <td>0.628286</td>\n",
       "      <td>0.346691</td>\n",
       "      <td>0.287099</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.519769</td>\n",
       "      <td>0.766820</td>\n",
       "      <td>0.594266</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.530317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.852200</td>\n",
       "      <td>0.646949</td>\n",
       "      <td>0.377757</td>\n",
       "      <td>0.317130</td>\n",
       "      <td>0.467045</td>\n",
       "      <td>0.504492</td>\n",
       "      <td>0.875382</td>\n",
       "      <td>0.640838</td>\n",
       "      <td>0.350598</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.551392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.863225</td>\n",
       "      <td>0.654266</td>\n",
       "      <td>0.385483</td>\n",
       "      <td>0.339085</td>\n",
       "      <td>0.446591</td>\n",
       "      <td>0.503868</td>\n",
       "      <td>0.795872</td>\n",
       "      <td>0.617659</td>\n",
       "      <td>0.356627</td>\n",
       "      <td>0.255172</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.523404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.851435</td>\n",
       "      <td>0.652689</td>\n",
       "      <td>0.389960</td>\n",
       "      <td>0.321984</td>\n",
       "      <td>0.494318</td>\n",
       "      <td>0.490571</td>\n",
       "      <td>0.863150</td>\n",
       "      <td>0.643165</td>\n",
       "      <td>0.362989</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.529911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.863552</td>\n",
       "      <td>0.665516</td>\n",
       "      <td>0.408144</td>\n",
       "      <td>0.349838</td>\n",
       "      <td>0.489773</td>\n",
       "      <td>0.483809</td>\n",
       "      <td>0.836391</td>\n",
       "      <td>0.630863</td>\n",
       "      <td>0.355422</td>\n",
       "      <td>0.285024</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.522796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.854492</td>\n",
       "      <td>0.661142</td>\n",
       "      <td>0.405176</td>\n",
       "      <td>0.333578</td>\n",
       "      <td>0.515909</td>\n",
       "      <td>0.482620</td>\n",
       "      <td>0.801223</td>\n",
       "      <td>0.616336</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.520104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.848379</td>\n",
       "      <td>0.662576</td>\n",
       "      <td>0.412188</td>\n",
       "      <td>0.328388</td>\n",
       "      <td>0.553409</td>\n",
       "      <td>0.472662</td>\n",
       "      <td>0.838685</td>\n",
       "      <td>0.638823</td>\n",
       "      <td>0.370149</td>\n",
       "      <td>0.295238</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.532173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  Train_accuracy  Train_mF1Score  Train_f1Score  Train_precision  \\\n",
       "0      1        0.903504        0.482428       0.015590         0.388889   \n",
       "1      2        0.886475        0.566989       0.195046         0.305825   \n",
       "2      3        0.838118        0.613773       0.319413         0.267898   \n",
       "3      4        0.841611        0.628286       0.346691         0.287099   \n",
       "4      5        0.852200        0.646949       0.377757         0.317130   \n",
       "5      6        0.863225        0.654266       0.385483         0.339085   \n",
       "6      7        0.851435        0.652689       0.389960         0.321984   \n",
       "7      8        0.863552        0.665516       0.408144         0.349838   \n",
       "8      9        0.854492        0.661142       0.405176         0.333578   \n",
       "9     10        0.848379        0.662576       0.412188         0.328388   \n",
       "\n",
       "   Train_recall  Train_avg_train_loss  Val_accuracy  Val_mF1Score  \\\n",
       "0      0.007955              0.636512      0.902905      0.482219   \n",
       "1      0.143182              0.578465      0.727064      0.565887   \n",
       "2      0.395455              0.541997      0.811162      0.618935   \n",
       "3      0.437500              0.519769      0.766820      0.594266   \n",
       "4      0.467045              0.504492      0.875382      0.640838   \n",
       "5      0.446591              0.503868      0.795872      0.617659   \n",
       "6      0.494318              0.490571      0.863150      0.643165   \n",
       "7      0.489773              0.483809      0.836391      0.630863   \n",
       "8      0.515909              0.482620      0.801223      0.616336   \n",
       "9      0.553409              0.472662      0.838685      0.638823   \n",
       "\n",
       "   Val_f1Score  Val_precision  Val_recall  Val_avg_val_loss  \n",
       "0     0.015504       0.250000       0.008          0.627918  \n",
       "1     0.301370       0.199482       0.616          0.552996  \n",
       "2     0.348285       0.259843       0.528          0.542392  \n",
       "3     0.329670       0.227273       0.600          0.530317  \n",
       "4     0.350598       0.349206       0.352          0.551392  \n",
       "5     0.356627       0.255172       0.592          0.523404  \n",
       "6     0.362989       0.326923       0.408          0.529911  \n",
       "7     0.355422       0.285024       0.472          0.522796  \n",
       "8     0.350000       0.254545       0.560          0.520104  \n",
       "9     0.370149       0.295238       0.496          0.532173  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('LSTM_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import random\n",
    "import io\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Utility\n",
    "from tqdm import tqdm #progress-bar\n",
    "from itertools import chain, repeat, islice #padding\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader,RandomSampler, SequentialSampler\n",
    "\n",
    "# Optimiser\n",
    "from transformers import AdamW\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Model\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, weights,embs_np,dimension=128):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        \n",
    "        self.weights = weights\n",
    "        input_size = embs_np.shape[1]\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embs_np).float())\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc = nn.Linear(2*dimension, 2)\n",
    "\n",
    "    def forward(self, text,labels):\n",
    "        text_len = 1024\n",
    "\n",
    "        text_emb = self.embedding(text)\n",
    "\n",
    "        #packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.lstm(text_emb)\n",
    "        #output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "        out_reverse = output[:, 0, self.dimension:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        output = self.fc(text_fea)\n",
    "        #text_fea = torch.squeeze(text_fea, 1)\n",
    "        #text_out = torch.sigmoid(text_fea)\n",
    "    \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(self.weights,dtype=torch.float))\n",
    "        \n",
    "        #oss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fct(output,labels)\n",
    "        return loss,output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self,args):\n",
    "        # fix the random\n",
    "        random.seed(args['seed_val'])\n",
    "        np.random.seed(args['seed_val'])\n",
    "        torch.manual_seed(args['seed_val'])\n",
    "        torch.cuda.manual_seed_all(args['seed_val'])\n",
    "                \n",
    "        self.device = torch.device(args['device'])\n",
    "        \n",
    "        self.embeddings,self.id2word,self.word2id = self.load_vec(args['embedding_path'])\n",
    "        \n",
    "    ##----------------------------------------------------------##\n",
    "    ##------------------- Utility Functions --------------------##\n",
    "    ##----------------------------------------------------------##\n",
    "    def load_vec(self,emb_path, nmax=50000):\n",
    "        vectors = []\n",
    "        word2id = {}\n",
    "        with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', \n",
    "                     errors='ignore') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vect = line.rstrip().split(' ', 1)\n",
    "                vect = np.fromstring(vect, sep=' ')\n",
    "                assert word not in word2id, 'word found twice'\n",
    "                vectors.append(vect)\n",
    "                word2id[word] = len(word2id)\n",
    "                if len(word2id) == nmax:\n",
    "                    break\n",
    "        id2word = {v: k for k, v in word2id.items()}\n",
    "        embeddings = np.vstack(vectors)\n",
    "        merged_vec = self.add_pad_unk(embeddings)\n",
    "        return merged_vec, id2word, word2id\n",
    "    \n",
    "    def encode_data(self,data,max_len):\n",
    "        new_data=[]\n",
    "        \n",
    "        for row in tqdm(data):\n",
    "            encoded=[]\n",
    "            words=row.split(' ')\n",
    "            unk_index = len(list(self.word2id.keys()))\n",
    "            pad_index = unk_index+1\n",
    "            num = min(max_len,len(words))\n",
    "            for word in words[0:num]:\n",
    "                word=word.lower()\n",
    "                try:\n",
    "                    index=self.word2id[word]\n",
    "                except KeyError:\n",
    "                    index=unk_index\n",
    "                encoded.append(index)\n",
    "            if(len(encoded)<max_len):\n",
    "                padding = [pad_index]*(max_len-len(encoded))\n",
    "                encoded.extend(padding)\n",
    "            else:\n",
    "                encoded=encoded[0:max_len]\n",
    "            new_data.append(encoded)\n",
    "                                                   \n",
    "        return new_data\n",
    "    \n",
    "    def add_pad_unk(self,vector):\n",
    "        pad_vec = np.zeros((1,vector.shape[1])) \n",
    "        unk_vec = np.mean(vector,axis=0,keepdims=True) \n",
    "        \n",
    "        merged_vec=np.append(vector, unk_vec, axis=0)\n",
    "        merged_vec=np.append(merged_vec, pad_vec, axis=0)\n",
    "        \n",
    "        return merged_vec\n",
    "    \n",
    "    ##----------------------------------------------------------##\n",
    "    ##---------------------- Data Loader -----------------------##\n",
    "    ##----------------------------------------------------------## \n",
    "    def get_dataloader(self,X,Y,max_len,batch_size,is_train=False):\n",
    "        inputs = self.encode_data(X,max_len)\n",
    "        labels = Y\n",
    "        \n",
    "        inputs = torch.tensor(inputs)\n",
    "        labels = torch.tensor(labels,dtype=torch.long)\n",
    "\n",
    "        data = TensorDataset(inputs,labels)\n",
    "\n",
    "        if(is_train==False):\n",
    "            sampler = SequentialSampler(data)\n",
    "        else:\n",
    "            sampler = RandomSampler(data) \n",
    "\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "        return dataloader\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Training Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------##  \n",
    "    def get_optimiser(self,learning_rate,model):\n",
    "         return AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8\n",
    "                )\n",
    "        \n",
    "    def evalMetric(self,y_true, y_pred,prefix):\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "        f1Score  = f1_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        area_under_c = auc(fpr, tpr)\n",
    "        recallScore = recall_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "        precisionScore = precision_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "        return dict({prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "                        prefix+'f1Score': f1Score, prefix+'precision': precisionScore, \n",
    "                        prefix+'recall': recallScore})\n",
    "    \n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##---------------- Different Train Loops --------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def evaluate(self,model,loader,which):\n",
    "    \n",
    "        model.eval() # put model in eval mode\n",
    "\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in loader:\n",
    "            b_inputs = batch[0].to(self.device)\n",
    "            b_labels = batch[1].to(self.device)\n",
    "\n",
    "            with torch.no_grad(): # do not construct compute graph\n",
    "                outputs = model(b_inputs,b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        metrics = self.evalMetric(y_true,y_pred,which+\"_\")\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_loss = total_eval_loss / len(loader)\n",
    "\n",
    "        metrics[which+'_avg_loss'] = avg_loss\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def run_train_loop(self,model,train_loader,optimiser):\n",
    "        \n",
    "        total_loss = 0\n",
    "        model.train() # put model in train mode\n",
    "\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for step, batch in tqdm(enumerate(train_loader)):\n",
    "\n",
    "            b_inputs = batch[0].to(self.device)\n",
    "            b_labels = batch[1].to(self.device)\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            outputs = model(b_inputs,b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimiser.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        train_metrics = self.evalMetric(y_true,y_pred,\"Train_\")\n",
    "\n",
    "        print('avg_train_loss',avg_train_loss)\n",
    "        print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "        print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "        train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "        return train_metrics\n",
    "    \n",
    "    \n",
    "    ##------------------------------------------------------------##\n",
    "    ##----------------- Main Train Loop --------------------------##\n",
    "    ##------------------------------------------------------------##\n",
    "    def train(self,model,data_loaders,optimiser,epochs):\n",
    "        train_stats = []\n",
    "        train_loader,val_loader,test_loader = data_loaders\n",
    "        for epoch_i in range(0, epochs):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            \n",
    "            print(\"\")\n",
    "            print('Training...')\n",
    "            train_metrics = self.run_train_loop(model,train_loader,optimiser)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\") \n",
    "            val_metrics = self.evaluate(model,val_loader,\"Val\")\n",
    "            \n",
    "            print(\"Validation Loss: \",val_metrics['Val_avg_loss'])\n",
    "            print(\"Validation Accuracy: \",val_metrics['Val_accuracy'])\n",
    "            \n",
    "            stats = {}\n",
    "\n",
    "            stats['epoch']=epoch_i+1\n",
    "\n",
    "            stats.update(train_metrics)\n",
    "            stats.update(val_metrics)\n",
    "\n",
    "            train_stats.append(stats)\n",
    "\n",
    "        return train_stats\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##--------------------- The Pipeline ------------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run(self,args,df_train,df_val,df_test):\n",
    "        X_train = df_train['Text'].values\n",
    "        Y_train = df_train['Label'].values\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "        X_val = df_val['Text'].values\n",
    "        Y_val = df_val['Label'].values\n",
    "        \n",
    "        train_dl = self.get_dataloader(X_train,Y_train,args['max_len'],\n",
    "                                       args['batch_size'],\n",
    "                                       True)\n",
    "        val_dl = self.get_dataloader(X_val,Y_val,args['max_len'],args['batch_size'])\n",
    "        test_dl = self.get_dataloader(X_test,Y_test,args['max_len'],args['batch_size'])\n",
    "        \n",
    "        model = LSTM_Model(args['weights'],self.embeddings)\n",
    "        \n",
    "        optimiser =self.get_optimiser(args['learning_rate'],model)\n",
    "        \n",
    "        train_stats = self.train(model,[train_dl,val_dl,test_dl],\n",
    "                            optimiser,args['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"Data_Processed/Shared_Task_eng/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_FOLDER+\"train_1.csv\")\n",
    "df_val = pd.read_csv(DATA_FOLDER+\"val_1.csv\")\n",
    "df_test = pd.read_csv(DATA_FOLDER+\"test_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(inplace=True)\n",
    "df_val.dropna(inplace=True)\n",
    "df_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9161, 1308, 2615)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train),len(df_val),len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.iloc[0:1000]\n",
    "df_val=df_val.iloc[0:200]\n",
    "df_test=df_test.iloc[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "    'seed_val':42,\n",
    "    'batch_size':32,\n",
    "    'max_len':1024,\n",
    "    'weights':[1.0,5.0],\n",
    "    'epochs': 10,\n",
    "    'learning_rate':1e-4,\n",
    "    'device':'cpu',\n",
    "    'embedding_path': \"Embeddings/cc.en.300.vec\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2300.20it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 2370.13it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 2286.57it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [01:18,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6898938808590174\n",
      "train_f1Score 0.110803324099723\n",
      "train_accuracy 0.679\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.670798625264849\n",
      "Validation Accuracy:  0.905\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [01:20,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6688580773770809\n",
      "train_f1Score 0.0\n",
      "train_accuracy 0.898\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.652502885886601\n",
      "Validation Accuracy:  0.905\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [01:20,  2.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-e7232f1b5bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-2790b6dda182>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, args, df_train, df_val, df_test)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_optimiser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         train_stats = self.train(model,[train_dl,val_dl,test_dl],\n\u001b[0m\u001b[1;32m    262\u001b[0m                             optimiser,args['epochs'])\n",
      "\u001b[0;32m<ipython-input-73-2790b6dda182>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, data_loaders, optimiser, epochs)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-2790b6dda182>\u001b[0m in \u001b[0;36mrun_train_loop\u001b[0;34m(self, model, train_loader, optimiser)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mb_y_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm.run(args,df_train,df_val,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
