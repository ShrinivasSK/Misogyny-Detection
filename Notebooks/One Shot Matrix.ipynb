{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c70ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_cleaning import Data_Preprocessing\n",
    "from arabert.preprocess import ArabertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f16b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import random\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Tokeniser\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Utility\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Optimiser\n",
    "from transformers import AdamW\n",
    "\n",
    "# Model\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERT:\n",
    "    def __init__(self,args):\n",
    "        # fix the random\n",
    "        random.seed(args['seed_val'])\n",
    "        np.random.seed(args['seed_val'])\n",
    "        torch.manual_seed(args['seed_val'])\n",
    "        torch.cuda.manual_seed_all(args['seed_val'])\n",
    "        \n",
    "        # set device\n",
    "        self.device = torch.device(args['device'])\n",
    "\n",
    "        self.weights=args['weights']\n",
    "        \n",
    "        # initiliase tokeniser\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(args['bert_model'])\n",
    "\n",
    "        self.model_save_path = args['model_save_path']\n",
    "        self.name = args['name']\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Utility Functions -----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def encode(self,data,max_len):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sent in tqdm(data):\n",
    "            # use in-built tokeniser of Bert\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                            sent,\n",
    "                            add_special_tokens =True, # for [CLS] and [SEP]\n",
    "                            max_length = max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt', # return pytorch tensors\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            # attention masks notify where padding has been added \n",
    "            # and where is the sentence\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "        return [input_ids,attention_masks]\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##------------------ Dataloader -----------------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def get_dataloader(self,samples, batch_size,is_train=False):\n",
    "        inputs,masks,labels = samples\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "        inputs = torch.cat(inputs, dim=0)\n",
    "        masks = torch.cat(masks, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # convert to dataset\n",
    "        data = TensorDataset(inputs,masks,labels)\n",
    "\n",
    "        if(is_train==False):\n",
    "            # use random sampler for training to shuffle\n",
    "            # train data\n",
    "            sampler = SequentialSampler(data)\n",
    "        else:\n",
    "            # order does not matter for validation as we just \n",
    "            # need the metrics\n",
    "            sampler = RandomSampler(data)  \n",
    "\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size,drop_last=True)\n",
    "\n",
    "        return dataloader\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Training Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def get_optimiser(self,learning_rate,model):\n",
    "        # using AdamW optimiser from transformers library\n",
    "        return AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    def get_scheduler(self,epochs,optimiser,train_dl):\n",
    "        total_steps = len(train_dl) * epochs\n",
    "        return get_linear_schedule_with_warmup(optimiser, \n",
    "                num_warmup_steps = 0, \n",
    "                num_training_steps = total_steps)\n",
    "    \n",
    "    def evalMetric(self, y_true, y_pred, prefix):\n",
    "        # calculate all the metrics and add prefix to them\n",
    "        # before saving in dictionary\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "        f1Score = f1_score(y_true, y_pred)\n",
    "        area_under_c = roc_auc_score(y_true, y_pred)\n",
    "        recallScore = recall_score(y_true, y_pred)\n",
    "        precisionScore = precision_score(y_true, y_pred)\n",
    "\n",
    "        nonhate_f1Score = f1_score(y_true, y_pred, pos_label=0)\n",
    "        non_recallScore = recall_score(y_true, y_pred, pos_label=0)\n",
    "        non_precisionScore = precision_score(y_true, y_pred, pos_label=0)\n",
    "        return {prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "            prefix+'f1Score': f1Score, prefix+'auc': area_under_c,\n",
    "            prefix+'precision': precisionScore, \n",
    "            prefix+'recall': recallScore, \n",
    "            prefix+'non_hatef1Score': nonhate_f1Score, \n",
    "            prefix+'non_recallScore': non_recallScore, \n",
    "            prefix+'non_precisionScore': non_precisionScore}\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##---------------- Different Train Loops --------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def evaluate(self,model,loader,which):\n",
    "        # to evaluate model on test and validation set\n",
    "\n",
    "        model.eval() # put model in eval mode\n",
    "\n",
    "        # maintain total loss to save in metrics\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(loader):\n",
    "            # separate input, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            with torch.no_grad(): # do not construct compute graph\n",
    "                outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            # output is always a tuple, thus we have to \n",
    "            # separate it manually\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "            # add the current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # calculate true labels and convert it into numpy array\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "            \n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = self.evalMetric(y_true,y_pred,which+\"_\")\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_loss = total_eval_loss / len(loader)\n",
    "        # add it to the metric\n",
    "        metrics[which+'_avg_loss'] = avg_loss\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def run_train_loop(self,model,train_loader,optimiser,scheduler):\n",
    "\n",
    "        model.train() # put model in train mode\n",
    "\n",
    "        # maintain total loss to add to metric\n",
    "        total_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            # separate inputs, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            # Ref: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch#:~:text=In%20PyTorch%20%2C%20we%20need%20to,backward()%20call.\n",
    "            model.zero_grad()                \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "            # outputs is always returned as tuple\n",
    "            # Separate it manually\n",
    "            logits = outputs[1]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float)).to(self.device)\n",
    "            \n",
    "            loss = loss_fct(logits,b_labels)\n",
    "            \n",
    "            # calculate current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Back-propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # calculate true labels\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "            # clip gradient to prevent exploding gradient\n",
    "            # problems\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # gradient descent\n",
    "            optimiser.step()\n",
    "            \n",
    "            # schedule learning rate accordingly\n",
    "            scheduler.step()\n",
    "\n",
    "        # calculate avg loss \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # calculate metrics\n",
    "        train_metrics = self.evalMetric(y_true,y_pred,\"Train_\")\n",
    "        \n",
    "        # print results\n",
    "        print('avg_train_loss',avg_train_loss)\n",
    "        print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "        print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "        # add loss to metrics\n",
    "        train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "        return train_metrics\n",
    "    \n",
    "    \n",
    "    ##------------------------------------------------------------##\n",
    "    ##----------------- Main Train Loop --------------------------##\n",
    "    ##------------------------------------------------------------##\n",
    "    def train(self,model,data_loaders,optimiser,scheduler,epochs,save_model):\n",
    "        # save train stats per epoch\n",
    "        train_stats = []\n",
    "        train_loader,val_loader,test_loader = data_loaders\n",
    "        # maintain best mF1 Score to save best model\n",
    "        best_mf1Score=-1.0\n",
    "        for epoch_i in range(0, epochs):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            \n",
    "            print(\"\")\n",
    "            print('Training...')\n",
    "            # run trian loop\n",
    "            train_metrics = self.run_train_loop(model,train_loader,\n",
    "                                            optimiser,scheduler)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\") \n",
    "            # test on validation set\n",
    "            val_metrics = self.evaluate(model,val_loader,\"Val\")\n",
    "            \n",
    "            print(\"Validation Loss: \",val_metrics['Val_avg_loss'])\n",
    "            print(\"Validation Accuracy: \",val_metrics['Val_accuracy'])\n",
    "            \n",
    "            stats = {}\n",
    "\n",
    "            # save model where validation mF1Score is best\n",
    "            if(val_metrics['Val_mF1Score']>best_mf1Score):\n",
    "                best_mf1Score=val_metrics['Val_mF1Score']\n",
    "                if(save_model):\n",
    "                    torch.save(model.state_dict(), self.model_save_path+\n",
    "                        '/best_bert_'+self.name+'.pt')\n",
    "                # evaluate best model on test set\n",
    "                test_metrics = self.evaluate(model,test_loader,\"Test\")\n",
    "\n",
    "            stats['epoch']=epoch_i+1\n",
    "\n",
    "            # add train and val metrics of the epoch to \n",
    "            # same dictionary\n",
    "            stats.update(train_metrics)\n",
    "            stats.update(val_metrics)\n",
    "\n",
    "            train_stats.append(stats)\n",
    "\n",
    "        return train_stats,test_metrics\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------------- Main Pipeline ---------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run(self,args,df_train,df_val,df_test):\n",
    "        # get X and Y data points \n",
    "        X_train = df_train['Text'].values\n",
    "        Y_train = df_train['Label'].values\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "        X_val = df_val['Text'].values\n",
    "        Y_val = df_val['Label'].values\n",
    "        \n",
    "        # encode data\n",
    "        # returns list of data and attention masks\n",
    "        train_data = self.encode(X_train,args['max_len'])\n",
    "        val_data = self.encode(X_val,args['max_len'])\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "        \n",
    "        # add labels to data so that we can send them to\n",
    "        # dataloader function together\n",
    "        train_data.append(Y_train)\n",
    "        val_data.append(Y_val)\n",
    "        test_data.append(Y_test)\n",
    "        \n",
    "        # convert to dataloader\n",
    "        train_dl =self.get_dataloader(train_data,args['batch_size'],True)\n",
    "        val_dl =self.get_dataloader(val_data,args['batch_size'])                          \n",
    "        test_dl =self.get_dataloader(test_data,args['batch_size'])\n",
    "        \n",
    "        # intialise model\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "                args['bert_model'], \n",
    "                num_labels = 2, \n",
    "                output_attentions = False, # Whether the model returns attentions weights.\n",
    "                output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "            )\n",
    "        model.to(self.device)\n",
    "        \n",
    "        optimiser = self.get_optimiser(args['learning_rate'],model)\n",
    "        \n",
    "        scheduler = self.get_scheduler(args['epochs'],optimiser,train_dl)\n",
    "        \n",
    "        # Run train loop and evaluate on validation data set\n",
    "        # on each epoch. Store best model from all epochs \n",
    "        # (best mF1 Score on Val set) and evaluate it on\n",
    "        # test set\n",
    "        train_stats,train_metrics = self.train(model,[train_dl,val_dl,test_dl],\n",
    "                                optimiser,scheduler,args['epochs'],args['save_model'])\n",
    "        \n",
    "        return train_stats,train_metrics\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##-------------------- Other Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run_test(self,model,df_test,args):\n",
    "        # to evaluate test set on the final saved model\n",
    "        # to retrieve results if necessary\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "\n",
    "        test_data.append(Y_test)\n",
    "\n",
    "        test_dl =self.get_dataloader(test_data,32)\n",
    "\n",
    "        metrics = self.evaluate(model,test_dl,\"Test\")\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def load_model(self,path,args):\n",
    "        # load saved best model\n",
    "#         config = BertConfig.from_pretrained(args['bert_model'])\n",
    "        saved_model = BertForSequenceClassification.from_pretrained(\n",
    "                args['bert_model'], \n",
    "                num_labels = 2, \n",
    "                output_attentions = False, # Whether the model returns attentions weights.\n",
    "                output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "            )\n",
    "#         saved_model.bert.embeddings.word_embeddings=torch.nn.Embedding(64000,768,padding_idx=0)\n",
    "        \n",
    "        saved_model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        return saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3817180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,isArabic):\n",
    "    \n",
    "    X = df['Text']\n",
    "    X_new=[]\n",
    "    if(isArabic):\n",
    "        prep = ArabertPreprocessor('bert-base-arabertv02')\n",
    "        for text in tqdm(X):\n",
    "            text = prep.preprocess(text)\n",
    "            X_new.append(text)\n",
    "    else:\n",
    "        processer = Data_Preprocessing()\n",
    "        for text in tqdm(X):\n",
    "            text= processer.removeEmojis(text)\n",
    "            text = processer.removeUrls(text)\n",
    "            text=processer.removeSpecialChar(text)\n",
    "            X_new.append(text)\n",
    "\n",
    "    df['Text']=X_new\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a347c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(args,data_path,index):\n",
    "    # read dataframes\n",
    "    df_test = pd.read_csv(data_path+'test_'+str(index)+'.csv')\n",
    "\n",
    "    # clean data\n",
    "    df_test=preprocess(df_test,args['isArabic'])\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b101fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot_output(model_path,data_path,obj,args):\n",
    "    saved_model=obj.load_model(model_path,args)\n",
    "    device = torch.device(args['device'])\n",
    "    saved_model=saved_model.to(device)\n",
    "    \n",
    "    all_metrics=[]\n",
    "    avg_metrics={}\n",
    "    \n",
    "    # preprocessing\n",
    "    for fold in [1,2,3,4,5]:\n",
    "        df = load_dataset(args,data_path,fold)\n",
    "\n",
    "        metrics = obj.run_test(saved_model,df,args)\n",
    "        \n",
    "        for key,value in metrics.items():\n",
    "            if(key not in avg_metrics):\n",
    "                avg_metrics[key]=value\n",
    "            else:\n",
    "                avg_metrics[key]+=value\n",
    "        \n",
    "        all_metrics.append(metrics)\n",
    "            \n",
    "    \n",
    "    for key,value in avg_metrics.items():\n",
    "        avg_metrics[key]/=5\n",
    "    \n",
    "    return all_metrics,avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3ab746e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertForSequenceClassification:\n\tMissing key(s) in state_dict: \"bert.embeddings.position_ids\", \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"roberta.embeddings.position_ids\", \"roberta.embeddings.word_embeddings.weight\", \"roberta.embeddings.position_embeddings.weight\", \"roberta.embeddings.token_type_embeddings.weight\", \"roberta.embeddings.LayerNorm.weight\", \"roberta.embeddings.LayerNorm.bias\", \"roberta.encoder.layer.0.attention.self.query.weight\", \"roberta.encoder.layer.0.attention.self.query.bias\", \"roberta.encoder.layer.0.attention.self.key.weight\", \"roberta.encoder.layer.0.attention.self.key.bias\", \"roberta.encoder.layer.0.attention.self.value.weight\", \"roberta.encoder.layer.0.attention.self.value.bias\", \"roberta.encoder.layer.0.attention.output.dense.weight\", \"roberta.encoder.layer.0.attention.output.dense.bias\", \"roberta.encoder.layer.0.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.0.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.0.intermediate.dense.weight\", \"roberta.encoder.layer.0.intermediate.dense.bias\", \"roberta.encoder.layer.0.output.dense.weight\", \"roberta.encoder.layer.0.output.dense.bias\", \"roberta.encoder.layer.0.output.LayerNorm.weight\", \"roberta.encoder.layer.0.output.LayerNorm.bias\", \"roberta.encoder.layer.1.attention.self.query.weight\", \"roberta.encoder.layer.1.attention.self.query.bias\", \"roberta.encoder.layer.1.attention.self.key.weight\", \"roberta.encoder.layer.1.attention.self.key.bias\", \"roberta.encoder.layer.1.attention.self.value.weight\", \"roberta.encoder.layer.1.attention.self.value.bias\", \"roberta.encoder.layer.1.attention.output.dense.weight\", \"roberta.encoder.layer.1.attention.output.dense.bias\", \"roberta.encoder.layer.1.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.1.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.1.intermediate.dense.weight\", \"roberta.encoder.layer.1.intermediate.dense.bias\", \"roberta.encoder.layer.1.output.dense.weight\", \"roberta.encoder.layer.1.output.dense.bias\", \"roberta.encoder.layer.1.output.LayerNorm.weight\", \"roberta.encoder.layer.1.output.LayerNorm.bias\", \"roberta.encoder.layer.2.attention.self.query.weight\", \"roberta.encoder.layer.2.attention.self.query.bias\", \"roberta.encoder.layer.2.attention.self.key.weight\", \"roberta.encoder.layer.2.attention.self.key.bias\", \"roberta.encoder.layer.2.attention.self.value.weight\", \"roberta.encoder.layer.2.attention.self.value.bias\", \"roberta.encoder.layer.2.attention.output.dense.weight\", \"roberta.encoder.layer.2.attention.output.dense.bias\", \"roberta.encoder.layer.2.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.2.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.2.intermediate.dense.weight\", \"roberta.encoder.layer.2.intermediate.dense.bias\", \"roberta.encoder.layer.2.output.dense.weight\", \"roberta.encoder.layer.2.output.dense.bias\", \"roberta.encoder.layer.2.output.LayerNorm.weight\", \"roberta.encoder.layer.2.output.LayerNorm.bias\", \"roberta.encoder.layer.3.attention.self.query.weight\", \"roberta.encoder.layer.3.attention.self.query.bias\", \"roberta.encoder.layer.3.attention.self.key.weight\", \"roberta.encoder.layer.3.attention.self.key.bias\", \"roberta.encoder.layer.3.attention.self.value.weight\", \"roberta.encoder.layer.3.attention.self.value.bias\", \"roberta.encoder.layer.3.attention.output.dense.weight\", \"roberta.encoder.layer.3.attention.output.dense.bias\", \"roberta.encoder.layer.3.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.3.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.3.intermediate.dense.weight\", \"roberta.encoder.layer.3.intermediate.dense.bias\", \"roberta.encoder.layer.3.output.dense.weight\", \"roberta.encoder.layer.3.output.dense.bias\", \"roberta.encoder.layer.3.output.LayerNorm.weight\", \"roberta.encoder.layer.3.output.LayerNorm.bias\", \"roberta.encoder.layer.4.attention.self.query.weight\", \"roberta.encoder.layer.4.attention.self.query.bias\", \"roberta.encoder.layer.4.attention.self.key.weight\", \"roberta.encoder.layer.4.attention.self.key.bias\", \"roberta.encoder.layer.4.attention.self.value.weight\", \"roberta.encoder.layer.4.attention.self.value.bias\", \"roberta.encoder.layer.4.attention.output.dense.weight\", \"roberta.encoder.layer.4.attention.output.dense.bias\", \"roberta.encoder.layer.4.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.4.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.4.intermediate.dense.weight\", \"roberta.encoder.layer.4.intermediate.dense.bias\", \"roberta.encoder.layer.4.output.dense.weight\", \"roberta.encoder.layer.4.output.dense.bias\", \"roberta.encoder.layer.4.output.LayerNorm.weight\", \"roberta.encoder.layer.4.output.LayerNorm.bias\", \"roberta.encoder.layer.5.attention.self.query.weight\", \"roberta.encoder.layer.5.attention.self.query.bias\", \"roberta.encoder.layer.5.attention.self.key.weight\", \"roberta.encoder.layer.5.attention.self.key.bias\", \"roberta.encoder.layer.5.attention.self.value.weight\", \"roberta.encoder.layer.5.attention.self.value.bias\", \"roberta.encoder.layer.5.attention.output.dense.weight\", \"roberta.encoder.layer.5.attention.output.dense.bias\", \"roberta.encoder.layer.5.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.5.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.5.intermediate.dense.weight\", \"roberta.encoder.layer.5.intermediate.dense.bias\", \"roberta.encoder.layer.5.output.dense.weight\", \"roberta.encoder.layer.5.output.dense.bias\", \"roberta.encoder.layer.5.output.LayerNorm.weight\", \"roberta.encoder.layer.5.output.LayerNorm.bias\", \"roberta.encoder.layer.6.attention.self.query.weight\", \"roberta.encoder.layer.6.attention.self.query.bias\", \"roberta.encoder.layer.6.attention.self.key.weight\", \"roberta.encoder.layer.6.attention.self.key.bias\", \"roberta.encoder.layer.6.attention.self.value.weight\", \"roberta.encoder.layer.6.attention.self.value.bias\", \"roberta.encoder.layer.6.attention.output.dense.weight\", \"roberta.encoder.layer.6.attention.output.dense.bias\", \"roberta.encoder.layer.6.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.6.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.6.intermediate.dense.weight\", \"roberta.encoder.layer.6.intermediate.dense.bias\", \"roberta.encoder.layer.6.output.dense.weight\", \"roberta.encoder.layer.6.output.dense.bias\", \"roberta.encoder.layer.6.output.LayerNorm.weight\", \"roberta.encoder.layer.6.output.LayerNorm.bias\", \"roberta.encoder.layer.7.attention.self.query.weight\", \"roberta.encoder.layer.7.attention.self.query.bias\", \"roberta.encoder.layer.7.attention.self.key.weight\", \"roberta.encoder.layer.7.attention.self.key.bias\", \"roberta.encoder.layer.7.attention.self.value.weight\", \"roberta.encoder.layer.7.attention.self.value.bias\", \"roberta.encoder.layer.7.attention.output.dense.weight\", \"roberta.encoder.layer.7.attention.output.dense.bias\", \"roberta.encoder.layer.7.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.7.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.7.intermediate.dense.weight\", \"roberta.encoder.layer.7.intermediate.dense.bias\", \"roberta.encoder.layer.7.output.dense.weight\", \"roberta.encoder.layer.7.output.dense.bias\", \"roberta.encoder.layer.7.output.LayerNorm.weight\", \"roberta.encoder.layer.7.output.LayerNorm.bias\", \"roberta.encoder.layer.8.attention.self.query.weight\", \"roberta.encoder.layer.8.attention.self.query.bias\", \"roberta.encoder.layer.8.attention.self.key.weight\", \"roberta.encoder.layer.8.attention.self.key.bias\", \"roberta.encoder.layer.8.attention.self.value.weight\", \"roberta.encoder.layer.8.attention.self.value.bias\", \"roberta.encoder.layer.8.attention.output.dense.weight\", \"roberta.encoder.layer.8.attention.output.dense.bias\", \"roberta.encoder.layer.8.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.8.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.8.intermediate.dense.weight\", \"roberta.encoder.layer.8.intermediate.dense.bias\", \"roberta.encoder.layer.8.output.dense.weight\", \"roberta.encoder.layer.8.output.dense.bias\", \"roberta.encoder.layer.8.output.LayerNorm.weight\", \"roberta.encoder.layer.8.output.LayerNorm.bias\", \"roberta.encoder.layer.9.attention.self.query.weight\", \"roberta.encoder.layer.9.attention.self.query.bias\", \"roberta.encoder.layer.9.attention.self.key.weight\", \"roberta.encoder.layer.9.attention.self.key.bias\", \"roberta.encoder.layer.9.attention.self.value.weight\", \"roberta.encoder.layer.9.attention.self.value.bias\", \"roberta.encoder.layer.9.attention.output.dense.weight\", \"roberta.encoder.layer.9.attention.output.dense.bias\", \"roberta.encoder.layer.9.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.9.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.9.intermediate.dense.weight\", \"roberta.encoder.layer.9.intermediate.dense.bias\", \"roberta.encoder.layer.9.output.dense.weight\", \"roberta.encoder.layer.9.output.dense.bias\", \"roberta.encoder.layer.9.output.LayerNorm.weight\", \"roberta.encoder.layer.9.output.LayerNorm.bias\", \"roberta.encoder.layer.10.attention.self.query.weight\", \"roberta.encoder.layer.10.attention.self.query.bias\", \"roberta.encoder.layer.10.attention.self.key.weight\", \"roberta.encoder.layer.10.attention.self.key.bias\", \"roberta.encoder.layer.10.attention.self.value.weight\", \"roberta.encoder.layer.10.attention.self.value.bias\", \"roberta.encoder.layer.10.attention.output.dense.weight\", \"roberta.encoder.layer.10.attention.output.dense.bias\", \"roberta.encoder.layer.10.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.10.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.10.intermediate.dense.weight\", \"roberta.encoder.layer.10.intermediate.dense.bias\", \"roberta.encoder.layer.10.output.dense.weight\", \"roberta.encoder.layer.10.output.dense.bias\", \"roberta.encoder.layer.10.output.LayerNorm.weight\", \"roberta.encoder.layer.10.output.LayerNorm.bias\", \"roberta.encoder.layer.11.attention.self.query.weight\", \"roberta.encoder.layer.11.attention.self.query.bias\", \"roberta.encoder.layer.11.attention.self.key.weight\", \"roberta.encoder.layer.11.attention.self.key.bias\", \"roberta.encoder.layer.11.attention.self.value.weight\", \"roberta.encoder.layer.11.attention.self.value.bias\", \"roberta.encoder.layer.11.attention.output.dense.weight\", \"roberta.encoder.layer.11.attention.output.dense.bias\", \"roberta.encoder.layer.11.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.11.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.11.intermediate.dense.weight\", \"roberta.encoder.layer.11.intermediate.dense.bias\", \"roberta.encoder.layer.11.output.dense.weight\", \"roberta.encoder.layer.11.output.dense.bias\", \"roberta.encoder.layer.11.output.LayerNorm.weight\", \"roberta.encoder.layer.11.output.LayerNorm.bias\", \"roberta.pooler.dense.weight\", \"roberta.pooler.dense.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ff13849476fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_shot_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-7b15e4a14783>\u001b[0m in \u001b[0;36mone_shot_output\u001b[0;34m(model_path, data_path, obj, args)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mone_shot_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msaved_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msaved_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-471a0940e863>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, path, args)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;31m#         saved_model.bert.embeddings.word_embeddings=torch.nn.Embedding(64000,768,padding_idx=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1045\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertForSequenceClassification:\n\tMissing key(s) in state_dict: \"bert.embeddings.position_ids\", \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"roberta.embeddings.position_ids\", \"roberta.embeddings.word_embeddings.weight\", \"roberta.embeddings.position_embeddings.weight\", \"roberta.embeddings.token_type_embeddings.weight\", \"roberta.embeddings.LayerNorm.weight\", \"roberta.embeddings.LayerNorm.bias\", \"roberta.encoder.layer.0.attention.self.query.weight\", \"roberta.encoder.layer.0.attention.self.query.bias\", \"roberta.encoder.layer.0.attention.self.key.weight\", \"roberta.encoder.layer.0.attention.self.key.bias\", \"roberta.encoder.layer.0.attention.self.value.weight\", \"roberta.encoder.layer.0.attention.self.value.bias\", \"roberta.encoder.layer.0.attention.output.dense.weight\", \"roberta.encoder.layer.0.attention.output.dense.bias\", \"roberta.encoder.layer.0.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.0.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.0.intermediate.dense.weight\", \"roberta.encoder.layer.0.intermediate.dense.bias\", \"roberta.encoder.layer.0.output.dense.weight\", \"roberta.encoder.layer.0.output.dense.bias\", \"roberta.encoder.layer.0.output.LayerNorm.weight\", \"roberta.encoder.layer.0.output.LayerNorm.bias\", \"roberta.encoder.layer.1.attention.self.query.weight\", \"roberta.encoder.layer.1.attention.self.query.bias\", \"roberta.encoder.layer.1.attention.self.key.weight\", \"roberta.encoder.layer.1.attention.self.key.bias\", \"roberta.encoder.layer.1.attention.self.value.weight\", \"roberta.encoder.layer.1.attention.self.value.bias\", \"roberta.encoder.layer.1.attention.output.dense.weight\", \"roberta.encoder.layer.1.attention.output.dense.bias\", \"roberta.encoder.layer.1.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.1.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.1.intermediate.dense.weight\", \"roberta.encoder.layer.1.intermediate.dense.bias\", \"roberta.encoder.layer.1.output.dense.weight\", \"roberta.encoder.layer.1.output.dense.bias\", \"roberta.encoder.layer.1.output.LayerNorm.weight\", \"roberta.encoder.layer.1.output.LayerNorm.bias\", \"roberta.encoder.layer.2.attention.self.query.weight\", \"roberta.encoder.layer.2.attention.self.query.bias\", \"roberta.encoder.layer.2.attention.self.key.weight\", \"roberta.encoder.layer.2.attention.self.key.bias\", \"roberta.encoder.layer.2.attention.self.value.weight\", \"roberta.encoder.layer.2.attention.self.value.bias\", \"roberta.encoder.layer.2.attention.output.dense.weight\", \"roberta.encoder.layer.2.attention.output.dense.bias\", \"roberta.encoder.layer.2.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.2.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.2.intermediate.dense.weight\", \"roberta.encoder.layer.2.intermediate.dense.bias\", \"roberta.encoder.layer.2.output.dense.weight\", \"roberta.encoder.layer.2.output.dense.bias\", \"roberta.encoder.layer.2.output.LayerNorm.weight\", \"roberta.encoder.layer.2.output.LayerNorm.bias\", \"roberta.encoder.layer.3.attention.self.query.weight\", \"roberta.encoder.layer.3.attention.self.query.bias\", \"roberta.encoder.layer.3.attention.self.key.weight\", \"roberta.encoder.layer.3.attention.self.key.bias\", \"roberta.encoder.layer.3.attention.self.value.weight\", \"roberta.encoder.layer.3.attention.self.value.bias\", \"roberta.encoder.layer.3.attention.output.dense.weight\", \"roberta.encoder.layer.3.attention.output.dense.bias\", \"roberta.encoder.layer.3.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.3.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.3.intermediate.dense.weight\", \"roberta.encoder.layer.3.intermediate.dense.bias\", \"roberta.encoder.layer.3.output.dense.weight\", \"roberta.encoder.layer.3.output.dense.bias\", \"roberta.encoder.layer.3.output.LayerNorm.weight\", \"roberta.encoder.layer.3.output.LayerNorm.bias\", \"roberta.encoder.layer.4.attention.self.query.weight\", \"roberta.encoder.layer.4.attention.self.query.bias\", \"roberta.encoder.layer.4.attention.self.key.weight\", \"roberta.encoder.layer.4.attention.self.key.bias\", \"roberta.encoder.layer.4.attention.self.value.weight\", \"roberta.encoder.layer.4.attention.self.value.bias\", \"roberta.encoder.layer.4.attention.output.dense.weight\", \"roberta.encoder.layer.4.attention.output.dense.bias\", \"roberta.encoder.layer.4.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.4.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.4.intermediate.dense.weight\", \"roberta.encoder.layer.4.intermediate.dense.bias\", \"roberta.encoder.layer.4.output.dense.weight\", \"roberta.encoder.layer.4.output.dense.bias\", \"roberta.encoder.layer.4.output.LayerNorm.weight\", \"roberta.encoder.layer.4.output.LayerNorm.bias\", \"roberta.encoder.layer.5.attention.self.query.weight\", \"roberta.encoder.layer.5.attention.self.query.bias\", \"roberta.encoder.layer.5.attention.self.key.weight\", \"roberta.encoder.layer.5.attention.self.key.bias\", \"roberta.encoder.layer.5.attention.self.value.weight\", \"roberta.encoder.layer.5.attention.self.value.bias\", \"roberta.encoder.layer.5.attention.output.dense.weight\", \"roberta.encoder.layer.5.attention.output.dense.bias\", \"roberta.encoder.layer.5.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.5.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.5.intermediate.dense.weight\", \"roberta.encoder.layer.5.intermediate.dense.bias\", \"roberta.encoder.layer.5.output.dense.weight\", \"roberta.encoder.layer.5.output.dense.bias\", \"roberta.encoder.layer.5.output.LayerNorm.weight\", \"roberta.encoder.layer.5.output.LayerNorm.bias\", \"roberta.encoder.layer.6.attention.self.query.weight\", \"roberta.encoder.layer.6.attention.self.query.bias\", \"roberta.encoder.layer.6.attention.self.key.weight\", \"roberta.encoder.layer.6.attention.self.key.bias\", \"roberta.encoder.layer.6.attention.self.value.weight\", \"roberta.encoder.layer.6.attention.self.value.bias\", \"roberta.encoder.layer.6.attention.output.dense.weight\", \"roberta.encoder.layer.6.attention.output.dense.bias\", \"roberta.encoder.layer.6.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.6.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.6.intermediate.dense.weight\", \"roberta.encoder.layer.6.intermediate.dense.bias\", \"roberta.encoder.layer.6.output.dense.weight\", \"roberta.encoder.layer.6.output.dense.bias\", \"roberta.encoder.layer.6.output.LayerNorm.weight\", \"roberta.encoder.layer.6.output.LayerNorm.bias\", \"roberta.encoder.layer.7.attention.self.query.weight\", \"roberta.encoder.layer.7.attention.self.query.bias\", \"roberta.encoder.layer.7.attention.self.key.weight\", \"roberta.encoder.layer.7.attention.self.key.bias\", \"roberta.encoder.layer.7.attention.self.value.weight\", \"roberta.encoder.layer.7.attention.self.value.bias\", \"roberta.encoder.layer.7.attention.output.dense.weight\", \"roberta.encoder.layer.7.attention.output.dense.bias\", \"roberta.encoder.layer.7.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.7.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.7.intermediate.dense.weight\", \"roberta.encoder.layer.7.intermediate.dense.bias\", \"roberta.encoder.layer.7.output.dense.weight\", \"roberta.encoder.layer.7.output.dense.bias\", \"roberta.encoder.layer.7.output.LayerNorm.weight\", \"roberta.encoder.layer.7.output.LayerNorm.bias\", \"roberta.encoder.layer.8.attention.self.query.weight\", \"roberta.encoder.layer.8.attention.self.query.bias\", \"roberta.encoder.layer.8.attention.self.key.weight\", \"roberta.encoder.layer.8.attention.self.key.bias\", \"roberta.encoder.layer.8.attention.self.value.weight\", \"roberta.encoder.layer.8.attention.self.value.bias\", \"roberta.encoder.layer.8.attention.output.dense.weight\", \"roberta.encoder.layer.8.attention.output.dense.bias\", \"roberta.encoder.layer.8.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.8.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.8.intermediate.dense.weight\", \"roberta.encoder.layer.8.intermediate.dense.bias\", \"roberta.encoder.layer.8.output.dense.weight\", \"roberta.encoder.layer.8.output.dense.bias\", \"roberta.encoder.layer.8.output.LayerNorm.weight\", \"roberta.encoder.layer.8.output.LayerNorm.bias\", \"roberta.encoder.layer.9.attention.self.query.weight\", \"roberta.encoder.layer.9.attention.self.query.bias\", \"roberta.encoder.layer.9.attention.self.key.weight\", \"roberta.encoder.layer.9.attention.self.key.bias\", \"roberta.encoder.layer.9.attention.self.value.weight\", \"roberta.encoder.layer.9.attention.self.value.bias\", \"roberta.encoder.layer.9.attention.output.dense.weight\", \"roberta.encoder.layer.9.attention.output.dense.bias\", \"roberta.encoder.layer.9.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.9.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.9.intermediate.dense.weight\", \"roberta.encoder.layer.9.intermediate.dense.bias\", \"roberta.encoder.layer.9.output.dense.weight\", \"roberta.encoder.layer.9.output.dense.bias\", \"roberta.encoder.layer.9.output.LayerNorm.weight\", \"roberta.encoder.layer.9.output.LayerNorm.bias\", \"roberta.encoder.layer.10.attention.self.query.weight\", \"roberta.encoder.layer.10.attention.self.query.bias\", \"roberta.encoder.layer.10.attention.self.key.weight\", \"roberta.encoder.layer.10.attention.self.key.bias\", \"roberta.encoder.layer.10.attention.self.value.weight\", \"roberta.encoder.layer.10.attention.self.value.bias\", \"roberta.encoder.layer.10.attention.output.dense.weight\", \"roberta.encoder.layer.10.attention.output.dense.bias\", \"roberta.encoder.layer.10.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.10.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.10.intermediate.dense.weight\", \"roberta.encoder.layer.10.intermediate.dense.bias\", \"roberta.encoder.layer.10.output.dense.weight\", \"roberta.encoder.layer.10.output.dense.bias\", \"roberta.encoder.layer.10.output.LayerNorm.weight\", \"roberta.encoder.layer.10.output.LayerNorm.bias\", \"roberta.encoder.layer.11.attention.self.query.weight\", \"roberta.encoder.layer.11.attention.self.query.bias\", \"roberta.encoder.layer.11.attention.self.key.weight\", \"roberta.encoder.layer.11.attention.self.key.bias\", \"roberta.encoder.layer.11.attention.self.value.weight\", \"roberta.encoder.layer.11.attention.self.value.bias\", \"roberta.encoder.layer.11.attention.output.dense.weight\", \"roberta.encoder.layer.11.attention.output.dense.bias\", \"roberta.encoder.layer.11.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.11.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.11.intermediate.dense.weight\", \"roberta.encoder.layer.11.intermediate.dense.bias\", \"roberta.encoder.layer.11.output.dense.weight\", \"roberta.encoder.layer.11.output.dense.bias\", \"roberta.encoder.layer.11.output.LayerNorm.weight\", \"roberta.encoder.layer.11.output.LayerNorm.bias\", \"roberta.pooler.dense.weight\", \"roberta.pooler.dense.bias\". "
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Data_Processed/Let-Mi/all.csv\"\n",
    "MODEL_PATH = \"Saved_Models/Let-Mi/best_bert_xlm_roberta_1_all.pt\"\n",
    "\n",
    "args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"bert-base-multilingual-cased\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_save_path': '',\n",
    "        'name': 'bert_one_shot',\n",
    "        'isArabic': True,\n",
    "    }\n",
    "\n",
    "model = BERT(args)\n",
    "\n",
    "metrics = one_shot_output(MODEL_PATH,DATA_PATH,model,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe820f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.5839723926380368,\n",
       " 'Test_mF1Score': 0.54390418648208,\n",
       " 'Test_f1Score': 0.4087193460490463,\n",
       " 'Test_auc': 0.5800991671272986,\n",
       " 'Test_precision': 0.6836827711941659,\n",
       " 'Test_recall': 0.2914885347842985,\n",
       " 'Test_non_hatef1Score': 0.6790890269151139,\n",
       " 'Test_non_recallScore': 0.8687097994702989,\n",
       " 'Test_non_precisionScore': 0.5574168487496965,\n",
       " 'Test_avg_loss': 1.530707430620135}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc2da859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model_path = \"../../HULK/HateModels/Shrinivas/Misogyny-Analysis/\"\n",
    "base_model_path=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7149a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneShotOutput(model_path,data_path):\n",
    "    args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"bert-base-multilingual-cased\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_save_path': '',\n",
    "        'name': 'bert_one_shot',\n",
    "        'isArabic': False,\n",
    "    }\n",
    "    \n",
    "    if('Let-Mi' in data_path):\n",
    "        args['isArabic']=True\n",
    "\n",
    "    model = BERT(args)\n",
    "\n",
    "    all_metrics, metrics = one_shot_output(base_model_path+model_path,data_path,model,args)\n",
    "    \n",
    "    return metrics,all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c504c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['Let-Mi','AMI-2020','AMI-Spanish','Shared_Task_eng',\n",
    "          'Shared_Task_hin','Shared_Task_iben']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4cec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths={\n",
    "    'Let-Mi':{\n",
    "        'model_path':'Saved_Models/Let-Mi/best_bert_1_all.pt',\n",
    "        'data_path':'Data_Processed/Let-Mi/',\n",
    "    },\n",
    "    'AMI-2020':{\n",
    "        'model_path': 'Saved_Models/AMI-2020/best_bert_1_all.pt',\n",
    "        'data_path': 'Data_Processed/AMI-2020/',\n",
    "    },\n",
    "    'AMI-Spanish':{\n",
    "        'model_path': 'Saved_Models/AMI-Spanish/best_bert_1_all.pt',\n",
    "        'data_path': 'Data_Processed/AMI-Spanish/',\n",
    "    },\n",
    "    'Shared_Task_eng':{\n",
    "        'model_path': 'Saved_Models/Shared_Task_eng/best_bert_1_allnew.pt',\n",
    "        'data_path': 'Data_Processed/Shared_Task_eng/',\n",
    "    },\n",
    "    'Shared_Task_iben':{\n",
    "        'model_path': 'Saved_Models/Shared_Task_iben/best_bert_4_all.pt',\n",
    "        'data_path': 'Data_Processed/Shared_Task_iben/',\n",
    "    },\n",
    "    'Shared_Task_hin':{\n",
    "        'model_path': 'Saved_Models/Shared_Task_hin/best_bert_5_all.pt',\n",
    "        'data_path': 'Data_Processed/Shared_Task_hin/',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f87e0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b0dcac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertForSequenceClassification:\n\tsize mismatch for bert.embeddings.word_embeddings.weight: copying a param with shape torch.Size([64000, 768]) from checkpoint, the shape in current model is torch.Size([119547, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e52364aae944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             metrics = getOneShotOutput(paths[dataset1]['model_path'],\n\u001b[0m\u001b[1;32m      6\u001b[0m                                       paths[dataset2]['data_path'])\n\u001b[1;32m      7\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-79bb41363c35>\u001b[0m in \u001b[0;36mgetOneShotOutput\u001b[0;34m(model_path, data_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_shot_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4b6b5af73ff6>\u001b[0m in \u001b[0;36mone_shot_output\u001b[0;34m(model_path, data_path, obj, args)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mone_shot_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msaved_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msaved_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-471a0940e863>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, path, args)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;31m#         saved_model.bert.embeddings.word_embeddings=torch.nn.Embedding(64000,768,padding_idx=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1045\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertForSequenceClassification:\n\tsize mismatch for bert.embeddings.word_embeddings.weight: copying a param with shape torch.Size([64000, 768]) from checkpoint, the shape in current model is torch.Size([119547, 768])."
     ]
    }
   ],
   "source": [
    "for dataset1 in datasets:\n",
    "    for dataset2 in datasets:\n",
    "        if(dataset1!=dataset2):\n",
    "            name = dataset1+'_'+dataset2\n",
    "            metrics = getOneShotOutput(paths[dataset1]['model_path'],\n",
    "                                      paths[dataset2]['data_path'])\n",
    "            metrics['Name']=dataset1+'_'+dataset2\n",
    "            res.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c364a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 660/660 [00:00<00:00, 2389.50it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2455.80it/s]\n",
      "100%|| 20/20 [00:02<00:00,  7.87it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2399.31it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2655.10it/s]\n",
      "100%|| 20/20 [00:02<00:00,  7.91it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2286.92it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2550.87it/s]\n",
      "100%|| 20/20 [00:02<00:00,  7.91it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2238.68it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2533.90it/s]\n",
      "100%|| 20/20 [00:02<00:00,  7.92it/s]\n",
      "100%|| 667/667 [00:00<00:00, 2351.38it/s]\n",
      "100%|| 667/667 [00:00<00:00, 2602.55it/s]\n",
      "100%|| 20/20 [00:02<00:00,  7.92it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics,all_metrics = getOneShotOutput(paths['Shared_Task_iben']['model_path'],\n",
    "                                      paths['AMI-Spanish']['data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4f8624a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.44781250000000006,\n",
       " 'Test_mF1Score': 0.4303362876251269,\n",
       " 'Test_f1Score': 0.33092505200430394,\n",
       " 'Test_auc': 0.44708845384478246,\n",
       " 'Test_precision': 0.4158416696905797,\n",
       " 'Test_recall': 0.27506946436065394,\n",
       " 'Test_non_hatef1Score': 0.5297475232459498,\n",
       " 'Test_non_recallScore': 0.6191074433289109,\n",
       " 'Test_non_precisionScore': 0.4630569261776423,\n",
       " 'Test_avg_loss': 2.5630367112159727}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce382768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1983/1983 [00:00<00:00, 3454.83it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3279.18it/s]\n",
      "100%|| 61/61 [00:07<00:00,  7.92it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3286.18it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3165.92it/s]\n",
      "100%|| 61/61 [00:07<00:00,  7.92it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3295.26it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3105.58it/s]\n",
      "100%|| 61/61 [00:07<00:00,  7.91it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3235.97it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3085.84it/s]\n",
      "100%|| 61/61 [00:07<00:00,  7.92it/s]\n",
      "100%|| 1990/1990 [00:00<00:00, 3260.20it/s]\n",
      "100%|| 1990/1990 [00:00<00:00, 3127.46it/s]\n",
      "100%|| 62/62 [00:07<00:00,  7.92it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics,all_metrics = getOneShotOutput(paths['Shared_Task_iben']['model_path'],\n",
    "                                      paths['AMI-2020']['data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52de1792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.505288207297726,\n",
       " 'Test_mF1Score': 0.4593800165680129,\n",
       " 'Test_f1Score': 0.30191044485540497,\n",
       " 'Test_auc': 0.49617032307761966,\n",
       " 'Test_precision': 0.47504922850222375,\n",
       " 'Test_recall': 0.2214316539800881,\n",
       " 'Test_non_hatef1Score': 0.6168495882806211,\n",
       " 'Test_non_recallScore': 0.7709089921751511,\n",
       " 'Test_non_precisionScore': 0.5141624654457451,\n",
       " 'Test_avg_loss': 2.4195593771450357}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8c5e1f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1047/1047 [00:00<00:00, 10311.66it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 2872.26it/s]\n",
      "100%|| 32/32 [00:04<00:00,  7.88it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10540.07it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 2864.05it/s]\n",
      "100%|| 32/32 [00:04<00:00,  7.92it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10358.24it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 2865.04it/s]\n",
      "100%|| 32/32 [00:04<00:00,  7.92it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10686.87it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 2877.33it/s]\n",
      "100%|| 32/32 [00:04<00:00,  7.91it/s]\n",
      "100%|| 1052/1052 [00:00<00:00, 10325.55it/s]\n",
      "100%|| 1052/1052 [00:00<00:00, 2810.76it/s]\n",
      "100%|| 32/32 [00:04<00:00,  7.91it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics,all_metrics = getOneShotOutput(paths['Shared_Task_iben']['model_path'],\n",
    "                                      paths['Let-Mi']['data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9940db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test_accuracy': 0.5048828125,\n",
       " 'Test_mF1Score': 0.34809105849848926,\n",
       " 'Test_f1Score': 0.028388843360249594,\n",
       " 'Test_auc': 0.4986508606676024,\n",
       " 'Test_precision': 0.46743330199212557,\n",
       " 'Test_recall': 0.014652260431328057,\n",
       " 'Test_non_hatef1Score': 0.6677932736367289,\n",
       " 'Test_non_recallScore': 0.9826494609038768,\n",
       " 'Test_non_precisionScore': 0.5057544093033222,\n",
       " 'Test_avg_loss': 2.514556273072958}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bdfdbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeb5eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09697bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.11.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 119547\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5add4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.vocab_size=64000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f819e114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun-binny/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "        saved_model = BertForSequenceClassification.from_pretrained(\n",
    "                args['bert_model'], \n",
    "                num_labels = 2, \n",
    "                output_attentions = False, # Whether the model returns attentions weights.\n",
    "                output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "#                 config=config\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32230c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2aab189",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.bert.embeddings.word_embeddings=torch.nn.Embedding(64000,768,padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f57b7161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(119547, 768, padding_idx=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Embedding(64000,768,padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0b162",
   "metadata": {},
   "source": [
    "## Translated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f2663",
   "metadata": {},
   "source": [
    "### Arabic Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae61a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb7898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa930e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed49d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87b7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0912721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa9d87af",
   "metadata": {},
   "source": [
    "## XLM Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ca3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import Data_Preprocessing\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaPreTrainedModel,RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355e7ba9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 19 20:53:52 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   54C    P0   102W / 250W |   2177MiB / 12198MiB |     80%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:DB:00.0 Off |                    0 |\n",
      "| N/A   66C    P0   187W / 250W |   9733MiB / 16280MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1424      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    262027      C   python                           2171MiB |\n",
      "|    1   N/A  N/A      1424      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A    258168      C   python                           9727MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a526bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import random\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Tokeniser\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# Utility\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Optimiser\n",
    "from transformers import AdamW\n",
    "\n",
    "# Model\n",
    "\n",
    "import torch.nn as nn\n",
    "from models import weighted_Roberta\n",
    "\n",
    "\n",
    "class XLM_Roberta:\n",
    "    def __init__(self,args):\n",
    "        # fix the random\n",
    "        random.seed(args['seed_val'])\n",
    "        np.random.seed(args['seed_val'])\n",
    "        torch.manual_seed(args['seed_val'])\n",
    "        torch.cuda.manual_seed_all(args['seed_val'])\n",
    "        \n",
    "        # set device\n",
    "        self.device = torch.device(args['device'])\n",
    "\n",
    "        self.weights=args['weights']\n",
    "        \n",
    "        # initiliase tokeniser\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case = True)\n",
    "\n",
    "        self.model_save_path = args['model_save_path']\n",
    "        self.name = args['name']\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Utility Functions -----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def encode(self,data,max_len):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sent in tqdm(data):\n",
    "            # use in-built tokeniser of Bert\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                            sent,\n",
    "                            add_special_tokens =True, # for [CLS] and [SEP]\n",
    "                            max_length = max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = True,\n",
    "#                             return_tensors = 'pt', # return pytorch tensors\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            # attention masks notify where padding has been added \n",
    "            # and where is the sentence\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            X_data = torch.tensor(input_ids)\n",
    "            attention_masks_data = torch.tensor(attention_masks)\n",
    "            \n",
    "        return [X_data,attention_masks_data]\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##------------------ Dataloader -----------------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def get_dataloader(self,samples, batch_size,is_train=False):\n",
    "        inputs,masks,labels = samples\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "#         inputs = torch.cat(inputs, dim=0)\n",
    "#         masks = torch.cat(masks, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # convert to dataset\n",
    "        data = TensorDataset(inputs,masks,labels)\n",
    "\n",
    "        if(is_train==False):\n",
    "            # use random sampler for training to shuffle\n",
    "            # train data\n",
    "            sampler = SequentialSampler(data)\n",
    "        else:\n",
    "            # order does not matter for validation as we just \n",
    "            # need the metrics\n",
    "            sampler = RandomSampler(data)  \n",
    "\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size,drop_last=True)\n",
    "\n",
    "        return dataloader\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Training Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def get_optimiser(self,learning_rate,model):\n",
    "        # using AdamW optimiser from transformers library\n",
    "        return AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    def get_scheduler(self,epochs,optimiser,train_dl):\n",
    "        total_steps = len(train_dl) * epochs\n",
    "        return get_linear_schedule_with_warmup(optimiser, \n",
    "                num_warmup_steps = 0, \n",
    "                num_training_steps = total_steps)\n",
    "    \n",
    "    def evalMetric(self, y_true, y_pred, prefix):\n",
    "        # calculate all the metrics and add prefix to them\n",
    "        # before saving in dictionary\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "        f1Score = f1_score(y_true, y_pred)\n",
    "        area_under_c = roc_auc_score(y_true, y_pred)\n",
    "        recallScore = recall_score(y_true, y_pred)\n",
    "        precisionScore = precision_score(y_true, y_pred)\n",
    "\n",
    "        nonhate_f1Score = f1_score(y_true, y_pred, pos_label=0)\n",
    "        non_recallScore = recall_score(y_true, y_pred, pos_label=0)\n",
    "        non_precisionScore = precision_score(y_true, y_pred, pos_label=0)\n",
    "        return {prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "            prefix+'f1Score': f1Score, prefix+'auc': area_under_c,\n",
    "            prefix+'precision': precisionScore, \n",
    "            prefix+'recall': recallScore, \n",
    "            prefix+'non_hatef1Score': nonhate_f1Score, \n",
    "            prefix+'non_recallScore': non_recallScore, \n",
    "            prefix+'non_precisionScore': non_precisionScore}\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##---------------- Different Train Loops --------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def evaluate(self,model,loader,which):\n",
    "        # to evaluate model on test and validation set\n",
    "\n",
    "        model.eval() # put model in eval mode\n",
    "\n",
    "        # maintain total loss to save in metrics\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(loader):\n",
    "            # separate input, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            with torch.no_grad(): # do not construct compute graph\n",
    "                outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            # output is always a tuple, thus we have to \n",
    "            # separate it manually\n",
    "            #loss = outputs[0]\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float).to(self.device))\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "\n",
    "            # add the current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # calculate true labels and convert it into numpy array\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "            \n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = self.evalMetric(y_true,y_pred,which+\"_\")\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_loss = total_eval_loss / len(loader)\n",
    "        # add it to the metric\n",
    "        metrics[which+'_avg_loss'] = avg_loss\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def run_train_loop(self,model,train_loader,optimiser,scheduler):\n",
    "\n",
    "        model.train() # put model in train mode\n",
    "\n",
    "        # maintain total loss to add to metric\n",
    "        total_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            # separate inputs, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            # Ref: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch#:~:text=In%20PyTorch%20%2C%20we%20need%20to,backward()%20call.\n",
    "            model.zero_grad()                \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "            # outputs is always returned as tuple\n",
    "            # Separate it manually\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float).to(self.device))\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "            \n",
    "            # calculate current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Back-propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # calculate true labels\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "            # clip gradient to prevent exploding gradient\n",
    "            # problems\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # gradient descent\n",
    "            optimiser.step()\n",
    "            \n",
    "            # schedule learning rate accordingly\n",
    "            scheduler.step()\n",
    "\n",
    "        # calculate avg loss \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # calculate metrics\n",
    "        train_metrics = self.evalMetric(y_true,y_pred,\"Train_\")\n",
    "        \n",
    "        # print results\n",
    "        print('avg_train_loss',avg_train_loss)\n",
    "        print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "        print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "        # add loss to metrics\n",
    "        train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "        return train_metrics\n",
    "    \n",
    "    \n",
    "    ##------------------------------------------------------------##\n",
    "    ##----------------- Main Train Loop --------------------------##\n",
    "    ##------------------------------------------------------------##\n",
    "    def train(self,model,data_loaders,optimiser,scheduler,epochs,save_model):\n",
    "        # save train stats per epoch\n",
    "        train_stats = []\n",
    "        train_loader,val_loader,test_loader = data_loaders\n",
    "        # maintain best mF1 Score to save best model\n",
    "        best_mf1Score=-1.0\n",
    "        for epoch_i in range(0, epochs):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            \n",
    "            print(\"\")\n",
    "            print('Training...')\n",
    "            # run trian loop\n",
    "            train_metrics = self.run_train_loop(model,train_loader,\n",
    "                                            optimiser,scheduler)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\") \n",
    "            # test on validation set\n",
    "            val_metrics = self.evaluate(model,val_loader,\"Val\")\n",
    "            \n",
    "            print(\"Validation Loss: \",val_metrics['Val_avg_loss'])\n",
    "            print(\"Validation Accuracy: \",val_metrics['Val_accuracy'])\n",
    "            \n",
    "            stats = {}\n",
    "\n",
    "            # save model where validation mF1Score is best\n",
    "            if(val_metrics['Val_mF1Score']>best_mf1Score):\n",
    "                best_mf1Score=val_metrics['Val_mF1Score']\n",
    "                if(save_model):\n",
    "                    torch.save(model.state_dict(), self.model_save_path+\n",
    "                        '/best_bert_'+self.name+'.pt')\n",
    "                # evaluate best model on test set\n",
    "                test_metrics = self.evaluate(model,test_loader,\"Test\")\n",
    "\n",
    "            stats['epoch']=epoch_i+1\n",
    "\n",
    "            # add train and val metrics of the epoch to \n",
    "            # same dictionary\n",
    "            stats.update(train_metrics)\n",
    "            stats.update(val_metrics)\n",
    "\n",
    "            train_stats.append(stats)\n",
    "\n",
    "        return train_stats,test_metrics\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------------- Main Pipeline ---------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run(self,args,df_train,df_val,df_test):\n",
    "        # get X and Y data points \n",
    "        X_train = df_train['Text'].values\n",
    "        Y_train = df_train['Label'].values\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "        X_val = df_val['Text'].values\n",
    "        Y_val = df_val['Label'].values\n",
    "        \n",
    "        # encode data\n",
    "        # returns list of data and attention masks\n",
    "        train_data = self.encode(X_train,args['max_len'])\n",
    "        val_data = self.encode(X_val,args['max_len'])\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "        \n",
    "        # add labels to data so that we can send them to\n",
    "        # dataloader function together\n",
    "        train_data.append(Y_train)\n",
    "        val_data.append(Y_val)\n",
    "        test_data.append(Y_test)\n",
    "        \n",
    "        # convert to dataloader\n",
    "        train_dl =self.get_dataloader(train_data,args['batch_size'],True)\n",
    "        val_dl =self.get_dataloader(val_data,args['batch_size'])                          \n",
    "        test_dl =self.get_dataloader(test_data,args['batch_size'])\n",
    "        \n",
    "        # intialise model\n",
    "        model = weighted_Roberta.from_pretrained(\n",
    "            'xlm-roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
    "            params=args['params'],\n",
    "        )\n",
    "        model.to(self.device)\n",
    "        \n",
    "        optimiser = self.get_optimiser(args['learning_rate'],model)\n",
    "        \n",
    "        scheduler = self.get_scheduler(args['epochs'],optimiser,train_dl)\n",
    "        \n",
    "        # Run train loop and evaluate on validation data set\n",
    "        # on each epoch. Store best model from all epochs \n",
    "        # (best mF1 Score on Val set) and evaluate it on\n",
    "        # test set\n",
    "        train_stats,train_metrics = self.train(model,[train_dl,val_dl,test_dl],\n",
    "                                optimiser,scheduler,args['epochs'],args['save_model'])\n",
    "        \n",
    "        return train_stats,train_metrics\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##-------------------- Other Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run_test(self,model,df_test,args):\n",
    "        # to evaluate test set on the final saved model\n",
    "        # to retrieve results if necessary\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "\n",
    "        test_data.append(Y_test)\n",
    "\n",
    "        test_dl =self.get_dataloader(test_data,32)\n",
    "\n",
    "        metrics = self.evaluate(model,test_dl,\"Test\")\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def load_model(self,path,args):\n",
    "        # load saved best model\n",
    "        saved_model = weighted_Roberta.from_pretrained(\n",
    "            'xlm-roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
    "            params=args['params'],\n",
    "        )\n",
    "        \n",
    "        saved_model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        return saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f94214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,isArabic):\n",
    "    \n",
    "    X = df['Text']\n",
    "    X_new=[]\n",
    "    if(isArabic):\n",
    "        prep = ArabertPreprocessor('bert-base-arabertv02')\n",
    "        for text in tqdm(X):\n",
    "            text = prep.preprocess(text)\n",
    "            X_new.append(text)\n",
    "    else:\n",
    "        processer = Data_Preprocessing()\n",
    "        for text in tqdm(X):\n",
    "            text= processer.removeEmojis(text)\n",
    "            text = processer.removeUrls(text)\n",
    "            text=processer.removeSpecialChar(text)\n",
    "            X_new.append(text)\n",
    "\n",
    "    df['Text']=X_new\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8346cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(args,data_path,index):\n",
    "    # read dataframes\n",
    "    df_test = pd.read_csv(data_path+'test_'+str(index)+'.csv')\n",
    "\n",
    "    # clean data\n",
    "    df_test=preprocess(df_test,args['isArabic'])\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b73f61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot_output(model_path,data_path,obj,args):\n",
    "    saved_model=obj.load_model(model_path,args)\n",
    "    device = torch.device(args['device'])\n",
    "    saved_model=saved_model.to(device)\n",
    "    \n",
    "    all_metrics=[]\n",
    "    avg_metrics={}\n",
    "    \n",
    "    # preprocessing\n",
    "    for fold in [1,2,3,4,5]:\n",
    "        df = load_dataset(args,data_path,fold)\n",
    "\n",
    "        metrics = obj.run_test(saved_model,df,args)\n",
    "        \n",
    "        for key,value in metrics.items():\n",
    "            if(key not in avg_metrics):\n",
    "                avg_metrics[key]=value\n",
    "            else:\n",
    "                avg_metrics[key]+=value\n",
    "        \n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    for key,value in avg_metrics.items():\n",
    "        avg_metrics[key]/=5\n",
    "    \n",
    "    return avg_metrics,all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f6b8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneShotOutput(model_path,data_path):\n",
    "    model_args={\n",
    "        'seed_val': 42,\n",
    "        'name': 'xlm_roberta',\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if('Let-Mi' in data_path):\n",
    "        model_args['isArabic']=True\n",
    "\n",
    "    model = XLM_Roberta(model_args)\n",
    "\n",
    "    avg_metrics,all_metrics = one_shot_output(model_path,data_path,model,model_args)\n",
    "    \n",
    "    return avg_metrics,all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93926640",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['Let-Mi','AMI-2020','AMI-Spanish','Shared_Task_eng',\n",
    "          'Shared_Task_hin','Shared_Task_iben']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3f54861",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths={\n",
    "    'Let-Mi':{\n",
    "        'model_path':'Saved_Models/Let-Mi/best_bert_xlm_roberta_3_all.pt',\n",
    "        'data_path':'Data_Processed/Let-Mi/',\n",
    "    },\n",
    "    'AMI-2020':{\n",
    "        'model_path': 'Saved_Models/AMI-2020/best_bert_xlm_roberta_2_all.pt',\n",
    "        'data_path': 'Data_Processed/AMI-2020/',\n",
    "    },\n",
    "    'AMI-Spanish':{\n",
    "        'model_path': 'Saved_Models/AMI-Spanish/best_bert_xlm_roberta_1_all.pt',\n",
    "        'data_path': 'Data_Processed/AMI-Spanish/',\n",
    "    },\n",
    "    'Shared_Task_eng':{\n",
    "        'model_path': 'Saved_Models/Shared_Task_eng/best_bert_xlm_roberta_4_all.pt',\n",
    "        'data_path': 'Data_Processed/Shared_Task_eng/',\n",
    "    },\n",
    "    'Shared_Task_iben':{\n",
    "        'model_path': 'Saved_Models/Shared_Task_iben/best_bert_xlm_roberta_4_all.pt',\n",
    "        'data_path': 'Data_Processed/Shared_Task_iben/',\n",
    "    },\n",
    "    'Shared_Task_hin':{\n",
    "        'model_path': 'Saved_Models/Shared_Task_hin/best_bert_xlm_roberta_1_all.pt',\n",
    "        'data_path': 'Data_Processed/Shared_Task_hin/',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9455c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4913624d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1983/1983 [00:00<00:00, 2154.44it/s]\n",
      "100%|| 1983/1983 [00:19<00:00, 101.67it/s]\n",
      "100%|| 61/61 [00:13<00:00,  4.47it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3099.70it/s]\n",
      "100%|| 1983/1983 [00:19<00:00, 103.72it/s]\n",
      "100%|| 61/61 [00:10<00:00,  6.03it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3226.16it/s]\n",
      "100%|| 1983/1983 [00:18<00:00, 104.73it/s]\n",
      "100%|| 61/61 [00:10<00:00,  5.98it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3123.72it/s]\n",
      "100%|| 1983/1983 [00:19<00:00, 103.78it/s]\n",
      "100%|| 61/61 [00:10<00:00,  6.00it/s]\n",
      "100%|| 1990/1990 [00:00<00:00, 2984.21it/s]\n",
      "100%|| 1990/1990 [00:19<00:00, 104.00it/s]\n",
      "100%|| 62/62 [00:10<00:00,  5.98it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 660/660 [00:00<00:00, 2206.13it/s]\n",
      "100%|| 660/660 [00:02<00:00, 298.49it/s] \n",
      "100%|| 20/20 [00:03<00:00,  6.11it/s]\n",
      "100%|| 660/660 [00:00<00:00, 1914.37it/s]\n",
      "100%|| 660/660 [00:02<00:00, 298.50it/s] \n",
      "100%|| 20/20 [00:03<00:00,  6.02it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2124.76it/s]\n",
      "100%|| 660/660 [00:02<00:00, 296.50it/s] \n",
      "100%|| 20/20 [00:03<00:00,  5.95it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2235.40it/s]\n",
      "100%|| 660/660 [00:02<00:00, 298.69it/s] \n",
      "100%|| 20/20 [00:03<00:00,  5.97it/s]\n",
      "100%|| 667/667 [00:00<00:00, 2165.32it/s]\n",
      "100%|| 667/667 [00:02<00:00, 295.74it/s] \n",
      "100%|| 20/20 [00:03<00:00,  6.15it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 3266/3266 [00:02<00:00, 1240.09it/s]\n",
      "100%|| 3266/3266 [00:51<00:00, 62.85it/s] \n",
      "100%|| 102/102 [00:21<00:00,  4.76it/s]\n",
      "100%|| 3266/3266 [00:02<00:00, 1150.67it/s]\n",
      "100%|| 3266/3266 [00:52<00:00, 62.02it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.02it/s]\n",
      "100%|| 3266/3266 [00:03<00:00, 1059.62it/s]\n",
      "100%|| 3266/3266 [00:52<00:00, 62.14it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.05it/s]\n",
      "100%|| 3266/3266 [00:02<00:00, 1110.60it/s]\n",
      "100%|| 3266/3266 [00:51<00:00, 63.08it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.04it/s]\n",
      "100%|| 3271/3271 [00:02<00:00, 1144.65it/s]\n",
      "100%|| 3271/3271 [00:51<00:00, 63.14it/s] \n",
      "100%|| 102/102 [00:21<00:00,  4.73it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1236/1236 [00:00<00:00, 2478.67it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 163.17it/s]\n",
      "100%|| 38/38 [00:06<00:00,  5.86it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2373.72it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 167.50it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.01it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2513.72it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 163.70it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.10it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2658.37it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 166.63it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.04it/s]\n",
      "100%|| 1237/1237 [00:00<00:00, 2488.02it/s]\n",
      "100%|| 1237/1237 [00:07<00:00, 166.98it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.03it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1194/1194 [00:00<00:00, 4917.63it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 173.71it/s]\n",
      "100%|| 37/37 [00:05<00:00,  6.17it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 5163.01it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 174.86it/s]\n",
      "100%|| 37/37 [00:06<00:00,  6.10it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4559.37it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 172.18it/s]\n",
      "100%|| 37/37 [00:06<00:00,  5.98it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4658.07it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 171.91it/s]\n",
      "100%|| 37/37 [00:05<00:00,  6.18it/s]\n",
      "100%|| 1195/1195 [00:00<00:00, 4854.80it/s]\n",
      "100%|| 1195/1195 [00:06<00:00, 173.95it/s]\n",
      "100%|| 37/37 [00:06<00:00,  5.97it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1047/1047 [00:00<00:00, 9258.25it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 192.85it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.92it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10855.73it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 196.60it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.92it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10313.91it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 198.31it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.98it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10714.40it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 195.21it/s]\n",
      "100%|| 32/32 [00:05<00:00,  6.11it/s]\n",
      "100%|| 1052/1052 [00:00<00:00, 10884.89it/s]\n",
      "100%|| 1052/1052 [00:05<00:00, 195.92it/s]\n",
      "100%|| 32/32 [00:07<00:00,  4.53it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 660/660 [00:00<00:00, 2198.63it/s]\n",
      "100%|| 660/660 [00:02<00:00, 295.21it/s] \n",
      "100%|| 20/20 [00:04<00:00,  4.51it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2159.45it/s]\n",
      "100%|| 660/660 [00:02<00:00, 303.04it/s] \n",
      "100%|| 20/20 [00:03<00:00,  5.41it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2296.89it/s]\n",
      "100%|| 660/660 [00:02<00:00, 287.11it/s] \n",
      "100%|| 20/20 [00:04<00:00,  4.50it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2127.74it/s]\n",
      "100%|| 660/660 [00:02<00:00, 303.84it/s] \n",
      "100%|| 20/20 [00:04<00:00,  4.50it/s]\n",
      "100%|| 667/667 [00:00<00:00, 2158.03it/s]\n",
      "100%|| 667/667 [00:02<00:00, 287.24it/s] \n",
      "100%|| 20/20 [00:04<00:00,  4.45it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 3266/3266 [00:02<00:00, 1246.95it/s]\n",
      "100%|| 3266/3266 [00:52<00:00, 62.17it/s] \n",
      "100%|| 102/102 [00:17<00:00,  5.96it/s]\n",
      "100%|| 3266/3266 [00:02<00:00, 1119.48it/s]\n",
      "100%|| 3266/3266 [00:53<00:00, 61.10it/s] \n",
      "100%|| 102/102 [00:17<00:00,  6.00it/s]\n",
      "100%|| 3266/3266 [00:02<00:00, 1090.36it/s]\n",
      "100%|| 3266/3266 [00:52<00:00, 62.64it/s] \n",
      "100%|| 102/102 [00:22<00:00,  4.51it/s]\n",
      "100%|| 3266/3266 [00:02<00:00, 1104.91it/s]\n",
      "100%|| 3266/3266 [00:52<00:00, 62.41it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.02it/s]\n",
      "100%|| 3271/3271 [00:02<00:00, 1157.10it/s]\n",
      "100%|| 3271/3271 [00:52<00:00, 62.51it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.09it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1236/1236 [00:00<00:00, 2477.91it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 165.67it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.10it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2406.33it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 167.35it/s]\n",
      "100%|| 38/38 [00:06<00:00,  5.97it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2691.43it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 166.82it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.03it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2657.83it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 165.66it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.01it/s]\n",
      "100%|| 1237/1237 [00:00<00:00, 2526.54it/s]\n",
      "100%|| 1237/1237 [00:07<00:00, 166.19it/s]\n",
      "100%|| 38/38 [00:06<00:00,  5.99it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1194/1194 [00:00<00:00, 5389.30it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 173.53it/s]\n",
      "100%|| 37/37 [00:08<00:00,  4.42it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4510.97it/s]\n",
      "100%|| 1194/1194 [00:07<00:00, 170.39it/s]\n",
      "100%|| 37/37 [00:08<00:00,  4.50it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4630.04it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 174.67it/s]\n",
      "100%|| 37/37 [00:08<00:00,  4.53it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4563.91it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 172.17it/s]\n",
      "100%|| 37/37 [00:08<00:00,  4.45it/s]\n",
      "100%|| 1195/1195 [00:00<00:00, 4536.18it/s]\n",
      "100%|| 1195/1195 [00:06<00:00, 173.98it/s]\n",
      "100%|| 37/37 [00:08<00:00,  4.55it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1047/1047 [00:00<00:00, 8937.65it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 189.09it/s]\n",
      "100%|| 32/32 [00:05<00:00,  6.12it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10307.45it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 196.22it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.92it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10491.02it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 195.92it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.99it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10540.07it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 194.73it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.87it/s]\n",
      "100%|| 1052/1052 [00:00<00:00, 10290.63it/s]\n",
      "100%|| 1052/1052 [00:05<00:00, 192.20it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.89it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1983/1983 [00:00<00:00, 2856.83it/s]\n",
      "100%|| 1983/1983 [00:19<00:00, 101.22it/s]\n",
      "100%|| 61/61 [00:10<00:00,  5.96it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 2828.53it/s]\n",
      "100%|| 1983/1983 [00:19<00:00, 99.92it/s] \n",
      "100%|| 61/61 [00:10<00:00,  5.94it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3229.85it/s]\n",
      "100%|| 1983/1983 [00:19<00:00, 104.05it/s]\n",
      "100%|| 61/61 [00:10<00:00,  5.96it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3161.85it/s]\n",
      "100%|| 1983/1983 [00:19<00:00, 103.51it/s]\n",
      "100%|| 61/61 [00:10<00:00,  5.95it/s]\n",
      "100%|| 1990/1990 [00:00<00:00, 3103.56it/s]\n",
      "100%|| 1990/1990 [00:19<00:00, 101.96it/s]\n",
      "100%|| 62/62 [00:13<00:00,  4.52it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 3266/3266 [00:02<00:00, 1202.49it/s]\n",
      "100%|| 3266/3266 [00:51<00:00, 63.00it/s] \n",
      "100%|| 102/102 [00:17<00:00,  5.94it/s]\n",
      "100%|| 3266/3266 [00:02<00:00, 1123.13it/s]\n",
      "100%|| 3266/3266 [00:52<00:00, 62.79it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.06it/s]\n",
      "100%|| 3266/3266 [00:03<00:00, 1086.91it/s]\n",
      "100%|| 3266/3266 [00:52<00:00, 62.61it/s] \n",
      "100%|| 102/102 [00:17<00:00,  5.98it/s]\n",
      "100%|| 3266/3266 [00:02<00:00, 1122.96it/s]\n",
      "100%|| 3266/3266 [00:53<00:00, 61.39it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.05it/s]\n",
      "100%|| 3271/3271 [00:02<00:00, 1164.51it/s]\n",
      "100%|| 3271/3271 [00:52<00:00, 62.48it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.05it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1236/1236 [00:00<00:00, 2490.59it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 162.09it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.08it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2411.29it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 166.53it/s]\n",
      "100%|| 38/38 [00:06<00:00,  5.94it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2508.93it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 166.96it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.07it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2436.20it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 164.03it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.03it/s]\n",
      "100%|| 1237/1237 [00:00<00:00, 2495.89it/s]\n",
      "100%|| 1237/1237 [00:07<00:00, 161.46it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.11it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1194/1194 [00:00<00:00, 5454.74it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 174.53it/s]\n",
      "100%|| 37/37 [00:06<00:00,  6.13it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 5209.85it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 171.81it/s]\n",
      "100%|| 37/37 [00:06<00:00,  6.15it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4632.86it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 174.66it/s]\n",
      "100%|| 37/37 [00:08<00:00,  4.53it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4340.72it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 171.53it/s]\n",
      "100%|| 37/37 [00:08<00:00,  4.53it/s]\n",
      "100%|| 1195/1195 [00:00<00:00, 4885.60it/s]\n",
      "100%|| 1195/1195 [00:06<00:00, 173.40it/s]\n",
      "100%|| 37/37 [00:08<00:00,  4.51it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1047/1047 [00:00<00:00, 10143.43it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 191.01it/s]\n",
      "100%|| 32/32 [00:07<00:00,  4.49it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|| 1047/1047 [00:00<00:00, 10852.97it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 190.45it/s]\n",
      "100%|| 32/32 [00:04<00:00,  6.51it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|| 1047/1047 [00:00<00:00, 10011.32it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 189.41it/s]\n",
      "100%|| 32/32 [00:05<00:00,  6.05it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|| 1047/1047 [00:00<00:00, 10904.96it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 197.85it/s]\n",
      "100%|| 32/32 [00:05<00:00,  6.06it/s]\n",
      "100%|| 1052/1052 [00:00<00:00, 10267.07it/s]\n",
      "100%|| 1052/1052 [00:05<00:00, 194.39it/s]\n",
      "100%|| 32/32 [00:05<00:00,  6.07it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1983/1983 [00:00<00:00, 3263.75it/s]\n",
      "100%|| 1983/1983 [00:18<00:00, 104.62it/s]\n",
      "100%|| 61/61 [00:09<00:00,  6.13it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 2990.00it/s]\n",
      "100%|| 1983/1983 [00:19<00:00, 104.23it/s]\n",
      "100%|| 61/61 [00:10<00:00,  5.96it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3213.01it/s]\n",
      "100%|| 1983/1983 [00:18<00:00, 105.04it/s]\n",
      "100%|| 61/61 [00:10<00:00,  6.02it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3191.02it/s]\n",
      "100%|| 1983/1983 [00:18<00:00, 107.47it/s]\n",
      "100%|| 61/61 [00:10<00:00,  5.97it/s]\n",
      "100%|| 1990/1990 [00:00<00:00, 3092.23it/s]\n",
      "100%|| 1990/1990 [00:18<00:00, 107.57it/s]\n",
      "100%|| 62/62 [00:10<00:00,  6.08it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 660/660 [00:00<00:00, 2249.69it/s]\n",
      "100%|| 660/660 [00:02<00:00, 303.17it/s] \n",
      "100%|| 20/20 [00:03<00:00,  5.96it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2367.82it/s]\n",
      "100%|| 660/660 [00:02<00:00, 306.89it/s] \n",
      "100%|| 20/20 [00:04<00:00,  4.47it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2169.62it/s]\n",
      "100%|| 660/660 [00:02<00:00, 298.32it/s] \n",
      "100%|| 20/20 [00:04<00:00,  4.45it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2068.67it/s]\n",
      "100%|| 660/660 [00:02<00:00, 304.57it/s] \n",
      "100%|| 20/20 [00:04<00:00,  4.18it/s]\n",
      "100%|| 667/667 [00:00<00:00, 2231.37it/s]\n",
      "100%|| 667/667 [00:02<00:00, 298.18it/s] \n",
      "100%|| 20/20 [00:04<00:00,  4.51it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1236/1236 [00:00<00:00, 2501.23it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 168.31it/s]\n",
      "100%|| 38/38 [00:08<00:00,  4.43it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2134.32it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 165.24it/s]\n",
      "100%|| 38/38 [00:08<00:00,  4.54it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2550.61it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 168.81it/s]\n",
      "100%|| 38/38 [00:06<00:00,  5.96it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2675.79it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 164.90it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.15it/s]\n",
      "100%|| 1237/1237 [00:00<00:00, 2584.13it/s]\n",
      "100%|| 1237/1237 [00:07<00:00, 165.99it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.01it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1194/1194 [00:00<00:00, 5453.42it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 176.34it/s]\n",
      "100%|| 37/37 [00:06<00:00,  5.95it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4307.38it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 174.13it/s]\n",
      "100%|| 37/37 [00:06<00:00,  6.04it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4510.31it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 176.32it/s]\n",
      "100%|| 37/37 [00:06<00:00,  6.03it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4588.48it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 175.55it/s]\n",
      "100%|| 37/37 [00:06<00:00,  5.99it/s]\n",
      "100%|| 1195/1195 [00:00<00:00, 4982.78it/s]\n",
      "100%|| 1195/1195 [00:06<00:00, 176.25it/s]\n",
      "100%|| 37/37 [00:06<00:00,  5.97it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1047/1047 [00:00<00:00, 10210.15it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 176.43it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.96it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10239.57it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 196.93it/s]\n",
      "100%|| 32/32 [00:05<00:00,  6.07it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10512.90it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 196.18it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.97it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10948.26it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 195.25it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.99it/s]\n",
      "100%|| 1052/1052 [00:00<00:00, 10796.49it/s]\n",
      "100%|| 1052/1052 [00:05<00:00, 198.53it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.90it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1983/1983 [00:00<00:00, 3302.11it/s]\n",
      "100%|| 1983/1983 [00:19<00:00, 104.33it/s]\n",
      "100%|| 61/61 [00:13<00:00,  4.51it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 2864.36it/s]\n",
      "100%|| 1983/1983 [00:18<00:00, 105.51it/s]\n",
      "100%|| 61/61 [00:13<00:00,  4.46it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3205.26it/s]\n",
      "100%|| 1983/1983 [00:18<00:00, 106.09it/s]\n",
      "100%|| 61/61 [00:10<00:00,  6.06it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3163.11it/s]\n",
      "100%|| 1983/1983 [00:18<00:00, 105.57it/s]\n",
      "100%|| 61/61 [00:10<00:00,  5.93it/s]\n",
      "100%|| 1990/1990 [00:00<00:00, 3143.06it/s]\n",
      "100%|| 1990/1990 [00:19<00:00, 104.54it/s]\n",
      "100%|| 62/62 [00:10<00:00,  6.00it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 660/660 [00:00<00:00, 1988.38it/s]\n",
      "100%|| 660/660 [00:02<00:00, 304.53it/s] \n",
      "100%|| 20/20 [00:03<00:00,  6.13it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2361.77it/s]\n",
      "100%|| 660/660 [00:02<00:00, 307.52it/s] \n",
      "100%|| 20/20 [00:03<00:00,  6.07it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2280.67it/s]\n",
      "100%|| 660/660 [00:02<00:00, 307.75it/s] \n",
      "100%|| 20/20 [00:03<00:00,  5.88it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2233.01it/s]\n",
      "100%|| 660/660 [00:02<00:00, 308.25it/s] \n",
      "100%|| 20/20 [00:03<00:00,  6.08it/s]\n",
      "100%|| 667/667 [00:00<00:00, 2362.95it/s]\n",
      "100%|| 667/667 [00:02<00:00, 303.11it/s] \n",
      "100%|| 20/20 [00:03<00:00,  6.09it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 3266/3266 [00:03<00:00, 1071.46it/s]\n",
      "100%|| 3266/3266 [01:01<00:00, 53.49it/s] \n",
      "100%|| 102/102 [00:22<00:00,  4.51it/s]\n",
      "100%|| 3266/3266 [00:03<00:00, 941.57it/s] \n",
      "100%|| 3266/3266 [01:03<00:00, 51.29it/s]\n",
      "100%|| 102/102 [00:16<00:00,  6.00it/s]\n",
      "100%|| 3266/3266 [00:03<00:00, 956.69it/s] \n",
      "100%|| 3266/3266 [01:00<00:00, 53.89it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.02it/s]\n",
      "100%|| 3266/3266 [00:03<00:00, 990.90it/s] \n",
      "100%|| 3266/3266 [01:00<00:00, 53.65it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.03it/s]\n",
      "100%|| 3271/3271 [00:03<00:00, 938.57it/s] \n",
      "100%|| 3271/3271 [01:00<00:00, 53.97it/s]\n",
      "100%|| 102/102 [00:17<00:00,  5.95it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1194/1194 [00:00<00:00, 5593.84it/s]\n",
      "100%|| 1194/1194 [00:07<00:00, 151.72it/s]\n",
      "100%|| 37/37 [00:06<00:00,  5.90it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 5045.50it/s]\n",
      "100%|| 1194/1194 [00:07<00:00, 156.10it/s]\n",
      "100%|| 37/37 [00:06<00:00,  5.94it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4667.98it/s]\n",
      "100%|| 1194/1194 [00:08<00:00, 146.72it/s]\n",
      "100%|| 37/37 [00:06<00:00,  5.93it/s]\n",
      "100%|| 1194/1194 [00:00<00:00, 4621.17it/s]\n",
      "100%|| 1194/1194 [00:06<00:00, 175.01it/s]\n",
      "100%|| 37/37 [00:06<00:00,  5.89it/s]\n",
      "100%|| 1195/1195 [00:00<00:00, 4623.67it/s]\n",
      "100%|| 1195/1195 [00:06<00:00, 176.15it/s]\n",
      "100%|| 37/37 [00:06<00:00,  6.10it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1047/1047 [00:00<00:00, 7815.20it/s]\n",
      "100%|| 1047/1047 [00:05<00:00, 182.95it/s]\n",
      "100%|| 32/32 [00:05<00:00,  6.02it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 8695.95it/s]\n",
      "100%|| 1047/1047 [00:06<00:00, 173.63it/s]\n",
      "100%|| 32/32 [00:05<00:00,  6.02it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10071.99it/s]\n",
      "100%|| 1047/1047 [00:06<00:00, 155.25it/s]\n",
      "100%|| 32/32 [00:05<00:00,  6.07it/s]\n",
      "100%|| 1047/1047 [00:00<00:00, 10664.05it/s]\n",
      "100%|| 1047/1047 [00:06<00:00, 166.86it/s]\n",
      "100%|| 32/32 [00:05<00:00,  6.01it/s]\n",
      "100%|| 1052/1052 [00:00<00:00, 8714.61it/s]\n",
      "100%|| 1052/1052 [00:06<00:00, 163.76it/s]\n",
      "100%|| 32/32 [00:05<00:00,  5.99it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1983/1983 [00:00<00:00, 2722.63it/s]\n",
      "100%|| 1983/1983 [00:21<00:00, 90.88it/s] \n",
      "100%|| 61/61 [00:12<00:00,  4.81it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 2383.76it/s]\n",
      "100%|| 1983/1983 [00:20<00:00, 95.44it/s] \n",
      "100%|| 61/61 [00:12<00:00,  4.98it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3135.73it/s]\n",
      "100%|| 1983/1983 [00:18<00:00, 104.48it/s]\n",
      "100%|| 61/61 [00:11<00:00,  5.49it/s]\n",
      "100%|| 1983/1983 [00:00<00:00, 3100.39it/s]\n",
      "100%|| 1983/1983 [00:18<00:00, 105.05it/s]\n",
      "100%|| 61/61 [00:10<00:00,  6.01it/s]\n",
      "100%|| 1990/1990 [00:00<00:00, 3093.89it/s]\n",
      "100%|| 1990/1990 [00:19<00:00, 104.01it/s]\n",
      "100%|| 62/62 [00:10<00:00,  6.08it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 660/660 [00:00<00:00, 2268.84it/s]\n",
      "100%|| 660/660 [00:02<00:00, 307.76it/s] \n",
      "100%|| 20/20 [00:03<00:00,  5.84it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2328.74it/s]\n",
      "100%|| 660/660 [00:02<00:00, 311.94it/s] \n",
      "100%|| 20/20 [00:03<00:00,  5.90it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2251.50it/s]\n",
      "100%|| 660/660 [00:02<00:00, 307.68it/s] \n",
      "100%|| 20/20 [00:03<00:00,  6.02it/s]\n",
      "100%|| 660/660 [00:00<00:00, 2162.65it/s]\n",
      "100%|| 660/660 [00:02<00:00, 303.09it/s] \n",
      "100%|| 20/20 [00:03<00:00,  5.90it/s]\n",
      "100%|| 667/667 [00:00<00:00, 2125.11it/s]\n",
      "100%|| 667/667 [00:02<00:00, 291.19it/s] \n",
      "100%|| 20/20 [00:03<00:00,  6.14it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 3266/3266 [00:02<00:00, 1239.98it/s]\n",
      "100%|| 3266/3266 [00:51<00:00, 63.54it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.03it/s]\n",
      "100%|| 3266/3266 [00:02<00:00, 1164.73it/s]\n",
      "100%|| 3266/3266 [00:51<00:00, 63.61it/s] \n",
      "100%|| 102/102 [00:22<00:00,  4.50it/s]\n",
      "100%|| 3266/3266 [00:03<00:00, 1087.23it/s]\n",
      "100%|| 3266/3266 [00:52<00:00, 61.69it/s] \n",
      "100%|| 102/102 [00:17<00:00,  5.93it/s]\n",
      "100%|| 3266/3266 [00:02<00:00, 1101.01it/s]\n",
      "100%|| 3266/3266 [00:51<00:00, 63.76it/s] \n",
      "100%|| 102/102 [00:16<00:00,  6.01it/s]\n",
      "100%|| 3271/3271 [00:02<00:00, 1144.35it/s]\n",
      "100%|| 3271/3271 [00:51<00:00, 63.62it/s] \n",
      "100%|| 102/102 [00:22<00:00,  4.44it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1236/1236 [00:00<00:00, 2539.52it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 166.41it/s]\n",
      "100%|| 38/38 [00:08<00:00,  4.45it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2420.15it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 165.55it/s]\n",
      "100%|| 38/38 [00:08<00:00,  4.50it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|| 1236/1236 [00:00<00:00, 2272.82it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 165.32it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.07it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2699.45it/s]\n",
      "100%|| 1236/1236 [00:07<00:00, 168.13it/s]\n",
      "100%|| 38/38 [00:06<00:00,  6.03it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|| 1237/1237 [00:00<00:00, 2461.76it/s]\n",
      "100%|| 1237/1237 [00:07<00:00, 167.46it/s]\n",
      "100%|| 38/38 [00:06<00:00,  5.93it/s]\n",
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for dataset1 in datasets:\n",
    "    for dataset2 in datasets:\n",
    "        if(dataset1!=dataset2):\n",
    "            name = dataset1+'_'+dataset2\n",
    "            avg_metrics,all_metrics = getOneShotOutput(paths[dataset1]['model_path'],\n",
    "                                      paths[dataset2]['data_path'])\n",
    "            metrics['Name']=dataset1+'_'+dataset2\n",
    "            res.append(metrics)\n",
    "            df = pd.DataFrame(res)\n",
    "            df.to_csv('Results_Processed/one_shot_matrix_xlm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea0313bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|| 1236/1236 [00:00<00:00, 2683.33it/s]\n",
      "100%|| 1236/1236 [00:06<00:00, 183.17it/s]\n",
      "100%|| 38/38 [00:04<00:00,  7.83it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2560.92it/s]\n",
      "100%|| 1236/1236 [00:06<00:00, 181.99it/s]\n",
      "100%|| 38/38 [00:04<00:00,  7.83it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2734.14it/s]\n",
      "100%|| 1236/1236 [00:06<00:00, 183.29it/s]\n",
      "100%|| 38/38 [00:04<00:00,  7.86it/s]\n",
      "100%|| 1236/1236 [00:00<00:00, 2829.74it/s]\n",
      "100%|| 1236/1236 [00:06<00:00, 182.54it/s]\n",
      "100%|| 38/38 [00:04<00:00,  7.89it/s]\n",
      "100%|| 1237/1237 [00:00<00:00, 2681.19it/s]\n",
      "100%|| 1237/1237 [00:06<00:00, 182.95it/s]\n",
      "100%|| 38/38 [00:04<00:00,  7.89it/s]\n"
     ]
    }
   ],
   "source": [
    "avg_metrics,all_metrics = getOneShotOutput(paths['Shared_Task_iben']['model_path'],\n",
    "                                      paths['Shared_Task_hin']['data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "487ada91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Test_accuracy': 0.6973684210526315,\n",
       "  'Test_mF1Score': 0.5339909357504665,\n",
       "  'Test_f1Score': 0.25806451612903225,\n",
       "  'Test_auc': 0.5328978617665647,\n",
       "  'Test_precision': 0.28193832599118945,\n",
       "  'Test_recall': 0.2379182156133829,\n",
       "  'Test_non_hatef1Score': 0.8099173553719008,\n",
       "  'Test_non_recallScore': 0.8278775079197466,\n",
       "  'Test_non_precisionScore': 0.7927199191102123,\n",
       "  'Test_avg_loss': 0.6579169536891737},\n",
       " {'Test_accuracy': 0.6743421052631579,\n",
       "  'Test_mF1Score': 0.49396162172386976,\n",
       "  'Test_f1Score': 0.19183673469387755,\n",
       "  'Test_auc': 0.4960636632727386,\n",
       "  'Test_precision': 0.2175925925925926,\n",
       "  'Test_recall': 0.17153284671532848,\n",
       "  'Test_non_hatef1Score': 0.796086508753862,\n",
       "  'Test_non_recallScore': 0.8205944798301487,\n",
       "  'Test_non_precisionScore': 0.773,\n",
       "  'Test_avg_loss': 0.6599660327560023},\n",
       " {'Test_accuracy': 0.6949013157894737,\n",
       "  'Test_mF1Score': 0.5131312411842345,\n",
       "  'Test_f1Score': 0.2156448202959831,\n",
       "  'Test_auc': 0.5143008474576272,\n",
       "  'Test_precision': 0.2537313432835821,\n",
       "  'Test_recall': 0.1875,\n",
       "  'Test_non_hatef1Score': 0.8106176620724859,\n",
       "  'Test_non_recallScore': 0.8411016949152542,\n",
       "  'Test_non_precisionScore': 0.7822660098522167,\n",
       "  'Test_avg_loss': 0.6582005714115343},\n",
       " {'Test_accuracy': 0.6932565789473685,\n",
       "  'Test_mF1Score': 0.5105066117566562,\n",
       "  'Test_f1Score': 0.2114164904862579,\n",
       "  'Test_auc': 0.5120416875453991,\n",
       "  'Test_precision': 0.25,\n",
       "  'Test_recall': 0.18315018315018314,\n",
       "  'Test_non_hatef1Score': 0.8095967330270546,\n",
       "  'Test_non_recallScore': 0.8409331919406151,\n",
       "  'Test_non_precisionScore': 0.780511811023622,\n",
       "  'Test_avg_loss': 0.66056479905781},\n",
       " {'Test_accuracy': 0.7146381578947368,\n",
       "  'Test_mF1Score': 0.5401844919931783,\n",
       "  'Test_f1Score': 0.2569593147751606,\n",
       "  'Test_auc': 0.5387434350533982,\n",
       "  'Test_precision': 0.30612244897959184,\n",
       "  'Test_recall': 0.22140221402214022,\n",
       "  'Test_non_hatef1Score': 0.823409669211196,\n",
       "  'Test_non_recallScore': 0.8560846560846561,\n",
       "  'Test_non_precisionScore': 0.7931372549019607,\n",
       "  'Test_avg_loss': 0.6534131106577421}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd31a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
