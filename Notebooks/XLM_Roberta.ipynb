{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b12b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import Data_Preprocessing\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697aa325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  2 23:19:22 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    34W / 250W |   5044MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    38W / 250W |   4683MiB / 16280MiB |     16%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      8819      C   python                            597MiB |\n",
      "|    0   N/A  N/A      9561      C   python                           1317MiB |\n",
      "|    0   N/A  N/A     20704      C   python                           1111MiB |\n",
      "|    0   N/A  N/A     28313      C   /usr/bin/python3                 2015MiB |\n",
      "|    1   N/A  N/A     19207      C   .../ravi-pg/myenv/bin/python     1005MiB |\n",
      "|    1   N/A  N/A     28313      C   /usr/bin/python3                 2115MiB |\n",
      "|    1   N/A  N/A     30593      C   python3                          1561MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c693c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaPreTrainedModel,RobertaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bdd9aa",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5028468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed_val=42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd52b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(args,index):\n",
    "    # initialise constants \n",
    "    path = args['data_path']\n",
    "    train_cnt = args['train_cnt']\n",
    "    model_name = args['model_name']\n",
    "    # read dataframes\n",
    "    df_train = pd.read_csv(path+'train_'+str(index)+'.csv')\n",
    "    df_val = pd.read_csv(path+'val_'+str(index)+'.csv')\n",
    "    df_test = pd.read_csv(path+'test_'+str(index)+'.csv')\n",
    "\n",
    "    # clean data\n",
    "    df_train=preprocess(df_train,args['isArabic'])\n",
    "    df_val=preprocess(df_val,args['isArabic'])\n",
    "    df_test=preprocess(df_test,args['isArabic'])\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36e207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_part(args,index,seed,train_cnt):\n",
    "    # initialise constants \n",
    "    path = args['data_path']\n",
    "    # read dataframes\n",
    "    df_train = pd.read_csv(path+'train_'+str(index)+'.csv')\n",
    "    df_val = pd.read_csv(path+'val_'+str(index)+'.csv')\n",
    "    df_test = pd.read_csv(path+'test_'+str(index)+'.csv')\n",
    "    \n",
    "    # split train into hate and non-hate and take train_cnt\n",
    "    # samples of each\n",
    "    df_train_hate = df_train[df_train['Label'] == 1].sample(train_cnt,random_state=seed)\n",
    "    df_train_non_hate = df_train[df_train['Label'] == 0].sample(train_cnt,random_state=seed)\n",
    "    # concatenate hate and non_hate\n",
    "    df_train = pd.concat([df_train_hate, df_train_non_hate])\n",
    "    # shuffle the train data\n",
    "    df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # clean data\n",
    "    df_train=preprocess(df_train,args['isArabic'])\n",
    "    df_val=preprocess(df_val,args['isArabic'])\n",
    "    df_test=preprocess(df_test,args['isArabic'])\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc060579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part(args,run,train_cnt,index,all_test_metrics,model_args,seed):\n",
    "    model_name = args['model_name']\n",
    "    model_args['name']=model_name+'_'+str(train_cnt)+'_'+str(index)+'_'+str(run)\n",
    "    print(\"\\tInitialising Model....\")\n",
    "    model = XLM_Roberta(model_args)\n",
    "    print(\"\\tLoading Dataset....\")\n",
    "    df_train, df_val, df_test = load_dataset_part(args,index,seed,train_cnt)\n",
    "    print(\"\\tTraining Starts....\")\n",
    "    train_metrics, test_metrics = model.run(model_args, \n",
    "                    df_train, df_val, df_test)\n",
    "    \n",
    "    test_metrics['name']=model_args['name']\n",
    "    # Save train metrics after generating path\n",
    "    res_path=args['res_base_path']+model_name+'_'+model_args['name']\n",
    "    save_metrics(res_path,train_metrics,\"train\")\n",
    "    \n",
    "    all_test_metrics.append(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216b1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_part(run_args,model_args,train_cnt):\n",
    "    all_test_metrics=[]\n",
    "    seeds = [42,43,44]\n",
    "    for fold in [1, 2, 3, 4, 5]:\n",
    "        print(\"Fold: \",fold)\n",
    "        for run in [1,2,3]:\n",
    "            print(\"Run: \",run)\n",
    "            fix_random()\n",
    "            train_part(run_args,run,train_cnt,fold,all_test_metrics,model_args,seeds[run-1])\n",
    "            print(\"Saving Test Metrics....\")\n",
    "            save_metrics(run_args['res_base_path']+run_args['model_name']+\n",
    "             '_'+str(train_cnt),all_test_metrics,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bddce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,isArabic):\n",
    "    \n",
    "    X = df['Text']\n",
    "    X_new=[]\n",
    "    if(isArabic):\n",
    "        prep = ArabertPreprocessor('bert-base-arabertv02')\n",
    "        for text in tqdm(X):\n",
    "            text = prep.preprocess(text)\n",
    "            X_new.append(text)\n",
    "    else:\n",
    "        processer = Data_Preprocessing()\n",
    "        for text in tqdm(X):\n",
    "            text= processer.removeEmojis(text)\n",
    "            text = processer.removeUrls(text)\n",
    "            text=processer.removeSpecialChar(text)\n",
    "            X_new.append(text)\n",
    "\n",
    "    df['Text']=X_new\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c087c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(path,metrics,which):\n",
    "    df = pd.DataFrame(metrics)\n",
    "    df.to_csv(path+\"_\"+which+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39b375a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, index,all_test_metrics,model_args):\n",
    "    model_name = args['model_name']\n",
    "    model_args['name']=model_name+'_'+str(index)+'_all'\n",
    "    print(\"\\tInitialising Model....\")\n",
    "    model = XLM_Roberta(model_args)\n",
    "    print(\"\\tLoading Dataset....\")\n",
    "    df_train, df_val, df_test = load_dataset(args,index)\n",
    "    print(\"\\tTraining Starts....\")\n",
    "    train_metrics, test_metrics = model.run(model_args, \n",
    "                    df_train, df_val, df_test)\n",
    "    test_metrics['name']=model_args['name']\n",
    "\n",
    "    # Save train metrics after generating path\n",
    "    res_path=args['res_base_path']+model_name+'_'+model_args['name']\n",
    "    save_metrics(res_path,train_metrics,\"train\")\n",
    "    \n",
    "    all_test_metrics.append(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed9e97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args,model_args):\n",
    "    all_test_metrics=[]\n",
    "    \n",
    "    for fold in [1, 2, 3, 4, 5]:\n",
    "        print(\"Fold: \",fold)\n",
    "        fix_random()\n",
    "        train(args,fold,all_test_metrics,model_args)\n",
    "        print(\"Saving Test Metrics....\")\n",
    "        save_metrics(args['res_base_path']+args['model_name']+\n",
    "             '_'+str(args['train_cnt']),all_test_metrics,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c4c49",
   "metadata": {},
   "source": [
    "## Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a06c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import random\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Tokeniser\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# Utility\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Optimiser\n",
    "from transformers import AdamW\n",
    "\n",
    "# Model\n",
    "\n",
    "import torch.nn as nn\n",
    "from models import weighted_Roberta\n",
    "\n",
    "\n",
    "class XLM_Roberta:\n",
    "    def __init__(self,args):\n",
    "        # fix the random\n",
    "        random.seed(args['seed_val'])\n",
    "        np.random.seed(args['seed_val'])\n",
    "        torch.manual_seed(args['seed_val'])\n",
    "        torch.cuda.manual_seed_all(args['seed_val'])\n",
    "        \n",
    "        # set device\n",
    "        self.device = torch.device(args['device'])\n",
    "\n",
    "        self.weights=args['weights']\n",
    "        \n",
    "        # initiliase tokeniser\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case = True)\n",
    "\n",
    "        self.model_save_path = args['model_save_path']\n",
    "        self.name = args['name']\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Utility Functions -----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def encode(self,data,max_len):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sent in tqdm(data):\n",
    "            # use in-built tokeniser of Bert\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                            sent,\n",
    "                            add_special_tokens =True, # for [CLS] and [SEP]\n",
    "                            max_length = max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = True,\n",
    "#                             return_tensors = 'pt', # return pytorch tensors\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            # attention masks notify where padding has been added \n",
    "            # and where is the sentence\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            X_data = torch.tensor(input_ids)\n",
    "            attention_masks_data = torch.tensor(attention_masks)\n",
    "            \n",
    "        return [X_data,attention_masks_data]\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##------------------ Dataloader -----------------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def get_dataloader(self,samples, batch_size,is_train=False):\n",
    "        inputs,masks,labels = samples\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "#         inputs = torch.cat(inputs, dim=0)\n",
    "#         masks = torch.cat(masks, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # convert to dataset\n",
    "        data = TensorDataset(inputs,masks,labels)\n",
    "\n",
    "        if(is_train==False):\n",
    "            # use random sampler for training to shuffle\n",
    "            # train data\n",
    "            sampler = SequentialSampler(data)\n",
    "        else:\n",
    "            # order does not matter for validation as we just \n",
    "            # need the metrics\n",
    "            sampler = RandomSampler(data)  \n",
    "\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size,drop_last=True)\n",
    "\n",
    "        return dataloader\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------- Training Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def get_optimiser(self,learning_rate,model):\n",
    "        # using AdamW optimiser from transformers library\n",
    "        return AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    def get_scheduler(self,epochs,optimiser,train_dl):\n",
    "        total_steps = len(train_dl) * epochs\n",
    "        return get_linear_schedule_with_warmup(optimiser, \n",
    "                num_warmup_steps = 0, \n",
    "                num_training_steps = total_steps)\n",
    "    \n",
    "    def evalMetric(self, y_true, y_pred, prefix):\n",
    "        # calculate all the metrics and add prefix to them\n",
    "        # before saving in dictionary\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "        f1Score = f1_score(y_true, y_pred)\n",
    "        area_under_c = roc_auc_score(y_true, y_pred)\n",
    "        recallScore = recall_score(y_true, y_pred)\n",
    "        precisionScore = precision_score(y_true, y_pred)\n",
    "\n",
    "        nonhate_f1Score = f1_score(y_true, y_pred, pos_label=0)\n",
    "        non_recallScore = recall_score(y_true, y_pred, pos_label=0)\n",
    "        non_precisionScore = precision_score(y_true, y_pred, pos_label=0)\n",
    "        return {prefix+\"accuracy\": accuracy, prefix+'mF1Score': mf1Score, \n",
    "            prefix+'f1Score': f1Score, prefix+'auc': area_under_c,\n",
    "            prefix+'precision': precisionScore, \n",
    "            prefix+'recall': recallScore, \n",
    "            prefix+'non_hatef1Score': nonhate_f1Score, \n",
    "            prefix+'non_recallScore': non_recallScore, \n",
    "            prefix+'non_precisionScore': non_precisionScore}\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##---------------- Different Train Loops --------------------##\n",
    "    ##-----------------------------------------------------------## \n",
    "    def evaluate(self,model,loader,which):\n",
    "        # to evaluate model on test and validation set\n",
    "\n",
    "        model.eval() # put model in eval mode\n",
    "\n",
    "        # maintain total loss to save in metrics\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(loader):\n",
    "            # separate input, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            with torch.no_grad(): # do not construct compute graph\n",
    "                outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            # output is always a tuple, thus we have to \n",
    "            # separate it manually\n",
    "            #loss = outputs[0]\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float).to(self.device))\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "\n",
    "            # add the current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # calculate true labels and convert it into numpy array\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "            \n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = self.evalMetric(y_true,y_pred,which+\"_\")\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_loss = total_eval_loss / len(loader)\n",
    "        # add it to the metric\n",
    "        metrics[which+'_avg_loss'] = avg_loss\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def run_train_loop(self,model,train_loader,optimiser,scheduler):\n",
    "\n",
    "        model.train() # put model in train mode\n",
    "\n",
    "        # maintain total loss to add to metric\n",
    "        total_loss = 0\n",
    "\n",
    "        # maintain predictions for each batch and calculate metrics\n",
    "        # at the end of the epoch\n",
    "        y_pred = np.zeros(shape=(0),dtype='int')\n",
    "        y_true = np.empty(shape=(0),dtype='int')\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            # separate inputs, labels and attention mask\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            # Ref: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch#:~:text=In%20PyTorch%20%2C%20we%20need%20to,backward()%20call.\n",
    "            model.zero_grad()                \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "            # outputs is always returned as tuple\n",
    "            # Separate it manually\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # define new loss function so that we can include\n",
    "            # weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "                        self.weights,dtype=torch.float).to(self.device))\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "            \n",
    "            # calculate current loss\n",
    "            # loss.item() extracts loss value as a float\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Back-propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # calculate true labels\n",
    "            b_y_true = b_labels.cpu().data.squeeze().numpy()\n",
    "\n",
    "            # calculate predicted labels by taking max of \n",
    "            # prediction scores\n",
    "            b_y_pred = torch.max(logits,1)[1]\n",
    "            b_y_pred = b_y_pred.cpu().data.squeeze().numpy()\n",
    "\n",
    "            y_pred = np.concatenate((y_pred,b_y_pred))\n",
    "            y_true = np.concatenate((y_true,b_y_true))\n",
    "\n",
    "            # clip gradient to prevent exploding gradient\n",
    "            # problems\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # gradient descent\n",
    "            optimiser.step()\n",
    "            \n",
    "            # schedule learning rate accordingly\n",
    "            scheduler.step()\n",
    "\n",
    "        # calculate avg loss \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # calculate metrics\n",
    "        train_metrics = self.evalMetric(y_true,y_pred,\"Train_\")\n",
    "        \n",
    "        # print results\n",
    "        print('avg_train_loss',avg_train_loss)\n",
    "        print('train_f1Score',train_metrics['Train_f1Score'])\n",
    "        print('train_accuracy',train_metrics['Train_accuracy'])\n",
    "\n",
    "        # add loss to metrics\n",
    "        train_metrics['Train_avg_loss'] = avg_train_loss\n",
    "\n",
    "        return train_metrics\n",
    "    \n",
    "    \n",
    "    ##------------------------------------------------------------##\n",
    "    ##----------------- Main Train Loop --------------------------##\n",
    "    ##------------------------------------------------------------##\n",
    "    def train(self,model,data_loaders,optimiser,scheduler,epochs,save_model):\n",
    "        # save train stats per epoch\n",
    "        train_stats = []\n",
    "        train_loader,val_loader,test_loader = data_loaders\n",
    "        # maintain best mF1 Score to save best model\n",
    "        best_mf1Score=-1.0\n",
    "        for epoch_i in range(0, epochs):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            \n",
    "            print(\"\")\n",
    "            print('Training...')\n",
    "            # run trian loop\n",
    "            train_metrics = self.run_train_loop(model,train_loader,\n",
    "                                            optimiser,scheduler)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\") \n",
    "            # test on validation set\n",
    "            val_metrics = self.evaluate(model,val_loader,\"Val\")\n",
    "            \n",
    "            print(\"Validation Loss: \",val_metrics['Val_avg_loss'])\n",
    "            print(\"Validation Accuracy: \",val_metrics['Val_accuracy'])\n",
    "            \n",
    "            stats = {}\n",
    "\n",
    "            # save model where validation mF1Score is best\n",
    "            if(val_metrics['Val_mF1Score']>best_mf1Score):\n",
    "                best_mf1Score=val_metrics['Val_mF1Score']\n",
    "                if(save_model):\n",
    "                    torch.save(model.state_dict(), self.model_save_path+\n",
    "                        '/best_bert_'+self.name+'.pt')\n",
    "                # evaluate best model on test set\n",
    "                test_metrics = self.evaluate(model,test_loader,\"Test\")\n",
    "\n",
    "            stats['epoch']=epoch_i+1\n",
    "\n",
    "            # add train and val metrics of the epoch to \n",
    "            # same dictionary\n",
    "            stats.update(train_metrics)\n",
    "            stats.update(val_metrics)\n",
    "\n",
    "            train_stats.append(stats)\n",
    "\n",
    "        return train_stats,test_metrics\n",
    "    \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##----------------------- Main Pipeline ---------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run(self,args,df_train,df_val,df_test):\n",
    "        # get X and Y data points \n",
    "        X_train = df_train['Text'].values\n",
    "        Y_train = df_train['Label'].values\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "        X_val = df_val['Text'].values\n",
    "        Y_val = df_val['Label'].values\n",
    "        \n",
    "        # encode data\n",
    "        # returns list of data and attention masks\n",
    "        train_data = self.encode(X_train,args['max_len'])\n",
    "        val_data = self.encode(X_val,args['max_len'])\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "        \n",
    "        # add labels to data so that we can send them to\n",
    "        # dataloader function together\n",
    "        train_data.append(Y_train)\n",
    "        val_data.append(Y_val)\n",
    "        test_data.append(Y_test)\n",
    "        \n",
    "        # convert to dataloader\n",
    "        train_dl =self.get_dataloader(train_data,args['batch_size'],True)\n",
    "        val_dl =self.get_dataloader(val_data,args['batch_size'])                          \n",
    "        test_dl =self.get_dataloader(test_data,args['batch_size'])\n",
    "        \n",
    "        # intialise model\n",
    "        model = weighted_Roberta.from_pretrained(\n",
    "            'xlm-roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
    "            params=args['params'],\n",
    "        )\n",
    "        model.to(self.device)\n",
    "        \n",
    "        optimiser = self.get_optimiser(args['learning_rate'],model)\n",
    "        \n",
    "        scheduler = self.get_scheduler(args['epochs'],optimiser,train_dl)\n",
    "        \n",
    "        # Run train loop and evaluate on validation data set\n",
    "        # on each epoch. Store best model from all epochs \n",
    "        # (best mF1 Score on Val set) and evaluate it on\n",
    "        # test set\n",
    "        train_stats,train_metrics = self.train(model,[train_dl,val_dl,test_dl],\n",
    "                                optimiser,scheduler,args['epochs'],args['save_model'])\n",
    "        \n",
    "        return train_stats,train_metrics\n",
    "        \n",
    "    ##-----------------------------------------------------------##\n",
    "    ##-------------------- Other Utilities ----------------------##\n",
    "    ##-----------------------------------------------------------##\n",
    "    def run_test(self,model,df_test,args):\n",
    "        # to evaluate test set on the final saved model\n",
    "        # to retrieve results if necessary\n",
    "        X_test = df_test['Text'].values\n",
    "        Y_test = df_test['Label'].values\n",
    "\n",
    "        test_data = self.encode(X_test,args['max_len'])\n",
    "\n",
    "        test_data.append(Y_test)\n",
    "\n",
    "        test_dl =self.get_dataloader(test_data,32)\n",
    "\n",
    "        metrics = self.evaluate(model,test_dl,\"Test\")\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def load_model(self,path,args):\n",
    "        # load saved best model\n",
    "        model = XLM_RobertaModel.from_pretrained(\n",
    "            args['bert_model'], \n",
    "            num_labels = 2, # The number of output labels--2 for binary classification                \n",
    "            args=args,\n",
    "        )\n",
    "        \n",
    "        saved_model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        return saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550872e",
   "metadata": {},
   "source": [
    "# Full Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353b620",
   "metadata": {},
   "source": [
    "## Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8526d49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29202/29202 [00:04<00:00, 6910.44it/s]\n",
      "100%|██████████| 4171/4171 [00:00<00:00, 6480.45it/s]\n",
      "100%|██████████| 8343/8343 [00:01<00:00, 7094.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 597/29202 [00:01<02:17, 208.26it/s] "
     ]
    }
   ],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/all_but_one/Let-Mi/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Let-Mi/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Let-Mi/all_but_one/',\n",
    "    'isArabic': True,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda:1',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': 'Saved_Models/Let-Mi/all_but_one/',\n",
    "        'isArabic': True,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bccd300",
   "metadata": {},
   "source": [
    "## Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccc1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/all_but_one/AMI-2020/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/AMI-2020/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/AMI-2020/all_but_one/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda:1',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': 'Saved_Models/AMI-2020/all_but_one/',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11528f13",
   "metadata": {},
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736c075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/AMI-Spanish/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/AMI-Spanish/xlmRoberta/',\n",
    "    'model_save_path': 'Saved_Models/AMI-Spanish/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,32,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "for train_cnt in [32,64,128,256,512]:\n",
    "    print(\"Train Count: \",train_cnt)\n",
    "    run_part(run_args,model_args,train_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ca584",
   "metadata": {},
   "source": [
    "## Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fff0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/Shared_Task_hin/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Shared_Task_hin/xlmRoberta/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_hin/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': False,\n",
    "        'model_save_path': '',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "for train_cnt in [32,64,128,256,512]:\n",
    "    print(\"Train Count: \",train_cnt)\n",
    "    run_part(run_args,model_args,train_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941732f",
   "metadata": {},
   "source": [
    "## Bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7fc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun-binny/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28692/28692 [00:24<00:00, 1151.37it/s]\n",
      "100%|██████████| 4097/4097 [00:03<00:00, 1198.87it/s]\n",
      "100%|██████████| 8196/8196 [00:06<00:00, 1228.50it/s]\n",
      "  1%|          | 219/28692 [00:00<00:44, 647.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 4228/28692 [01:38<19:11, 21.24it/s]"
     ]
    }
   ],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/all_but_one/Shared_Task_iben/',\n",
    "    'train_cnt':'all',\n",
    "    'res_base_path': 'Results/Shared_Task_iben/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_iben/all_but_one/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 6.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': 'Saved_Models/Shared_Task_iben/all_but_one/',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,6.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc64f6",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/all_but_one/Shared_Task_eng/',\n",
    "    'train_cnt':'all',\n",
    "    'res_base_path': 'Results/Shared_Task_eng/all_but_one/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_eng/all_but_one/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 8.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': 'Saved_Models/Shared_Task_eng/all_but_one/',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,8.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0643a59",
   "metadata": {},
   "source": [
    "## Complete Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36989c27",
   "metadata": {},
   "source": [
    "## Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d75a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3670/3670 [00:00<00:00, 6030.40it/s]\n",
      "100%|██████████| 523/523 [00:00<00:00, 7298.86it/s]\n",
      "100%|██████████| 1047/1047 [00:00<00:00, 7183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3670/3670 [01:45<00:00, 34.93it/s]\n",
      "100%|██████████| 523/523 [00:02<00:00, 260.40it/s] \n",
      "100%|██████████| 1047/1047 [00:08<00:00, 120.71it/s]\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'roberta.embeddings.position_ids', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [01:22<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5979597700930579\n",
      "train_f1Score 0.6757055893746541\n",
      "train_accuracy 0.6801310043668122\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:02<00:00, 26.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.37396468164829105\n",
      "Validation Accuracy:  0.8519230769230769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:04<00:00, 26.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 402/458 [01:13<00:10,  5.42it/s]"
     ]
    }
   ],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/Let-Mi/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Let-Mi/xlmRoberta/',\n",
    "    'model_save_path': 'Saved_Models/Let-Mi/',\n",
    "    'isArabic': True,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': 'Saved_Models/Let-Mi/',\n",
    "        'isArabic': True,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12342ae4",
   "metadata": {},
   "source": [
    "## Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98733d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/AMI-2020/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/AMI-2020/xlmRoberta/',\n",
    "    'model_save_path': 'Saved_Models/AMI-2020/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': 'Saved_Models/AMI-2020/',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfbba8c",
   "metadata": {},
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61b2817a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun-binny/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2317/2317 [00:01<00:00, 1250.02it/s]\n",
      "100%|██████████| 330/330 [00:00<00:00, 1379.65it/s]\n",
      "100%|██████████| 660/660 [00:00<00:00, 1434.98it/s]\n",
      " 10%|▉         | 224/2317 [00:00<00:03, 622.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2317/2317 [00:31<00:00, 74.46it/s]\n",
      "100%|██████████| 330/330 [00:00<00:00, 470.50it/s] \n",
      "100%|██████████| 660/660 [00:02<00:00, 254.25it/s] \n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 1/289 [00:00<00:52,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.04it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6828816292813905\n",
      "train_f1Score 0.5474910394265232\n",
      "train_accuracy 0.5631487889273357\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.6444884800329441\n",
      "Validation Accuracy:  0.6890243902439024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 27.85it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.07it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5762685356874367\n",
      "train_f1Score 0.7291225095379398\n",
      "train_accuracy 0.7236159169550173\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.5649227754371923\n",
      "Validation Accuracy:  0.7347560975609756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.02it/s]\n",
      "  0%|          | 1/289 [00:00<00:44,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:48<00:00,  5.91it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.4992307694854415\n",
      "train_f1Score 0.7838070628768304\n",
      "train_accuracy 0.782871972318339\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.5689367771875568\n",
      "Validation Accuracy:  0.774390243902439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.27it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:48<00:00,  6.01it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.43261147946583356\n",
      "train_f1Score 0.8317432784041631\n",
      "train_accuracy 0.8321799307958477\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.6204255539469603\n",
      "Validation Accuracy:  0.7774390243902439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 27.94it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.11it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 27.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.38242505725382936\n",
      "train_f1Score 0.8591244039878629\n",
      "train_accuracy 0.8594290657439446\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.7072209803978118\n",
      "Validation Accuracy:  0.7804878048780488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.07it/s]\n",
      "  0%|          | 1/289 [00:00<00:44,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.08it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.34333170479595354\n",
      "train_f1Score 0.898876404494382\n",
      "train_accuracy 0.8987889273356401\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.8001245863781106\n",
      "Validation Accuracy:  0.7865853658536586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.26it/s]\n",
      "  0%|          | 1/289 [00:00<00:42,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:46<00:00,  6.15it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.29620550631845183\n",
      "train_f1Score 0.9194395796847636\n",
      "train_accuracy 0.9204152249134948\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.42it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.8764934266304097\n",
      "Validation Accuracy:  0.7835365853658537\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.05it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.23090586084453552\n",
      "train_f1Score 0.9425087108013938\n",
      "train_accuracy 0.9429065743944637\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.20it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.9808037922967497\n",
      "Validation Accuracy:  0.7652439024390244\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:46<00:00,  6.17it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.22979885100461156\n",
      "train_f1Score 0.9465449804432855\n",
      "train_accuracy 0.9467993079584776\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.41it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.0639216825025293\n",
      "Validation Accuracy:  0.7804878048780488\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:46<00:00,  6.17it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.19758924518173696\n",
      "train_f1Score 0.9526704298740772\n",
      "train_accuracy 0.9528546712802768\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.0868234064872944\n",
      "Validation Accuracy:  0.7804878048780488\n",
      "Saving Test Metrics....\n",
      "Fold:  2\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun-binny/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2317/2317 [00:01<00:00, 1343.88it/s]\n",
      "100%|██████████| 330/330 [00:00<00:00, 1378.84it/s]\n",
      "100%|██████████| 660/660 [00:00<00:00, 1401.41it/s]\n",
      " 10%|▉         | 226/2317 [00:00<00:03, 585.87it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2317/2317 [00:33<00:00, 69.99it/s]\n",
      "100%|██████████| 330/330 [00:00<00:00, 457.13it/s] \n",
      "100%|██████████| 660/660 [00:02<00:00, 248.02it/s] \n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 1/289 [00:00<00:49,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.04it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6677813916676597\n",
      "train_f1Score 0.5878103837471783\n",
      "train_accuracy 0.6051038062283737\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.6318625727804695\n",
      "Validation Accuracy:  0.6890243902439024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.25it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.06it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5483093634763391\n",
      "train_f1Score 0.7425044091710759\n",
      "train_accuracy 0.7474048442906575\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.5619550776190874\n",
      "Validation Accuracy:  0.7286585365853658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.40it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.05it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.49657273395663726\n",
      "train_f1Score 0.8158906450234943\n",
      "train_accuracy 0.8135813148788927\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.641144265307159\n",
      "Validation Accuracy:  0.7469512195121951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.09it/s]\n",
      "  0%|          | 1/289 [00:00<00:46,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.05it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.41587670950483285\n",
      "train_f1Score 0.8534009294465568\n",
      "train_accuracy 0.8499134948096886\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.59it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.8180443862589394\n",
      "Validation Accuracy:  0.7195121951219512\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:48<00:00,  6.00it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.36983791763240076\n",
      "train_f1Score 0.8765849535080305\n",
      "train_accuracy 0.8737024221453287\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.16it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.7877233534506182\n",
      "Validation Accuracy:  0.7469512195121951\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.05it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.33430334540876755\n",
      "train_f1Score 0.9050232165470663\n",
      "train_accuracy 0.902681660899654\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.17it/s]\n",
      "  0%|          | 1/289 [00:00<00:46,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.8637761160186151\n",
      "Validation Accuracy:  0.7439024390243902\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:48<00:00,  5.94it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.30351490856415775\n",
      "train_f1Score 0.9168081494057726\n",
      "train_accuracy 0.9152249134948097\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.895672481893221\n",
      "Validation Accuracy:  0.7560975609756098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.23it/s]\n",
      "  0%|          | 1/289 [00:00<00:43,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:46<00:00,  6.16it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.2594379236451173\n",
      "train_f1Score 0.9307692307692307\n",
      "train_accuracy 0.9299307958477508\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.07it/s]\n",
      "  0%|          | 1/289 [00:00<00:46,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.0733581936645618\n",
      "Validation Accuracy:  0.7439024390243902\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:46<00:00,  6.18it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.24418548063719872\n",
      "train_f1Score 0.9390557939914163\n",
      "train_accuracy 0.9385813148788927\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.1127654781973944\n",
      "Validation Accuracy:  0.7591463414634146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.09it/s]\n",
      "  0%|          | 1/289 [00:00<00:46,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:46<00:00,  6.23it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.19462652655904886\n",
      "train_f1Score 0.9521757862990091\n",
      "train_accuracy 0.9519896193771626\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.1794899458792516\n",
      "Validation Accuracy:  0.7347560975609756\n",
      "Saving Test Metrics....\n",
      "Fold:  3\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun-binny/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2317/2317 [00:01<00:00, 1394.27it/s]\n",
      "100%|██████████| 330/330 [00:00<00:00, 1236.34it/s]\n",
      "100%|██████████| 660/660 [00:00<00:00, 1372.52it/s]\n",
      " 10%|▉         | 223/2317 [00:00<00:03, 600.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2317/2317 [00:33<00:00, 69.57it/s]\n",
      "100%|██████████| 330/330 [00:00<00:00, 437.09it/s] \n",
      "100%|██████████| 660/660 [00:02<00:00, 237.03it/s] \n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 1/289 [00:00<00:49,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.08it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.663399139696339\n",
      "train_f1Score 0.5921231326392032\n",
      "train_accuracy 0.6102941176470589\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.604553437087594\n",
      "Validation Accuracy:  0.7195121951219512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.25it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.07it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5611517334737167\n",
      "train_f1Score 0.7304048234280792\n",
      "train_accuracy 0.7292387543252595\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.48272592483497245\n",
      "Validation Accuracy:  0.7926829268292683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.29it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.10it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.4624910622989843\n",
      "train_f1Score 0.8025917926565874\n",
      "train_accuracy 0.8023356401384083\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.4811749752701783\n",
      "Validation Accuracy:  0.8048780487804879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.23it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.10it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.380923919418382\n",
      "train_f1Score 0.853541938287701\n",
      "train_accuracy 0.8542387543252595\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.41it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.5271880978309527\n",
      "Validation Accuracy:  0.7926829268292683\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.08it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.3280626193210998\n",
      "train_f1Score 0.897854077253219\n",
      "train_accuracy 0.8970588235294118\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.34it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.6921038177863854\n",
      "Validation Accuracy:  0.801829268292683\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.10it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.3002836862804568\n",
      "train_f1Score 0.9124783362218369\n",
      "train_accuracy 0.9126297577854672\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.24it/s]\n",
      "  0%|          | 1/289 [00:00<00:47,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.9722265628075636\n",
      "Validation Accuracy:  0.7926829268292683\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:48<00:00,  6.01it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.2550682529652407\n",
      "train_f1Score 0.9359861591695501\n",
      "train_accuracy 0.9359861591695502\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.18it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.9647107299576264\n",
      "Validation Accuracy:  0.7957317073170732\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.06it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.17882617372394277\n",
      "train_f1Score 0.9572354211663067\n",
      "train_accuracy 0.9571799307958477\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.08it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.0816843475118598\n",
      "Validation Accuracy:  0.7926829268292683\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.09it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.15849582823742644\n",
      "train_f1Score 0.9627705627705628\n",
      "train_accuracy 0.96280276816609\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.19it/s]\n",
      "  0%|          | 1/289 [00:00<00:48,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.2468893926039837\n",
      "Validation Accuracy:  0.7835365853658537\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:46<00:00,  6.17it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.12400248032151867\n",
      "train_f1Score 0.9744478129060199\n",
      "train_accuracy 0.9744809688581315\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.2229824357200414\n",
      "Validation Accuracy:  0.7865853658536586\n",
      "Saving Test Metrics....\n",
      "Fold:  4\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun-binny/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2317/2317 [00:01<00:00, 1353.51it/s]\n",
      "100%|██████████| 330/330 [00:00<00:00, 1358.08it/s]\n",
      "100%|██████████| 660/660 [00:00<00:00, 1312.76it/s]\n",
      "  9%|▉         | 220/2317 [00:00<00:03, 602.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2317/2317 [00:33<00:00, 69.43it/s]\n",
      "100%|██████████| 330/330 [00:00<00:00, 438.28it/s] \n",
      "100%|██████████| 660/660 [00:02<00:00, 235.10it/s] \n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 1/289 [00:00<00:48,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.05it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6830372660320936\n",
      "train_f1Score 0.5687446626814687\n",
      "train_accuracy 0.5631487889273357\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.6176318250051359\n",
      "Validation Accuracy:  0.6737804878048781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.27it/s]\n",
      "  0%|          | 1/289 [00:00<00:47,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:48<00:00,  6.00it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6708462628629381\n",
      "train_f1Score 0.6337662337662338\n",
      "train_accuracy 0.634083044982699\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.06it/s]\n",
      "  0%|          | 1/289 [00:00<00:46,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.6402274842669324\n",
      "Validation Accuracy:  0.6341463414634146\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.08it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6008061499537893\n",
      "train_f1Score 0.7013977128335451\n",
      "train_accuracy 0.6950692041522492\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.656789829091328\n",
      "Validation Accuracy:  0.7134146341463414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.28it/s]\n",
      "  0%|          | 1/289 [00:00<00:44,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.07it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5579455849105511\n",
      "train_f1Score 0.7332468655425854\n",
      "train_accuracy 0.7331314878892734\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.5421613894584703\n",
      "Validation Accuracy:  0.7286585365853658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.36it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.11it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5152624558824981\n",
      "train_f1Score 0.7615384615384616\n",
      "train_accuracy 0.7586505190311419\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.39it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.5911606059568685\n",
      "Validation Accuracy:  0.7012195121951219\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.05it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.4765607891249822\n",
      "train_f1Score 0.783461210571185\n",
      "train_accuracy 0.7802768166089965\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.21it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.6309694072640524\n",
      "Validation Accuracy:  0.7195121951219512\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.04it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.43294536399696937\n",
      "train_f1Score 0.8209531987977673\n",
      "train_accuracy 0.8196366782006921\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.25it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.6221024401304198\n",
      "Validation Accuracy:  0.7286585365853658\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.03it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.3927792716500668\n",
      "train_f1Score 0.8491620111731844\n",
      "train_accuracy 0.8481833910034602\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.613307374097952\n",
      "Validation Accuracy:  0.7774390243902439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 28.33it/s]\n",
      "  0%|          | 1/289 [00:00<00:46,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.04it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.35829892275680925\n",
      "train_f1Score 0.8740359897172237\n",
      "train_accuracy 0.8728373702422145\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.41it/s]\n",
      "  0%|          | 1/289 [00:00<00:45,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.8013818335106097\n",
      "Validation Accuracy:  0.7469512195121951\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:47<00:00,  6.04it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.3479170919747243\n",
      "train_f1Score 0.8843830888697154\n",
      "train_accuracy 0.884083044982699\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.8608244124618245\n",
      "Validation Accuracy:  0.7408536585365854\n",
      "Saving Test Metrics....\n",
      "Fold:  5\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun-binny/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2311/2311 [00:01<00:00, 1266.97it/s]\n",
      "100%|██████████| 329/329 [00:00<00:00, 1324.44it/s]\n",
      "100%|██████████| 667/667 [00:00<00:00, 1274.61it/s]\n",
      " 10%|▉         | 220/2311 [00:00<00:03, 585.96it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2311/2311 [00:33<00:00, 69.72it/s]\n",
      "100%|██████████| 329/329 [00:00<00:00, 428.85it/s] \n",
      "100%|██████████| 667/667 [00:03<00:00, 221.33it/s] \n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 1/288 [00:00<00:48,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:47<00:00,  6.02it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.6706833925305141\n",
      "train_f1Score 0.571685393258427\n",
      "train_accuracy 0.5863715277777778\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.6305938835551099\n",
      "Validation Accuracy:  0.6371951219512195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:02<00:00, 28.19it/s]\n",
      "  0%|          | 1/288 [00:00<00:45,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:47<00:00,  6.07it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.5595699339691136\n",
      "train_f1Score 0.7267927848658161\n",
      "train_accuracy 0.73046875\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.5396254807710648\n",
      "Validation Accuracy:  0.7347560975609756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:02<00:00, 28.25it/s]\n",
      "  0%|          | 1/288 [00:00<00:45,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:47<00:00,  6.09it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.46136956483436126\n",
      "train_f1Score 0.8092983211364616\n",
      "train_accuracy 0.8077256944444444\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.48537610262268926\n",
      "Validation Accuracy:  0.8140243902439024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:02<00:00, 28.05it/s]\n",
      "  0%|          | 1/288 [00:00<00:45,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:47<00:00,  6.01it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.3741553157128187\n",
      "train_f1Score 0.8616052060737528\n",
      "train_accuracy 0.8615451388888888\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.31it/s]\n",
      "  0%|          | 1/288 [00:00<00:45,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.6598425133199226\n",
      "Validation Accuracy:  0.7774390243902439\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:47<00:00,  6.06it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.3521869686956052\n",
      "train_f1Score 0.8945770065075922\n",
      "train_accuracy 0.89453125\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.33it/s]\n",
      "  0%|          | 1/288 [00:00<00:47,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.660993749820968\n",
      "Validation Accuracy:  0.7957317073170732\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:47<00:00,  6.09it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.2706906588217761\n",
      "train_f1Score 0.9193408499566347\n",
      "train_accuracy 0.9192708333333334\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.38it/s]\n",
      "  0%|          | 1/288 [00:00<00:45,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.8303681856430158\n",
      "Validation Accuracy:  0.801829268292683\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:47<00:00,  6.09it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.21065765885578003\n",
      "train_f1Score 0.9469565217391305\n",
      "train_accuracy 0.9470486111111112\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.44it/s]\n",
      "  0%|          | 1/288 [00:00<00:45,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.9670906404044661\n",
      "Validation Accuracy:  0.7957317073170732\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:46<00:00,  6.18it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.15824072990290006\n",
      "train_f1Score 0.9617723718505647\n",
      "train_accuracy 0.9618055555555556\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.12it/s]\n",
      "  0%|          | 1/288 [00:00<00:46,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.1181725668323386\n",
      "Validation Accuracy:  0.7804878048780488\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:46<00:00,  6.16it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.13257589722949584\n",
      "train_f1Score 0.9703056768558952\n",
      "train_accuracy 0.9704861111111112\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.49it/s]\n",
      "  0%|          | 1/288 [00:00<00:44,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.1820524025327959\n",
      "Validation Accuracy:  0.7774390243902439\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:46<00:00,  6.19it/s]\n",
      "  7%|▋         | 3/41 [00:00<00:01, 28.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.09458160093628168\n",
      "train_f1Score 0.9790575916230366\n",
      "train_accuracy 0.9791666666666666\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.19474210029343\n",
      "Validation Accuracy:  0.7865853658536586\n",
      "Saving Test Metrics....\n"
     ]
    }
   ],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/AMI-Spanish/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/AMI-Spanish/xlmRoberta/',\n",
    "    'model_save_path': 'Saved_Models/AMI-Spanish/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 1.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': 'Saved_Models/AMI-Spanish/',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,1.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a690c93",
   "metadata": {},
   "source": [
    "# Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2645263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "\tInitialising Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithundas/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading Dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4327/4327 [00:03<00:00, 1255.95it/s]\n",
      "100%|██████████| 618/618 [00:00<00:00, 1484.94it/s]\n",
      "100%|██████████| 1236/1236 [00:00<00:00, 1426.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Starts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4327/4327 [02:30<00:00, 28.76it/s]\n",
      "100%|██████████| 618/618 [00:03<00:00, 200.42it/s] \n",
      "100%|██████████| 1236/1236 [00:12<00:00, 97.91it/s]\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing weighted_Roberta: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing weighted_Roberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing weighted_Roberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of weighted_Roberta were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'roberta.embeddings.position_ids', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 540/540 [01:39<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.7009918770304433\n",
      "train_f1Score 0.33937282229965154\n",
      "train_accuracy 0.5611111111111111\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:02<00:00, 26.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.7104347154691621\n",
      "Validation Accuracy:  0.7792207792207793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:05<00:00, 26.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 233/540 [00:42<00:55,  5.50it/s]"
     ]
    }
   ],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/Shared_Task_hin/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Shared_Task_hin/xlmRoberta/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_hin/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 4.5],\n",
    "        'save_model': True,\n",
    "        'model_save_path': 'Saved_Models/Shared_Task_hin/',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,4.5],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47c7d1",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf973e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/Shared_Task_eng/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Shared_Task_eng/xlmRoberta/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_eng/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 8.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': 'Saved_Models/Shared_Task_eng/',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,8.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317d7c9",
   "metadata": {},
   "source": [
    "# Bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args={\n",
    "    'model_name':'xlm_roberta',\n",
    "    'data_path':'Data_Processed/Shared_Task_iben/',\n",
    "    'train_cnt':256,\n",
    "    'res_base_path': 'Results/Shared_Task_iben/xlmRoberta/',\n",
    "    'model_save_path': 'Saved_Models/Shared_Task_iben/',\n",
    "    'isArabic': False,\n",
    "}\n",
    "\n",
    "model_args={\n",
    "        'seed_val': 42,\n",
    "        'batch_size': 8,\n",
    "        'bert_model': \"xlm-roberta-base\",\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 10,\n",
    "        'max_len': 128,\n",
    "        'device': 'cuda',\n",
    "        'weights': [1.0, 6.0],\n",
    "        'save_model': True,\n",
    "        'model_save_path': 'Saved_Models/Shared_Task_iben/',\n",
    "        'isArabic': False,\n",
    "        'model_path': \"\",\n",
    "        'max_length':128,\n",
    "        'is_train':True,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':30,\n",
    "        'to_save':True,\n",
    "        'frac':0.8,\n",
    "        'params':{\n",
    "            'max_length':128,\n",
    "            'path_files': 'xlm-roberta-base',\n",
    "            'what_bert':'weighted',\n",
    "            'batch_size':8,\n",
    "            'is_train':True,\n",
    "            'learning_rate':2e-5,\n",
    "            'epsilon':1e-8,\n",
    "            'random_seed':30,\n",
    "            'epochs':10,\n",
    "            'to_save':True,\n",
    "            'weights':[1.0,6.0],\n",
    "            'frac':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "run(run_args,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3312ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
