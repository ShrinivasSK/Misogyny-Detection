{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:00:23.193431Z",
     "start_time": "2021-08-19T06:00:22.912546Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:00:25.529905Z",
     "start_time": "2021-08-19T06:00:25.432902Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:00:28.109655Z",
     "start_time": "2021-08-19T06:00:28.105563Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"Data_Processed/Shared_Task_eng/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:00:30.659496Z",
     "start_time": "2021-08-19T06:00:30.393156Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_FOLDER+\"train_1.csv\")\n",
    "df_val = pd.read_csv(DATA_FOLDER+\"val_1.csv\")\n",
    "df_test = pd.read_csv(DATA_FOLDER+\"test_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:00:31.595282Z",
     "start_time": "2021-08-19T06:00:31.582114Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.dropna(inplace=True)\n",
    "df_val.dropna(inplace=True)\n",
    "df_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:00:32.573919Z",
     "start_time": "2021-08-19T06:00:32.566331Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "- https://towardsdatascience.com/nlp-embedding-techniques-51b7e6ec9f92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['Text'].values\n",
    "Y_train = df_train['Label'].values\n",
    "X_val = df_val['Text'].values\n",
    "Y_val = df_val['Label'].values\n",
    "X_test = df_test['Text'].values\n",
    "Y_test = df_test['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_val.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_embedding(train,val,test):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(train)\n",
    "    \n",
    "    x_train = vectorizer.transform(train)\n",
    "    x_val = vectorizer.transform(val)\n",
    "    x_test = vectorizer.transform(test)\n",
    "    \n",
    "    return x_train,x_val,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,x_test = get_bow_embedding(X_train,X_val,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_val.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_tfidf = vectorizer_tfidf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_embedding(train,val,test):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train)\n",
    "    \n",
    "    x_train = vectorizer.transform(train)\n",
    "    x_val = vectorizer.transform(val)\n",
    "    x_test = vectorizer.transform(test)\n",
    "    \n",
    "    return x_train,x_val,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,x_test = get_tfidf_embedding(X_train,X_val,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_val.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google's Idea\n",
    "- Ref: https://developers.google.com/machine-learning/guides/text-classification/step-3\n",
    "- Convert Words into uni-grams and bi-grams and calculate all features using tf-idf and then select top 20K features using f_classif or chi_2\n",
    "- This article says that normalisation does not help text datasets much so it is not used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'float32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorise(kwargs,train_text,train_labels,val_text,test_text):\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    \n",
    "    x_train = vectorizer.fit_transform(train_text)\n",
    "    \n",
    "    x_val = vectorizer.transform(val_text)\n",
    "    x_test = vectorizer.transform(test_text)\n",
    "    \n",
    "    print(\"Features Calculated: \",x_train.shape[1])\n",
    "    \n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    \n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    x_test = selector.transform(x_test).astype('float32')\n",
    "    \n",
    "    return x_train,x_val,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,x_test = ngram_vectorise(kwargs,X_train,Y_train,X_val,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_val.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- Ref: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv['gold'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_w2v(wv,data):\n",
    "    new_data=[]\n",
    "    for sentence in data:\n",
    "        sample=[]\n",
    "        for word in sentence:\n",
    "            if word in wv:\n",
    "                sample.append(wv[word])\n",
    "            else:\n",
    "                sample.append(np.zeros(shape=(300)))\n",
    "        new_data.append(np.mean(np.array(sample),axis=0))\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_word2vec = get_word2vec_embeddings(wv,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_embedding(wv,train,val,test):\n",
    "    x_train = encode_w2v(wv,train)\n",
    "    x_val = encode_w2v(wv,val)\n",
    "    x_test = encode_w2v(wv,test)\n",
    "    \n",
    "    return x_test,x_val,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,x_test = get_w2v_embedding(wv,X_train,X_val,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_val.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec\n",
    "- https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents, vector_size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_d2v(model,data):\n",
    "    embed = [list(model.infer_vector(ele.split('.'))) for ele in data]\n",
    "    return np.array(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_d2v_embedding(model,train,val,test):\n",
    "    x_train = encode_d2v(model,train)\n",
    "    x_val = encode_d2v(model,val)\n",
    "    x_test = encode_d2v(model,test)\n",
    "    \n",
    "    return x_train,x_val,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,x_test = get_d2v_embedding(model,X_train,X_val,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_val.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:01:44.443346Z",
     "start_time": "2021-08-19T06:01:43.266219Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install laserembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:02:34.013811Z",
     "start_time": "2021-08-19T06:01:57.850379Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m laserembeddings download-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:02:43.944461Z",
     "start_time": "2021-08-19T06:02:43.710514Z"
    }
   },
   "outputs": [],
   "source": [
    "from laserembeddings import Laser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:02:52.153914Z",
     "start_time": "2021-08-19T06:02:49.890324Z"
    }
   },
   "outputs": [],
   "source": [
    "laser= Laser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:07:10.279129Z",
     "start_time": "2021-08-19T06:07:10.273974Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_laser(train,test,laser,l):\n",
    "    x_train = laser.embed_sentences(train)\n",
    "    x_test = laser.embed_sentences(test)\n",
    "    \n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:07:44.474768Z",
     "start_time": "2021-08-19T06:07:44.469153Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train['Text'].values\n",
    "Y_train = df_train['Label'].values\n",
    "X_val = df_val['Text'].values\n",
    "Y_val = df_val['Label'].values\n",
    "X_test = df_test['Text'].values\n",
    "Y_test = df_test['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T06:08:11.401193Z",
     "start_time": "2021-08-19T06:08:11.329537Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train,x_val = encode_laser(X_train,X_test,laser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- KNN\n",
    "- Gaussian NB\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMetric(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    mf1Score = f1_score(y_true, y_pred, average='macro')\n",
    "    f1Score  = f1_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    area_under_c = auc(fpr, tpr)\n",
    "    recallScore = recall_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "    precisionScore = precision_score(y_true, y_pred, labels = np.unique(y_pred))\n",
    "    return dict({\"accuracy\": accuracy, 'mF1Score': mf1Score, \n",
    "                    'f1Score': f1Score, 'precision': precisionScore, \n",
    "                    'recall': recallScore})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {0:1,1:8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(kernel=\"rbf\",class_weight=weights,probability=True)\n",
    "SVM.fit(x_train,Y_train)\n",
    "y_pred = SVM.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalMetric(Y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier(class_weight=weights, \n",
    "            criterion='gini', max_depth=100, max_features=1.0, \n",
    "            max_leaf_nodes=10, min_impurity_split=1e-07, \n",
    "            min_samples_leaf=1, min_samples_split=2, \n",
    "            min_weight_fraction_leaf=0.10, presort=False, \n",
    "            random_state=42, splitter='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(kernel=\"rbf\",class_weight=weights,probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=1000,random_state=0,\n",
    "                    n_jobs=1000,max_depth=100,bootstrap=True,\n",
    "                      class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB =AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(leaf_size=1,p=2,n_neighbors=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNB = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(C=1.0, class_weight=weights, dual=False, \n",
    "        fit_intercept=True, intercept_scaling=1, max_iter=100, \n",
    "        multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, \n",
    "        solver='liblinear', tol=0.0001,verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBC = XGBClassifier(learning_rate =0.1, n_estimators=100000, \n",
    "            max_depth=6, min_child_weight=6, gamma=0, subsample=0.6, \n",
    "            colsample_bytree=0.8, reg_alpha=0.005, \n",
    "            objective= 'binary:logistic', nthread=2, \n",
    "            scale_pos_weight=1, seed=42, class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers=[SVM,XGBC,RF,DT,AB,KNN,GNB,LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['SVC','XGBoost','Random Forest','Decision Tree','AdaBoost',\n",
    "        'KNN','Gausian NB','Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,cf in enumerate(classifiers):\n",
    "    cf.fit(x_train,Y_train)\n",
    "    y_pred = cf.predict(x_val)\n",
    "    metric = evalMetric(Y_val,y_pred)\n",
    "    res[names[i]]=metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
